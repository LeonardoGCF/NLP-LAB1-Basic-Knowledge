{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK包括古腾堡计划电子文本档案库中的少量文本，该档案库包含约25,000本免费电子书，这些电子书存放在http://www.gutenberg.org/。 首先，让Python解释器加载NLTK程序包，然后要求查看nltk.corpus.gutenberg.fileids（），即该语料库中的文件标识符："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['austen-emma.txt',\n",
       " 'austen-persuasion.txt',\n",
       " 'austen-sense.txt',\n",
       " 'bible-kjv.txt',\n",
       " 'blake-poems.txt',\n",
       " 'bryant-stories.txt',\n",
       " 'burgess-busterbrown.txt',\n",
       " 'carroll-alice.txt',\n",
       " 'chesterton-ball.txt',\n",
       " 'chesterton-brown.txt',\n",
       " 'chesterton-thursday.txt',\n",
       " 'edgeworth-parents.txt',\n",
       " 'melville-moby_dick.txt',\n",
       " 'milton-paradise.txt',\n",
       " 'shakespeare-caesar.txt',\n",
       " 'shakespeare-hamlet.txt',\n",
       " 'shakespeare-macbeth.txt',\n",
       " 'whitman-leaves.txt']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.corpus.gutenberg.fileids()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "选出其中一个文本，并且重命名，然后统计字数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "emma = nltk.corpus.gutenberg.words('austen-emma.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "192427"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(emma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "emmat = nltk.Text(emma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 25 of 37 matches:\n",
      "er father , was sometimes taken by surprize at his being still able to pity ` \n",
      "hem do the other any good .\" \" You surprize me ! Emma must do Harriet good : a\n",
      "Knightley actually looked red with surprize and displeasure , as he stood up ,\n",
      "r . Elton , and found to his great surprize , that Mr . Elton was actually on \n",
      "d aid .\" Emma saw Mrs . Weston ' s surprize , and felt that it must be great ,\n",
      "father was quite taken up with the surprize of so sudden a journey , and his f\n",
      "y , in all the favouring warmth of surprize and conjecture . She was , moreove\n",
      "he appeared , to have her share of surprize , introduction , and pleasure . Th\n",
      "ir plans ; and it was an agreeable surprize to her , therefore , to perceive t\n",
      "talking aunt had taken me quite by surprize , it must have been the death of m\n",
      "f all the dialogue which ensued of surprize , and inquiry , and congratulation\n",
      " the present . They might chuse to surprize her .\" Mrs . Cole had many to agre\n",
      "the mode of it , the mystery , the surprize , is more like a young woman ' s s\n",
      " to her song took her agreeably by surprize -- a second , slightly but correct\n",
      "\" \" Oh ! no -- there is nothing to surprize one at all .-- A pretty fortune ; \n",
      "t to be considered . Emma ' s only surprize was that Jane Fairfax should accep\n",
      "of your admiration may take you by surprize some day or other .\" Mr . Knightle\n",
      "ation for her will ever take me by surprize .-- I never had a thought of her i\n",
      " expected by the best judges , for surprize -- but there was great joy . Mr . \n",
      " sound of at first , without great surprize . \" So unreasonably early !\" she w\n",
      "d Frank Churchill , with a look of surprize and displeasure .-- \" That is easy\n",
      "; and Emma could imagine with what surprize and mortification she must be retu\n",
      "tled that Jane should go . Quite a surprize to me ! I had not the least idea !\n",
      " . It is impossible to express our surprize . He came to speak to his father o\n",
      "g engaged !\" Emma even jumped with surprize ;-- and , horror - struck , exclai\n"
     ]
    }
   ],
   "source": [
    "emmat.concordance('surprize')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于每次调用包需要写很长，python为了节省这些，提供了import的语句解决"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import gutenberg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['austen-emma.txt',\n",
       " 'austen-persuasion.txt',\n",
       " 'austen-sense.txt',\n",
       " 'bible-kjv.txt',\n",
       " 'blake-poems.txt',\n",
       " 'bryant-stories.txt',\n",
       " 'burgess-busterbrown.txt',\n",
       " 'carroll-alice.txt',\n",
       " 'chesterton-ball.txt',\n",
       " 'chesterton-brown.txt',\n",
       " 'chesterton-thursday.txt',\n",
       " 'edgeworth-parents.txt',\n",
       " 'melville-moby_dick.txt',\n",
       " 'milton-paradise.txt',\n",
       " 'shakespeare-caesar.txt',\n",
       " 'shakespeare-hamlet.txt',\n",
       " 'shakespeare-macbeth.txt',\n",
       " 'whitman-leaves.txt']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gutenberg.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "emma = gutenberg.words('austen-emma.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "该程序为每个文本显示三个统计信息：平均单词长度，平均句子长度以及每个词汇项平均出现在文本中的次数（我们的词汇多样性得分）。\n",
    "\n",
    "raw（）访问原始文本==告诉我们文本一共多少letters 包含空格  words（）字 sents==把文本分为句子，每个句子都是单词列表 vocab = vocabulary round（）四舍五入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 25 26 austen-emma.txt\n",
      "5 26 17 austen-persuasion.txt\n",
      "5 28 22 austen-sense.txt\n",
      "4 34 79 bible-kjv.txt\n",
      "5 19 5 blake-poems.txt\n",
      "4 19 14 bryant-stories.txt\n",
      "4 18 12 burgess-busterbrown.txt\n",
      "4 20 13 carroll-alice.txt\n",
      "5 20 12 chesterton-ball.txt\n",
      "5 23 11 chesterton-brown.txt\n",
      "5 18 11 chesterton-thursday.txt\n",
      "4 21 25 edgeworth-parents.txt\n",
      "5 26 15 melville-moby_dick.txt\n",
      "5 52 11 milton-paradise.txt\n",
      "4 12 9 shakespeare-caesar.txt\n",
      "4 12 8 shakespeare-hamlet.txt\n",
      "4 12 7 shakespeare-macbeth.txt\n",
      "5 36 12 whitman-leaves.txt\n"
     ]
    }
   ],
   "source": [
    "for fileid in gutenberg.fileids():\n",
    "    num_chars = len( gutenberg.raw(fileid))\n",
    "    num_words = len(gutenberg.words(fileid))\n",
    "    num_sents = len(gutenberg.sents(fileid))\n",
    "    num_vocab = len(set(w.lower() for w in gutenberg.words(fileid)))\n",
    "    print(round(num_chars/num_words),round(num_words/num_sents),round(num_words/num_vocab),fileid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "macbeth_sentences = gutenberg.sents('shakespeare-macbeth.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['[', 'The', 'Tragedie', 'of', 'Macbeth', 'by', 'William', 'Shakespeare', '1603', ']'], ['Actus', 'Primus', '.'], ...]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "macbeth_sentences "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "longest_len = max(len(s) for s in macbeth_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Doubtfull',\n",
       "  'it',\n",
       "  'stood',\n",
       "  ',',\n",
       "  'As',\n",
       "  'two',\n",
       "  'spent',\n",
       "  'Swimmers',\n",
       "  ',',\n",
       "  'that',\n",
       "  'doe',\n",
       "  'cling',\n",
       "  'together',\n",
       "  ',',\n",
       "  'And',\n",
       "  'choake',\n",
       "  'their',\n",
       "  'Art',\n",
       "  ':',\n",
       "  'The',\n",
       "  'mercilesse',\n",
       "  'Macdonwald',\n",
       "  '(',\n",
       "  'Worthie',\n",
       "  'to',\n",
       "  'be',\n",
       "  'a',\n",
       "  'Rebell',\n",
       "  ',',\n",
       "  'for',\n",
       "  'to',\n",
       "  'that',\n",
       "  'The',\n",
       "  'multiplying',\n",
       "  'Villanies',\n",
       "  'of',\n",
       "  'Nature',\n",
       "  'Doe',\n",
       "  'swarme',\n",
       "  'vpon',\n",
       "  'him',\n",
       "  ')',\n",
       "  'from',\n",
       "  'the',\n",
       "  'Westerne',\n",
       "  'Isles',\n",
       "  'Of',\n",
       "  'Kernes',\n",
       "  'and',\n",
       "  'Gallowgrosses',\n",
       "  'is',\n",
       "  'supply',\n",
       "  \"'\",\n",
       "  'd',\n",
       "  ',',\n",
       "  'And',\n",
       "  'Fortune',\n",
       "  'on',\n",
       "  'his',\n",
       "  'damned',\n",
       "  'Quarry',\n",
       "  'smiling',\n",
       "  ',',\n",
       "  'Shew',\n",
       "  \"'\",\n",
       "  'd',\n",
       "  'like',\n",
       "  'a',\n",
       "  'Rebells',\n",
       "  'Whore',\n",
       "  ':',\n",
       "  'but',\n",
       "  'all',\n",
       "  \"'\",\n",
       "  's',\n",
       "  'too',\n",
       "  'weake',\n",
       "  ':',\n",
       "  'For',\n",
       "  'braue',\n",
       "  'Macbeth',\n",
       "  '(',\n",
       "  'well',\n",
       "  'hee',\n",
       "  'deserues',\n",
       "  'that',\n",
       "  'Name',\n",
       "  ')',\n",
       "  'Disdayning',\n",
       "  'Fortune',\n",
       "  ',',\n",
       "  'with',\n",
       "  'his',\n",
       "  'brandisht',\n",
       "  'Steele',\n",
       "  ',',\n",
       "  'Which',\n",
       "  'smoak',\n",
       "  \"'\",\n",
       "  'd',\n",
       "  'with',\n",
       "  'bloody',\n",
       "  'execution',\n",
       "  '(',\n",
       "  'Like',\n",
       "  'Valours',\n",
       "  'Minion',\n",
       "  ')',\n",
       "  'caru',\n",
       "  \"'\",\n",
       "  'd',\n",
       "  'out',\n",
       "  'his',\n",
       "  'passage',\n",
       "  ',',\n",
       "  'Till',\n",
       "  'hee',\n",
       "  'fac',\n",
       "  \"'\",\n",
       "  'd',\n",
       "  'the',\n",
       "  'Slaue',\n",
       "  ':',\n",
       "  'Which',\n",
       "  'neu',\n",
       "  \"'\",\n",
       "  'r',\n",
       "  'shooke',\n",
       "  'hands',\n",
       "  ',',\n",
       "  'nor',\n",
       "  'bad',\n",
       "  'farwell',\n",
       "  'to',\n",
       "  'him',\n",
       "  ',',\n",
       "  'Till',\n",
       "  'he',\n",
       "  'vnseam',\n",
       "  \"'\",\n",
       "  'd',\n",
       "  'him',\n",
       "  'from',\n",
       "  'the',\n",
       "  'Naue',\n",
       "  'toth',\n",
       "  \"'\",\n",
       "  'Chops',\n",
       "  ',',\n",
       "  'And',\n",
       "  'fix',\n",
       "  \"'\",\n",
       "  'd',\n",
       "  'his',\n",
       "  'Head',\n",
       "  'vpon',\n",
       "  'our',\n",
       "  'Battlements']]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[s for s in macbeth_sentences if len(s) == longest_len]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=============================Web and chat text======================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gutenberg是正式的书， 我们仍需要一些不那么正式的语言文本，webtext包含一些网络上火狐讨论区的文本，在纽约听到的对话，加勒比海盗的电影脚本，个人广告和葡萄酒评论"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import webtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "firefox.txt Cookie Manager: \"Don't allow sites that set removed cookies to set future cookies\" should stay check G-----G\n",
      "grail.txt SCENE 1: [wind] [clop clop clop] \n",
      "KING ARTHUR: Whoa there!  [clop clop clop] \n",
      "SOLDIER #1: Halt!  Who G-----G\n",
      "overheard.txt White guy: So, do you have any plans for this evening?\n",
      "Asian girl: Yeah, being angry!\n",
      "White guy: Oh, G-----G\n",
      "pirates.txt PIRATES OF THE CARRIBEAN: DEAD MAN'S CHEST, by Ted Elliott & Terry Rossio\n",
      "[view looking straight dow G-----G\n",
      "singles.txt 25 SEXY MALE, seeks attrac older single lady, for discreet encounters.\n",
      "35YO Security Guard, seeking  G-----G\n",
      "wine.txt Lovely delicate, fragrant Rhone wine. Polished leather and strawberries. Perhaps a bit dilute, but g G-----G\n"
     ]
    }
   ],
   "source": [
    "for fileid in webtext.fileids():\n",
    "    print(fileid, webtext.raw(fileid)[:100], 'G-----G')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "还有一类即时消息聊天会话，最初是由海军研究生院收集的，用于研究自动检测网络掠食者。 该语料库包含10,000多个帖子，通过用“ UserNNN”形式的通用名称替换用户名来匿名化，并进行手动编辑以删除任何其他标识信息。 语料库分为15个文件，每个文件包含在给定日期收集的数百个帖子，用于特定年龄的聊天室（青少年，20多岁，30多岁，40多岁，以及普通成人聊天室）。 文件名包含日期，聊天室和帖子数； 例如，10-19-20s_706posts.xml包含2006年10月19日从20多岁聊天室收集的706个帖子。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import nps_chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "chatroom = nps_chat.posts('10-19-20s_706posts.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'do',\n",
       " \"n't\",\n",
       " 'want',\n",
       " 'hot',\n",
       " 'pics',\n",
       " 'of',\n",
       " 'a',\n",
       " 'female',\n",
       " ',',\n",
       " 'I',\n",
       " 'can',\n",
       " 'look',\n",
       " 'in',\n",
       " 'a',\n",
       " 'mirror',\n",
       " '.']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chatroom[123]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "布朗语料库是1961年在布朗大学创建的第一个百万字英语英语电子语料库。 该语料库包含来自500个来源的文本，并且这些来源已按类型进行了分类，例如新闻，社论等。 1.1提供了每种流派的示例（有关完整列表，请参见http://icame.uib.no/brown/bcm-los.html）。\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['adventure',\n",
       " 'belles_lettres',\n",
       " 'editorial',\n",
       " 'fiction',\n",
       " 'government',\n",
       " 'hobbies',\n",
       " 'humor',\n",
       " 'learned',\n",
       " 'lore',\n",
       " 'mystery',\n",
       " 'news',\n",
       " 'religion',\n",
       " 'reviews',\n",
       " 'romance',\n",
       " 'science_fiction']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown.categories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', ...]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown.words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The', 'Office', 'of', 'Business', 'Economics', '(', ...]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown.words(categories = 'government')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['American', 'democratic', 'thought', ',', 'pointed', ...]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown.words(fileids = ['cg21'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['In', 'American', 'romance', ',', 'almost', 'nothing', 'rates', 'higher', 'than', 'what', 'the', 'movie', 'men', 'have', 'called', '``', 'meeting', 'cute', \"''\", '--', 'that', 'is', ',', 'boy-meets-girl', 'seems', 'more', 'adorable', 'if', 'it', \"doesn't\", 'take', 'place', 'in', 'an', 'atmosphere', 'of', 'correct', 'and', 'acute', 'boredom', '.'], ['Just', 'about', 'the', 'most', 'enthralling', 'real-life', 'example', 'of', 'meeting', 'cute', 'is', 'the', 'Charles', 'MacArthur-Helen', 'Hayes', 'saga', ':', 'reputedly', 'all', 'he', 'did', 'was', 'give', 'her', 'a', 'handful', 'of', 'peanuts', ',', 'but', 'he', 'said', 'simultaneously', ',', '``', 'I', 'wish', 'they', 'were', 'emeralds', \"''\", '.'], ...]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown.sents(categories = ['fiction','lore','romance'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_text = brown.words(categories='news')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist = nltk.FreqDist(w.lower() for w in news_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "modals = ['can', 'could', 'may', 'might', 'must', 'will']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "end=' ' 是为了让print输出函数把输出结果放到同一行内"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "can: 94    could: 87    may: 93    might: 38    must: 53    will: 389    "
     ]
    }
   ],
   "source": [
    "for m in modals:\n",
    "    print(m + ':' ,fdist[m], end = '    ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "路透社语料库包含10,788个新闻文档，总计130万字。 这些文档已分为90个主题，并分为两组，分别称为“培训”和“测试”。 因此，文件名为“ test / 14826”的文本是从测试集中提取的文档。 此拆分用于培训和测试算法，这些算法会自动检测文档的主题，这将在密集型数据中找到。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import reuters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['test/14826',\n",
       " 'test/14828',\n",
       " 'test/14829',\n",
       " 'test/14832',\n",
       " 'test/14833',\n",
       " 'test/14839',\n",
       " 'test/14840',\n",
       " 'test/14841',\n",
       " 'test/14842',\n",
       " 'test/14843',\n",
       " 'test/14844',\n",
       " 'test/14849',\n",
       " 'test/14852',\n",
       " 'test/14854',\n",
       " 'test/14858',\n",
       " 'test/14859',\n",
       " 'test/14860',\n",
       " 'test/14861',\n",
       " 'test/14862',\n",
       " 'test/14863',\n",
       " 'test/14865',\n",
       " 'test/14867',\n",
       " 'test/14872',\n",
       " 'test/14873',\n",
       " 'test/14875',\n",
       " 'test/14876',\n",
       " 'test/14877',\n",
       " 'test/14881',\n",
       " 'test/14882',\n",
       " 'test/14885',\n",
       " 'test/14886',\n",
       " 'test/14888',\n",
       " 'test/14890',\n",
       " 'test/14891',\n",
       " 'test/14892',\n",
       " 'test/14899',\n",
       " 'test/14900',\n",
       " 'test/14903',\n",
       " 'test/14904',\n",
       " 'test/14907',\n",
       " 'test/14909',\n",
       " 'test/14911',\n",
       " 'test/14912',\n",
       " 'test/14913',\n",
       " 'test/14918',\n",
       " 'test/14919',\n",
       " 'test/14921',\n",
       " 'test/14922',\n",
       " 'test/14923',\n",
       " 'test/14926',\n",
       " 'test/14928',\n",
       " 'test/14930',\n",
       " 'test/14931',\n",
       " 'test/14932',\n",
       " 'test/14933',\n",
       " 'test/14934',\n",
       " 'test/14941',\n",
       " 'test/14943',\n",
       " 'test/14949',\n",
       " 'test/14951',\n",
       " 'test/14954',\n",
       " 'test/14957',\n",
       " 'test/14958',\n",
       " 'test/14959',\n",
       " 'test/14960',\n",
       " 'test/14962',\n",
       " 'test/14963',\n",
       " 'test/14964',\n",
       " 'test/14965',\n",
       " 'test/14967',\n",
       " 'test/14968',\n",
       " 'test/14969',\n",
       " 'test/14970',\n",
       " 'test/14971',\n",
       " 'test/14974',\n",
       " 'test/14975',\n",
       " 'test/14978',\n",
       " 'test/14981',\n",
       " 'test/14982',\n",
       " 'test/14983',\n",
       " 'test/14984',\n",
       " 'test/14985',\n",
       " 'test/14986',\n",
       " 'test/14987',\n",
       " 'test/14988',\n",
       " 'test/14993',\n",
       " 'test/14995',\n",
       " 'test/14998',\n",
       " 'test/15000',\n",
       " 'test/15001',\n",
       " 'test/15002',\n",
       " 'test/15004',\n",
       " 'test/15005',\n",
       " 'test/15006',\n",
       " 'test/15011',\n",
       " 'test/15012',\n",
       " 'test/15013',\n",
       " 'test/15016',\n",
       " 'test/15017',\n",
       " 'test/15020',\n",
       " 'test/15023',\n",
       " 'test/15024',\n",
       " 'test/15026',\n",
       " 'test/15027',\n",
       " 'test/15028',\n",
       " 'test/15029',\n",
       " 'test/15031',\n",
       " 'test/15032',\n",
       " 'test/15033',\n",
       " 'test/15037',\n",
       " 'test/15038',\n",
       " 'test/15043',\n",
       " 'test/15045',\n",
       " 'test/15046',\n",
       " 'test/15048',\n",
       " 'test/15049',\n",
       " 'test/15052',\n",
       " 'test/15053',\n",
       " 'test/15055',\n",
       " 'test/15056',\n",
       " 'test/15060',\n",
       " 'test/15061',\n",
       " 'test/15062',\n",
       " 'test/15063',\n",
       " 'test/15065',\n",
       " 'test/15067',\n",
       " 'test/15069',\n",
       " 'test/15070',\n",
       " 'test/15074',\n",
       " 'test/15077',\n",
       " 'test/15078',\n",
       " 'test/15079',\n",
       " 'test/15082',\n",
       " 'test/15090',\n",
       " 'test/15091',\n",
       " 'test/15092',\n",
       " 'test/15093',\n",
       " 'test/15094',\n",
       " 'test/15095',\n",
       " 'test/15096',\n",
       " 'test/15097',\n",
       " 'test/15103',\n",
       " 'test/15104',\n",
       " 'test/15106',\n",
       " 'test/15107',\n",
       " 'test/15109',\n",
       " 'test/15110',\n",
       " 'test/15111',\n",
       " 'test/15112',\n",
       " 'test/15118',\n",
       " 'test/15119',\n",
       " 'test/15120',\n",
       " 'test/15121',\n",
       " 'test/15122',\n",
       " 'test/15124',\n",
       " 'test/15126',\n",
       " 'test/15128',\n",
       " 'test/15129',\n",
       " 'test/15130',\n",
       " 'test/15132',\n",
       " 'test/15136',\n",
       " 'test/15138',\n",
       " 'test/15141',\n",
       " 'test/15144',\n",
       " 'test/15145',\n",
       " 'test/15146',\n",
       " 'test/15149',\n",
       " 'test/15152',\n",
       " 'test/15153',\n",
       " 'test/15154',\n",
       " 'test/15156',\n",
       " 'test/15157',\n",
       " 'test/15161',\n",
       " 'test/15162',\n",
       " 'test/15171',\n",
       " 'test/15172',\n",
       " 'test/15175',\n",
       " 'test/15179',\n",
       " 'test/15180',\n",
       " 'test/15185',\n",
       " 'test/15188',\n",
       " 'test/15189',\n",
       " 'test/15190',\n",
       " 'test/15193',\n",
       " 'test/15194',\n",
       " 'test/15197',\n",
       " 'test/15198',\n",
       " 'test/15200',\n",
       " 'test/15204',\n",
       " 'test/15205',\n",
       " 'test/15206',\n",
       " 'test/15207',\n",
       " 'test/15208',\n",
       " 'test/15210',\n",
       " 'test/15211',\n",
       " 'test/15212',\n",
       " 'test/15213',\n",
       " 'test/15217',\n",
       " 'test/15219',\n",
       " 'test/15220',\n",
       " 'test/15221',\n",
       " 'test/15222',\n",
       " 'test/15223',\n",
       " 'test/15226',\n",
       " 'test/15227',\n",
       " 'test/15230',\n",
       " 'test/15233',\n",
       " 'test/15234',\n",
       " 'test/15237',\n",
       " 'test/15238',\n",
       " 'test/15239',\n",
       " 'test/15240',\n",
       " 'test/15242',\n",
       " 'test/15243',\n",
       " 'test/15244',\n",
       " 'test/15246',\n",
       " 'test/15247',\n",
       " 'test/15250',\n",
       " 'test/15253',\n",
       " 'test/15254',\n",
       " 'test/15255',\n",
       " 'test/15258',\n",
       " 'test/15259',\n",
       " 'test/15262',\n",
       " 'test/15263',\n",
       " 'test/15264',\n",
       " 'test/15265',\n",
       " 'test/15270',\n",
       " 'test/15271',\n",
       " 'test/15273',\n",
       " 'test/15274',\n",
       " 'test/15276',\n",
       " 'test/15278',\n",
       " 'test/15280',\n",
       " 'test/15281',\n",
       " 'test/15283',\n",
       " 'test/15287',\n",
       " 'test/15290',\n",
       " 'test/15292',\n",
       " 'test/15294',\n",
       " 'test/15295',\n",
       " 'test/15296',\n",
       " 'test/15299',\n",
       " 'test/15300',\n",
       " 'test/15302',\n",
       " 'test/15303',\n",
       " 'test/15306',\n",
       " 'test/15307',\n",
       " 'test/15308',\n",
       " 'test/15309',\n",
       " 'test/15310',\n",
       " 'test/15311',\n",
       " 'test/15312',\n",
       " 'test/15313',\n",
       " 'test/15314',\n",
       " 'test/15315',\n",
       " 'test/15321',\n",
       " 'test/15322',\n",
       " 'test/15324',\n",
       " 'test/15325',\n",
       " 'test/15326',\n",
       " 'test/15327',\n",
       " 'test/15329',\n",
       " 'test/15335',\n",
       " 'test/15336',\n",
       " 'test/15337',\n",
       " 'test/15339',\n",
       " 'test/15341',\n",
       " 'test/15344',\n",
       " 'test/15345',\n",
       " 'test/15348',\n",
       " 'test/15349',\n",
       " 'test/15351',\n",
       " 'test/15352',\n",
       " 'test/15354',\n",
       " 'test/15356',\n",
       " 'test/15357',\n",
       " 'test/15359',\n",
       " 'test/15363',\n",
       " 'test/15364',\n",
       " 'test/15365',\n",
       " 'test/15366',\n",
       " 'test/15367',\n",
       " 'test/15368',\n",
       " 'test/15372',\n",
       " 'test/15375',\n",
       " 'test/15378',\n",
       " 'test/15379',\n",
       " 'test/15380',\n",
       " 'test/15383',\n",
       " 'test/15384',\n",
       " 'test/15386',\n",
       " 'test/15387',\n",
       " 'test/15388',\n",
       " 'test/15389',\n",
       " 'test/15391',\n",
       " 'test/15394',\n",
       " 'test/15396',\n",
       " 'test/15397',\n",
       " 'test/15400',\n",
       " 'test/15404',\n",
       " 'test/15406',\n",
       " 'test/15409',\n",
       " 'test/15410',\n",
       " 'test/15411',\n",
       " 'test/15413',\n",
       " 'test/15415',\n",
       " 'test/15416',\n",
       " 'test/15417',\n",
       " 'test/15420',\n",
       " 'test/15421',\n",
       " 'test/15424',\n",
       " 'test/15425',\n",
       " 'test/15427',\n",
       " 'test/15428',\n",
       " 'test/15429',\n",
       " 'test/15430',\n",
       " 'test/15431',\n",
       " 'test/15432',\n",
       " 'test/15436',\n",
       " 'test/15438',\n",
       " 'test/15441',\n",
       " 'test/15442',\n",
       " 'test/15444',\n",
       " 'test/15446',\n",
       " 'test/15447',\n",
       " 'test/15448',\n",
       " 'test/15449',\n",
       " 'test/15450',\n",
       " 'test/15451',\n",
       " 'test/15452',\n",
       " 'test/15453',\n",
       " 'test/15454',\n",
       " 'test/15455',\n",
       " 'test/15457',\n",
       " 'test/15459',\n",
       " 'test/15460',\n",
       " 'test/15462',\n",
       " 'test/15464',\n",
       " 'test/15467',\n",
       " 'test/15468',\n",
       " 'test/15471',\n",
       " 'test/15472',\n",
       " 'test/15476',\n",
       " 'test/15477',\n",
       " 'test/15478',\n",
       " 'test/15479',\n",
       " 'test/15481',\n",
       " 'test/15482',\n",
       " 'test/15483',\n",
       " 'test/15484',\n",
       " 'test/15485',\n",
       " 'test/15487',\n",
       " 'test/15489',\n",
       " 'test/15494',\n",
       " 'test/15495',\n",
       " 'test/15496',\n",
       " 'test/15500',\n",
       " 'test/15501',\n",
       " 'test/15503',\n",
       " 'test/15504',\n",
       " 'test/15510',\n",
       " 'test/15511',\n",
       " 'test/15515',\n",
       " 'test/15520',\n",
       " 'test/15521',\n",
       " 'test/15522',\n",
       " 'test/15523',\n",
       " 'test/15527',\n",
       " 'test/15528',\n",
       " 'test/15531',\n",
       " 'test/15532',\n",
       " 'test/15535',\n",
       " 'test/15536',\n",
       " 'test/15539',\n",
       " 'test/15540',\n",
       " 'test/15542',\n",
       " 'test/15543',\n",
       " 'test/15544',\n",
       " 'test/15545',\n",
       " 'test/15547',\n",
       " 'test/15548',\n",
       " 'test/15549',\n",
       " 'test/15550',\n",
       " 'test/15551',\n",
       " 'test/15552',\n",
       " 'test/15553',\n",
       " 'test/15556',\n",
       " 'test/15558',\n",
       " 'test/15559',\n",
       " 'test/15560',\n",
       " 'test/15561',\n",
       " 'test/15562',\n",
       " 'test/15563',\n",
       " 'test/15565',\n",
       " 'test/15566',\n",
       " 'test/15567',\n",
       " 'test/15568',\n",
       " 'test/15569',\n",
       " 'test/15570',\n",
       " 'test/15571',\n",
       " 'test/15572',\n",
       " 'test/15573',\n",
       " 'test/15574',\n",
       " 'test/15575',\n",
       " 'test/15578',\n",
       " 'test/15579',\n",
       " 'test/15580',\n",
       " 'test/15581',\n",
       " 'test/15582',\n",
       " 'test/15583',\n",
       " 'test/15584',\n",
       " 'test/15585',\n",
       " 'test/15590',\n",
       " 'test/15591',\n",
       " 'test/15593',\n",
       " 'test/15594',\n",
       " 'test/15595',\n",
       " 'test/15596',\n",
       " 'test/15597',\n",
       " 'test/15598',\n",
       " 'test/15600',\n",
       " 'test/15601',\n",
       " 'test/15602',\n",
       " 'test/15603',\n",
       " 'test/15605',\n",
       " 'test/15607',\n",
       " 'test/15610',\n",
       " 'test/15613',\n",
       " 'test/15615',\n",
       " 'test/15616',\n",
       " 'test/15617',\n",
       " 'test/15618',\n",
       " 'test/15620',\n",
       " 'test/15621',\n",
       " 'test/15623',\n",
       " 'test/15624',\n",
       " 'test/15625',\n",
       " 'test/15626',\n",
       " 'test/15629',\n",
       " 'test/15632',\n",
       " 'test/15634',\n",
       " 'test/15636',\n",
       " 'test/15637',\n",
       " 'test/15639',\n",
       " 'test/15640',\n",
       " 'test/15641',\n",
       " 'test/15642',\n",
       " 'test/15643',\n",
       " 'test/15646',\n",
       " 'test/15648',\n",
       " 'test/15649',\n",
       " 'test/15651',\n",
       " 'test/15653',\n",
       " 'test/15655',\n",
       " 'test/15656',\n",
       " 'test/15664',\n",
       " 'test/15666',\n",
       " 'test/15667',\n",
       " 'test/15668',\n",
       " 'test/15669',\n",
       " 'test/15672',\n",
       " 'test/15674',\n",
       " 'test/15675',\n",
       " 'test/15676',\n",
       " 'test/15677',\n",
       " 'test/15679',\n",
       " 'test/15680',\n",
       " 'test/15682',\n",
       " 'test/15686',\n",
       " 'test/15688',\n",
       " 'test/15689',\n",
       " 'test/15691',\n",
       " 'test/15692',\n",
       " 'test/15694',\n",
       " 'test/15695',\n",
       " 'test/15696',\n",
       " 'test/15698',\n",
       " 'test/15702',\n",
       " 'test/15703',\n",
       " 'test/15704',\n",
       " 'test/15707',\n",
       " 'test/15708',\n",
       " 'test/15709',\n",
       " 'test/15710',\n",
       " 'test/15713',\n",
       " 'test/15715',\n",
       " 'test/15717',\n",
       " 'test/15719',\n",
       " 'test/15720',\n",
       " 'test/15721',\n",
       " 'test/15723',\n",
       " 'test/15725',\n",
       " 'test/15726',\n",
       " 'test/15727',\n",
       " 'test/15728',\n",
       " 'test/15729',\n",
       " 'test/15732',\n",
       " 'test/15733',\n",
       " 'test/15736',\n",
       " 'test/15737',\n",
       " 'test/15739',\n",
       " 'test/15742',\n",
       " 'test/15749',\n",
       " 'test/15751',\n",
       " 'test/15753',\n",
       " 'test/15757',\n",
       " 'test/15759',\n",
       " 'test/15762',\n",
       " 'test/15767',\n",
       " 'test/15768',\n",
       " 'test/15769',\n",
       " 'test/15772',\n",
       " 'test/15777',\n",
       " 'test/15778',\n",
       " 'test/15780',\n",
       " 'test/15782',\n",
       " 'test/15785',\n",
       " 'test/15790',\n",
       " 'test/15793',\n",
       " 'test/15797',\n",
       " 'test/15798',\n",
       " 'test/15800',\n",
       " 'test/15801',\n",
       " 'test/15803',\n",
       " 'test/15804',\n",
       " 'test/15805',\n",
       " 'test/15807',\n",
       " 'test/15808',\n",
       " 'test/15810',\n",
       " 'test/15811',\n",
       " 'test/15816',\n",
       " 'test/15817',\n",
       " 'test/15819',\n",
       " 'test/15821',\n",
       " 'test/15822',\n",
       " 'test/15823',\n",
       " 'test/15829',\n",
       " 'test/15831',\n",
       " 'test/15832',\n",
       " 'test/15833',\n",
       " 'test/15834',\n",
       " 'test/15836',\n",
       " 'test/15838',\n",
       " 'test/15840',\n",
       " 'test/15841',\n",
       " 'test/15842',\n",
       " 'test/15844',\n",
       " 'test/15845',\n",
       " 'test/15846',\n",
       " 'test/15847',\n",
       " 'test/15851',\n",
       " 'test/15852',\n",
       " 'test/15853',\n",
       " 'test/15854',\n",
       " 'test/15855',\n",
       " 'test/15856',\n",
       " 'test/15858',\n",
       " 'test/15859',\n",
       " 'test/15860',\n",
       " 'test/15861',\n",
       " 'test/15863',\n",
       " 'test/15864',\n",
       " 'test/15865',\n",
       " 'test/15866',\n",
       " 'test/15867',\n",
       " 'test/15868',\n",
       " 'test/15869',\n",
       " 'test/15870',\n",
       " 'test/15871',\n",
       " 'test/15872',\n",
       " 'test/15874',\n",
       " 'test/15875',\n",
       " 'test/15876',\n",
       " 'test/15877',\n",
       " 'test/15878',\n",
       " 'test/15879',\n",
       " 'test/15881',\n",
       " 'test/15885',\n",
       " 'test/15886',\n",
       " 'test/15888',\n",
       " 'test/15889',\n",
       " 'test/15890',\n",
       " 'test/15892',\n",
       " 'test/15893',\n",
       " 'test/15894',\n",
       " 'test/15895',\n",
       " 'test/15896',\n",
       " 'test/15897',\n",
       " 'test/15898',\n",
       " 'test/15899',\n",
       " 'test/15900',\n",
       " 'test/15901',\n",
       " 'test/15902',\n",
       " 'test/15903',\n",
       " 'test/15904',\n",
       " 'test/15906',\n",
       " 'test/15908',\n",
       " 'test/15909',\n",
       " 'test/15910',\n",
       " 'test/15911',\n",
       " 'test/15912',\n",
       " 'test/15913',\n",
       " 'test/15914',\n",
       " 'test/15916',\n",
       " 'test/15917',\n",
       " 'test/15918',\n",
       " 'test/15920',\n",
       " 'test/15921',\n",
       " 'test/15922',\n",
       " 'test/15923',\n",
       " 'test/15924',\n",
       " 'test/15925',\n",
       " 'test/15927',\n",
       " 'test/15928',\n",
       " 'test/15929',\n",
       " 'test/15930',\n",
       " 'test/15932',\n",
       " 'test/15933',\n",
       " 'test/15934',\n",
       " 'test/15937',\n",
       " 'test/15939',\n",
       " 'test/15942',\n",
       " 'test/15944',\n",
       " 'test/15949',\n",
       " 'test/15950',\n",
       " 'test/15951',\n",
       " 'test/15952',\n",
       " 'test/15953',\n",
       " 'test/15956',\n",
       " 'test/15959',\n",
       " 'test/15960',\n",
       " 'test/15961',\n",
       " 'test/15963',\n",
       " 'test/15964',\n",
       " 'test/15967',\n",
       " 'test/15968',\n",
       " 'test/15969',\n",
       " 'test/15970',\n",
       " 'test/15973',\n",
       " 'test/15975',\n",
       " 'test/15976',\n",
       " 'test/15977',\n",
       " 'test/15978',\n",
       " 'test/15979',\n",
       " 'test/15980',\n",
       " 'test/15981',\n",
       " 'test/15984',\n",
       " 'test/15985',\n",
       " 'test/15987',\n",
       " 'test/15988',\n",
       " 'test/15989',\n",
       " 'test/15993',\n",
       " 'test/15995',\n",
       " 'test/15996',\n",
       " 'test/15997',\n",
       " 'test/15999',\n",
       " 'test/16002',\n",
       " 'test/16003',\n",
       " 'test/16004',\n",
       " 'test/16005',\n",
       " 'test/16006',\n",
       " 'test/16007',\n",
       " 'test/16009',\n",
       " 'test/16012',\n",
       " 'test/16013',\n",
       " 'test/16014',\n",
       " 'test/16015',\n",
       " 'test/16016',\n",
       " 'test/16021',\n",
       " 'test/16022',\n",
       " 'test/16023',\n",
       " 'test/16026',\n",
       " 'test/16029',\n",
       " 'test/16030',\n",
       " 'test/16033',\n",
       " 'test/16037',\n",
       " 'test/16040',\n",
       " 'test/16041',\n",
       " 'test/16045',\n",
       " 'test/16052',\n",
       " 'test/16053',\n",
       " 'test/16055',\n",
       " 'test/16063',\n",
       " 'test/16066',\n",
       " 'test/16067',\n",
       " 'test/16068',\n",
       " 'test/16069',\n",
       " 'test/16071',\n",
       " 'test/16072',\n",
       " 'test/16074',\n",
       " 'test/16075',\n",
       " 'test/16076',\n",
       " 'test/16077',\n",
       " 'test/16079',\n",
       " 'test/16080',\n",
       " 'test/16083',\n",
       " 'test/16086',\n",
       " 'test/16088',\n",
       " 'test/16091',\n",
       " 'test/16093',\n",
       " 'test/16094',\n",
       " 'test/16095',\n",
       " 'test/16096',\n",
       " 'test/16097',\n",
       " 'test/16098',\n",
       " 'test/16099',\n",
       " 'test/16100',\n",
       " 'test/16103',\n",
       " 'test/16106',\n",
       " 'test/16107',\n",
       " 'test/16108',\n",
       " 'test/16110',\n",
       " 'test/16111',\n",
       " 'test/16112',\n",
       " 'test/16115',\n",
       " 'test/16117',\n",
       " 'test/16118',\n",
       " 'test/16119',\n",
       " 'test/16120',\n",
       " 'test/16122',\n",
       " 'test/16123',\n",
       " 'test/16125',\n",
       " 'test/16126',\n",
       " 'test/16130',\n",
       " 'test/16133',\n",
       " 'test/16134',\n",
       " 'test/16136',\n",
       " 'test/16139',\n",
       " 'test/16140',\n",
       " 'test/16141',\n",
       " 'test/16142',\n",
       " 'test/16143',\n",
       " 'test/16144',\n",
       " 'test/16145',\n",
       " 'test/16146',\n",
       " 'test/16147',\n",
       " 'test/16148',\n",
       " 'test/16149',\n",
       " 'test/16150',\n",
       " 'test/16152',\n",
       " 'test/16155',\n",
       " 'test/16158',\n",
       " 'test/16159',\n",
       " 'test/16161',\n",
       " 'test/16162',\n",
       " 'test/16163',\n",
       " 'test/16164',\n",
       " 'test/16166',\n",
       " 'test/16170',\n",
       " 'test/16171',\n",
       " 'test/16172',\n",
       " 'test/16173',\n",
       " 'test/16175',\n",
       " 'test/16176',\n",
       " 'test/16177',\n",
       " 'test/16179',\n",
       " 'test/16180',\n",
       " 'test/16185',\n",
       " 'test/16188',\n",
       " 'test/16189',\n",
       " 'test/16190',\n",
       " 'test/16193',\n",
       " 'test/16194',\n",
       " 'test/16195',\n",
       " 'test/16196',\n",
       " 'test/16197',\n",
       " 'test/16200',\n",
       " 'test/16201',\n",
       " 'test/16202',\n",
       " 'test/16203',\n",
       " 'test/16206',\n",
       " 'test/16207',\n",
       " 'test/16210',\n",
       " 'test/16211',\n",
       " 'test/16212',\n",
       " 'test/16213',\n",
       " 'test/16214',\n",
       " 'test/16215',\n",
       " 'test/16216',\n",
       " 'test/16219',\n",
       " 'test/16221',\n",
       " 'test/16223',\n",
       " 'test/16225',\n",
       " 'test/16226',\n",
       " 'test/16228',\n",
       " 'test/16230',\n",
       " 'test/16232',\n",
       " 'test/16233',\n",
       " 'test/16234',\n",
       " 'test/16236',\n",
       " 'test/16238',\n",
       " 'test/16241',\n",
       " 'test/16243',\n",
       " 'test/16244',\n",
       " 'test/16246',\n",
       " 'test/16247',\n",
       " 'test/16248',\n",
       " 'test/16250',\n",
       " 'test/16251',\n",
       " 'test/16252',\n",
       " 'test/16255',\n",
       " 'test/16256',\n",
       " 'test/16257',\n",
       " 'test/16258',\n",
       " 'test/16260',\n",
       " 'test/16262',\n",
       " 'test/16263',\n",
       " 'test/16264',\n",
       " 'test/16265',\n",
       " 'test/16266',\n",
       " 'test/16268',\n",
       " 'test/16269',\n",
       " 'test/16270',\n",
       " 'test/16271',\n",
       " 'test/16274',\n",
       " 'test/16275',\n",
       " 'test/16277',\n",
       " 'test/16278',\n",
       " 'test/16279',\n",
       " 'test/16281',\n",
       " 'test/16282',\n",
       " 'test/16283',\n",
       " 'test/16284',\n",
       " 'test/16285',\n",
       " 'test/16286',\n",
       " 'test/16287',\n",
       " 'test/16288',\n",
       " 'test/16289',\n",
       " 'test/16291',\n",
       " 'test/16294',\n",
       " 'test/16297',\n",
       " 'test/16298',\n",
       " 'test/16299',\n",
       " 'test/16300',\n",
       " 'test/16301',\n",
       " 'test/16302',\n",
       " 'test/16303',\n",
       " 'test/16304',\n",
       " 'test/16307',\n",
       " 'test/16310',\n",
       " 'test/16311',\n",
       " 'test/16312',\n",
       " 'test/16314',\n",
       " 'test/16315',\n",
       " 'test/16316',\n",
       " 'test/16317',\n",
       " 'test/16318',\n",
       " 'test/16319',\n",
       " 'test/16320',\n",
       " 'test/16324',\n",
       " 'test/16327',\n",
       " 'test/16331',\n",
       " 'test/16332',\n",
       " 'test/16336',\n",
       " 'test/16337',\n",
       " 'test/16339',\n",
       " 'test/16342',\n",
       " 'test/16343',\n",
       " 'test/16346',\n",
       " 'test/16347',\n",
       " 'test/16348',\n",
       " 'test/16350',\n",
       " 'test/16354',\n",
       " 'test/16357',\n",
       " 'test/16359',\n",
       " 'test/16360',\n",
       " 'test/16362',\n",
       " 'test/16363',\n",
       " 'test/16365',\n",
       " 'test/16366',\n",
       " 'test/16367',\n",
       " 'test/16369',\n",
       " 'test/16370',\n",
       " 'test/16371',\n",
       " 'test/16372',\n",
       " 'test/16374',\n",
       " 'test/16376',\n",
       " 'test/16377',\n",
       " 'test/16379',\n",
       " 'test/16380',\n",
       " 'test/16383',\n",
       " 'test/16385',\n",
       " 'test/16386',\n",
       " 'test/16388',\n",
       " 'test/16390',\n",
       " 'test/16392',\n",
       " 'test/16393',\n",
       " 'test/16394',\n",
       " 'test/16395',\n",
       " 'test/16396',\n",
       " 'test/16398',\n",
       " 'test/16399',\n",
       " 'test/16400',\n",
       " 'test/16401',\n",
       " 'test/16402',\n",
       " 'test/16403',\n",
       " 'test/16404',\n",
       " 'test/16405',\n",
       " 'test/16406',\n",
       " 'test/16407',\n",
       " 'test/16409',\n",
       " 'test/16410',\n",
       " 'test/16415',\n",
       " 'test/16417',\n",
       " 'test/16418',\n",
       " 'test/16419',\n",
       " 'test/16420',\n",
       " 'test/16421',\n",
       " 'test/16422',\n",
       " 'test/16424',\n",
       " 'test/16426',\n",
       " 'test/16427',\n",
       " 'test/16428',\n",
       " 'test/16429',\n",
       " 'test/16430',\n",
       " 'test/16432',\n",
       " 'test/16433',\n",
       " 'test/16434',\n",
       " 'test/16437',\n",
       " 'test/16438',\n",
       " 'test/16440',\n",
       " 'test/16441',\n",
       " 'test/16442',\n",
       " 'test/16443',\n",
       " 'test/16444',\n",
       " 'test/16448',\n",
       " 'test/16449',\n",
       " 'test/16450',\n",
       " 'test/16454',\n",
       " 'test/16457',\n",
       " 'test/16458',\n",
       " 'test/16459',\n",
       " 'test/16460',\n",
       " 'test/16461',\n",
       " 'test/16463',\n",
       " 'test/16465',\n",
       " 'test/16468',\n",
       " 'test/16469',\n",
       " 'test/16470',\n",
       " 'test/16471',\n",
       " 'test/16472',\n",
       " 'test/16473',\n",
       " 'test/16475',\n",
       " 'test/16476',\n",
       " 'test/16478',\n",
       " 'test/16479',\n",
       " 'test/16480',\n",
       " 'test/16481',\n",
       " 'test/16483',\n",
       " 'test/16486',\n",
       " 'test/16487',\n",
       " 'test/16488',\n",
       " 'test/16490',\n",
       " 'test/16492',\n",
       " 'test/16493',\n",
       " 'test/16495',\n",
       " 'test/16496',\n",
       " 'test/16499',\n",
       " 'test/16502',\n",
       " 'test/16505',\n",
       " 'test/16510',\n",
       " 'test/16512',\n",
       " 'test/16513',\n",
       " 'test/16518',\n",
       " 'test/16519',\n",
       " 'test/16521',\n",
       " 'test/16522',\n",
       " 'test/16523',\n",
       " 'test/16525',\n",
       " 'test/16527',\n",
       " 'test/16530',\n",
       " 'test/16531',\n",
       " 'test/16533',\n",
       " 'test/16538',\n",
       " 'test/16539',\n",
       " 'test/16545',\n",
       " 'test/16546',\n",
       " 'test/16549',\n",
       " 'test/16551',\n",
       " 'test/16554',\n",
       " 'test/16555',\n",
       " 'test/16561',\n",
       " 'test/16563',\n",
       " 'test/16564',\n",
       " 'test/16565',\n",
       " 'test/16568',\n",
       " 'test/16569',\n",
       " 'test/16570',\n",
       " 'test/16574',\n",
       " 'test/16577',\n",
       " 'test/16581',\n",
       " 'test/16583',\n",
       " 'test/16584',\n",
       " 'test/16585',\n",
       " 'test/16587',\n",
       " 'test/16588',\n",
       " 'test/16589',\n",
       " 'test/16590',\n",
       " 'test/16591',\n",
       " ...]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reuters.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['acq',\n",
       " 'alum',\n",
       " 'barley',\n",
       " 'bop',\n",
       " 'carcass',\n",
       " 'castor-oil',\n",
       " 'cocoa',\n",
       " 'coconut',\n",
       " 'coconut-oil',\n",
       " 'coffee',\n",
       " 'copper',\n",
       " 'copra-cake',\n",
       " 'corn',\n",
       " 'cotton',\n",
       " 'cotton-oil',\n",
       " 'cpi',\n",
       " 'cpu',\n",
       " 'crude',\n",
       " 'dfl',\n",
       " 'dlr',\n",
       " 'dmk',\n",
       " 'earn',\n",
       " 'fuel',\n",
       " 'gas',\n",
       " 'gnp',\n",
       " 'gold',\n",
       " 'grain',\n",
       " 'groundnut',\n",
       " 'groundnut-oil',\n",
       " 'heat',\n",
       " 'hog',\n",
       " 'housing',\n",
       " 'income',\n",
       " 'instal-debt',\n",
       " 'interest',\n",
       " 'ipi',\n",
       " 'iron-steel',\n",
       " 'jet',\n",
       " 'jobs',\n",
       " 'l-cattle',\n",
       " 'lead',\n",
       " 'lei',\n",
       " 'lin-oil',\n",
       " 'livestock',\n",
       " 'lumber',\n",
       " 'meal-feed',\n",
       " 'money-fx',\n",
       " 'money-supply',\n",
       " 'naphtha',\n",
       " 'nat-gas',\n",
       " 'nickel',\n",
       " 'nkr',\n",
       " 'nzdlr',\n",
       " 'oat',\n",
       " 'oilseed',\n",
       " 'orange',\n",
       " 'palladium',\n",
       " 'palm-oil',\n",
       " 'palmkernel',\n",
       " 'pet-chem',\n",
       " 'platinum',\n",
       " 'potato',\n",
       " 'propane',\n",
       " 'rand',\n",
       " 'rape-oil',\n",
       " 'rapeseed',\n",
       " 'reserves',\n",
       " 'retail',\n",
       " 'rice',\n",
       " 'rubber',\n",
       " 'rye',\n",
       " 'ship',\n",
       " 'silver',\n",
       " 'sorghum',\n",
       " 'soy-meal',\n",
       " 'soy-oil',\n",
       " 'soybean',\n",
       " 'strategic-metal',\n",
       " 'sugar',\n",
       " 'sun-meal',\n",
       " 'sun-oil',\n",
       " 'sunseed',\n",
       " 'tea',\n",
       " 'tin',\n",
       " 'trade',\n",
       " 'veg-oil',\n",
       " 'wheat',\n",
       " 'wpi',\n",
       " 'yen',\n",
       " 'zinc']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reuters.categories()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reuters的类别与brown不同，他是存在overlap的，因为路透社的新闻大多是能覆盖多个topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['test/15618',\n",
       " 'test/15649',\n",
       " 'test/15676',\n",
       " 'test/15728',\n",
       " 'test/15871',\n",
       " 'test/15875',\n",
       " 'test/15952',\n",
       " 'test/17767',\n",
       " 'test/17769',\n",
       " 'test/18024',\n",
       " 'test/18263',\n",
       " 'test/18908',\n",
       " 'test/19275',\n",
       " 'test/19668',\n",
       " 'training/10175',\n",
       " 'training/1067',\n",
       " 'training/11208',\n",
       " 'training/11316',\n",
       " 'training/11885',\n",
       " 'training/12428',\n",
       " 'training/13099',\n",
       " 'training/13744',\n",
       " 'training/13795',\n",
       " 'training/13852',\n",
       " 'training/13856',\n",
       " 'training/1652',\n",
       " 'training/1970',\n",
       " 'training/2044',\n",
       " 'training/2171',\n",
       " 'training/2172',\n",
       " 'training/2191',\n",
       " 'training/2217',\n",
       " 'training/2232',\n",
       " 'training/3132',\n",
       " 'training/3324',\n",
       " 'training/395',\n",
       " 'training/4280',\n",
       " 'training/4296',\n",
       " 'training/5',\n",
       " 'training/501',\n",
       " 'training/5467',\n",
       " 'training/5610',\n",
       " 'training/5640',\n",
       " 'training/6626',\n",
       " 'training/7205',\n",
       " 'training/7579',\n",
       " 'training/8213',\n",
       " 'training/8257',\n",
       " 'training/8759',\n",
       " 'training/9865',\n",
       " 'training/9958']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reuters.fileids('barley')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['test/14832',\n",
       " 'test/14858',\n",
       " 'test/15033',\n",
       " 'test/15043',\n",
       " 'test/15106',\n",
       " 'test/15287',\n",
       " 'test/15341',\n",
       " 'test/15618',\n",
       " 'test/15648',\n",
       " 'test/15649',\n",
       " 'test/15676',\n",
       " 'test/15686',\n",
       " 'test/15720',\n",
       " 'test/15728',\n",
       " 'test/15845',\n",
       " 'test/15856',\n",
       " 'test/15860',\n",
       " 'test/15863',\n",
       " 'test/15871',\n",
       " 'test/15875',\n",
       " 'test/15877',\n",
       " 'test/15890',\n",
       " 'test/15904',\n",
       " 'test/15906',\n",
       " 'test/15910',\n",
       " 'test/15911',\n",
       " 'test/15917',\n",
       " 'test/15952',\n",
       " 'test/15999',\n",
       " 'test/16012',\n",
       " 'test/16071',\n",
       " 'test/16099',\n",
       " 'test/16147',\n",
       " 'test/16525',\n",
       " 'test/16624',\n",
       " 'test/16751',\n",
       " 'test/16765',\n",
       " 'test/17503',\n",
       " 'test/17509',\n",
       " 'test/17722',\n",
       " 'test/17767',\n",
       " 'test/17769',\n",
       " 'test/18024',\n",
       " 'test/18035',\n",
       " 'test/18263',\n",
       " 'test/18482',\n",
       " 'test/18614',\n",
       " 'test/18908',\n",
       " 'test/18954',\n",
       " 'test/18973',\n",
       " 'test/19165',\n",
       " 'test/19275',\n",
       " 'test/19668',\n",
       " 'test/19721',\n",
       " 'test/19821',\n",
       " 'test/20018',\n",
       " 'test/20366',\n",
       " 'test/20637',\n",
       " 'test/20645',\n",
       " 'test/20649',\n",
       " 'test/20723',\n",
       " 'test/20763',\n",
       " 'test/21091',\n",
       " 'test/21243',\n",
       " 'test/21493',\n",
       " 'training/10120',\n",
       " 'training/10139',\n",
       " 'training/10172',\n",
       " 'training/10175',\n",
       " 'training/10319',\n",
       " 'training/10339',\n",
       " 'training/10487',\n",
       " 'training/10489',\n",
       " 'training/10519',\n",
       " 'training/1067',\n",
       " 'training/10701',\n",
       " 'training/10882',\n",
       " 'training/10956',\n",
       " 'training/11012',\n",
       " 'training/11085',\n",
       " 'training/11091',\n",
       " 'training/11208',\n",
       " 'training/11269',\n",
       " 'training/1131',\n",
       " 'training/11316',\n",
       " 'training/11392',\n",
       " 'training/11436',\n",
       " 'training/11607',\n",
       " 'training/11612',\n",
       " 'training/11729',\n",
       " 'training/11739',\n",
       " 'training/11769',\n",
       " 'training/11885',\n",
       " 'training/11936',\n",
       " 'training/11939',\n",
       " 'training/11964',\n",
       " 'training/12002',\n",
       " 'training/12052',\n",
       " 'training/12055',\n",
       " 'training/1215',\n",
       " 'training/12160',\n",
       " 'training/12311',\n",
       " 'training/12323',\n",
       " 'training/12372',\n",
       " 'training/12417',\n",
       " 'training/12428',\n",
       " 'training/12436',\n",
       " 'training/12500',\n",
       " 'training/12583',\n",
       " 'training/12587',\n",
       " 'training/1268',\n",
       " 'training/1273',\n",
       " 'training/12872',\n",
       " 'training/13099',\n",
       " 'training/13173',\n",
       " 'training/13179',\n",
       " 'training/1369',\n",
       " 'training/13744',\n",
       " 'training/13795',\n",
       " 'training/1385',\n",
       " 'training/13852',\n",
       " 'training/13856',\n",
       " 'training/1395',\n",
       " 'training/1399',\n",
       " 'training/14483',\n",
       " 'training/1582',\n",
       " 'training/1652',\n",
       " 'training/1777',\n",
       " 'training/1843',\n",
       " 'training/193',\n",
       " 'training/1952',\n",
       " 'training/197',\n",
       " 'training/1970',\n",
       " 'training/2044',\n",
       " 'training/2171',\n",
       " 'training/2172',\n",
       " 'training/2183',\n",
       " 'training/2191',\n",
       " 'training/2217',\n",
       " 'training/2232',\n",
       " 'training/2264',\n",
       " 'training/235',\n",
       " 'training/2382',\n",
       " 'training/2436',\n",
       " 'training/2456',\n",
       " 'training/2595',\n",
       " 'training/2599',\n",
       " 'training/2617',\n",
       " 'training/2727',\n",
       " 'training/2741',\n",
       " 'training/2749',\n",
       " 'training/2777',\n",
       " 'training/2848',\n",
       " 'training/2913',\n",
       " 'training/2922',\n",
       " 'training/2947',\n",
       " 'training/3132',\n",
       " 'training/3138',\n",
       " 'training/3191',\n",
       " 'training/327',\n",
       " 'training/3282',\n",
       " 'training/3299',\n",
       " 'training/3306',\n",
       " 'training/3324',\n",
       " 'training/3330',\n",
       " 'training/3337',\n",
       " 'training/3358',\n",
       " 'training/3401',\n",
       " 'training/3429',\n",
       " 'training/3847',\n",
       " 'training/3855',\n",
       " 'training/3881',\n",
       " 'training/3949',\n",
       " 'training/395',\n",
       " 'training/3979',\n",
       " 'training/3981',\n",
       " 'training/4047',\n",
       " 'training/4133',\n",
       " 'training/4280',\n",
       " 'training/4289',\n",
       " 'training/4296',\n",
       " 'training/4382',\n",
       " 'training/4490',\n",
       " 'training/4599',\n",
       " 'training/4825',\n",
       " 'training/4905',\n",
       " 'training/4939',\n",
       " 'training/4988',\n",
       " 'training/5',\n",
       " 'training/5003',\n",
       " 'training/501',\n",
       " 'training/5017',\n",
       " 'training/5033',\n",
       " 'training/5109',\n",
       " 'training/516',\n",
       " 'training/5185',\n",
       " 'training/5338',\n",
       " 'training/5467',\n",
       " 'training/5518',\n",
       " 'training/5531',\n",
       " 'training/5606',\n",
       " 'training/5610',\n",
       " 'training/5636',\n",
       " 'training/5637',\n",
       " 'training/5640',\n",
       " 'training/57',\n",
       " 'training/5847',\n",
       " 'training/5933',\n",
       " 'training/6',\n",
       " 'training/6142',\n",
       " 'training/6221',\n",
       " 'training/6236',\n",
       " 'training/6239',\n",
       " 'training/6259',\n",
       " 'training/6269',\n",
       " 'training/6386',\n",
       " 'training/6585',\n",
       " 'training/6588',\n",
       " 'training/6626',\n",
       " 'training/6735',\n",
       " 'training/6890',\n",
       " 'training/6897',\n",
       " 'training/694',\n",
       " 'training/7062',\n",
       " 'training/7205',\n",
       " 'training/7215',\n",
       " 'training/7336',\n",
       " 'training/7387',\n",
       " 'training/7389',\n",
       " 'training/7390',\n",
       " 'training/7395',\n",
       " 'training/7579',\n",
       " 'training/7700',\n",
       " 'training/7792',\n",
       " 'training/7917',\n",
       " 'training/7934',\n",
       " 'training/7943',\n",
       " 'training/8004',\n",
       " 'training/8140',\n",
       " 'training/8161',\n",
       " 'training/8166',\n",
       " 'training/8213',\n",
       " 'training/8257',\n",
       " 'training/8273',\n",
       " 'training/8400',\n",
       " 'training/8443',\n",
       " 'training/8446',\n",
       " 'training/8535',\n",
       " 'training/855',\n",
       " 'training/8759',\n",
       " 'training/8941',\n",
       " 'training/8983',\n",
       " 'training/8993',\n",
       " 'training/9058',\n",
       " 'training/9093',\n",
       " 'training/9094',\n",
       " 'training/934',\n",
       " 'training/9470',\n",
       " 'training/9521',\n",
       " 'training/9667',\n",
       " 'training/97',\n",
       " 'training/9865',\n",
       " 'training/9958',\n",
       " 'training/9989']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reuters.fileids(['barley','corn'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['barley', 'corn', 'grain', 'wheat']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reuters.categories('training/9865')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['FRENCH', 'FREE', 'MARKET', 'CEREAL', 'EXPORT', ...]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reuters.words('training/9865')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['FRENCH',\n",
       " 'FREE',\n",
       " 'MARKET',\n",
       " 'CEREAL',\n",
       " 'EXPORT',\n",
       " 'BIDS',\n",
       " 'DETAILED',\n",
       " 'French',\n",
       " 'operators',\n",
       " 'have',\n",
       " 'requested',\n",
       " 'licences',\n",
       " 'to',\n",
       " 'export']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reuters.words('training/9865')[:14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['barley', 'corn', 'grain', 'money-fx', 'wheat']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reuters.categories(['training/9865','training/9880'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['FRENCH', 'FREE', 'MARKET', 'CEREAL', 'EXPORT', ...]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reuters.words(['training/9865','training/9880'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['FRENCH', 'FREE', 'MARKET', 'CEREAL', 'EXPORT', ...]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reuters.words(categories = 'barley')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In 1, we looked at the Inaugural Address Corpus, but treated it as a single text. The graph in fig-inaugural used \"word offset\" as one of the axes; this is the numerical index of the word in the corpus, counting from the first word of the first address. However, the corpus is actually a collection of 55 texts, one for each presidential address. An interesting property of this collection is its time dimension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import inaugural"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1789-Washington.txt',\n",
       " '1793-Washington.txt',\n",
       " '1797-Adams.txt',\n",
       " '1801-Jefferson.txt',\n",
       " '1805-Jefferson.txt',\n",
       " '1809-Madison.txt',\n",
       " '1813-Madison.txt',\n",
       " '1817-Monroe.txt',\n",
       " '1821-Monroe.txt',\n",
       " '1825-Adams.txt',\n",
       " '1829-Jackson.txt',\n",
       " '1833-Jackson.txt',\n",
       " '1837-VanBuren.txt',\n",
       " '1841-Harrison.txt',\n",
       " '1845-Polk.txt',\n",
       " '1849-Taylor.txt',\n",
       " '1853-Pierce.txt',\n",
       " '1857-Buchanan.txt',\n",
       " '1861-Lincoln.txt',\n",
       " '1865-Lincoln.txt',\n",
       " '1869-Grant.txt',\n",
       " '1873-Grant.txt',\n",
       " '1877-Hayes.txt',\n",
       " '1881-Garfield.txt',\n",
       " '1885-Cleveland.txt',\n",
       " '1889-Harrison.txt',\n",
       " '1893-Cleveland.txt',\n",
       " '1897-McKinley.txt',\n",
       " '1901-McKinley.txt',\n",
       " '1905-Roosevelt.txt',\n",
       " '1909-Taft.txt',\n",
       " '1913-Wilson.txt',\n",
       " '1917-Wilson.txt',\n",
       " '1921-Harding.txt',\n",
       " '1925-Coolidge.txt',\n",
       " '1929-Hoover.txt',\n",
       " '1933-Roosevelt.txt',\n",
       " '1937-Roosevelt.txt',\n",
       " '1941-Roosevelt.txt',\n",
       " '1945-Roosevelt.txt',\n",
       " '1949-Truman.txt',\n",
       " '1953-Eisenhower.txt',\n",
       " '1957-Eisenhower.txt',\n",
       " '1961-Kennedy.txt',\n",
       " '1965-Johnson.txt',\n",
       " '1969-Nixon.txt',\n",
       " '1973-Nixon.txt',\n",
       " '1977-Carter.txt',\n",
       " '1981-Reagan.txt',\n",
       " '1985-Reagan.txt',\n",
       " '1989-Bush.txt',\n",
       " '1993-Clinton.txt',\n",
       " '1997-Clinton.txt',\n",
       " '2001-Bush.txt',\n",
       " '2005-Bush.txt',\n",
       " '2009-Obama.txt',\n",
       " '2013-Obama.txt',\n",
       " '2017-Trump.txt']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inaugural.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1789',\n",
       " '1793',\n",
       " '1797',\n",
       " '1801',\n",
       " '1805',\n",
       " '1809',\n",
       " '1813',\n",
       " '1817',\n",
       " '1821',\n",
       " '1825',\n",
       " '1829',\n",
       " '1833',\n",
       " '1837',\n",
       " '1841',\n",
       " '1845',\n",
       " '1849',\n",
       " '1853',\n",
       " '1857',\n",
       " '1861',\n",
       " '1865',\n",
       " '1869',\n",
       " '1873',\n",
       " '1877',\n",
       " '1881',\n",
       " '1885',\n",
       " '1889',\n",
       " '1893',\n",
       " '1897',\n",
       " '1901',\n",
       " '1905',\n",
       " '1909',\n",
       " '1913',\n",
       " '1917',\n",
       " '1921',\n",
       " '1925',\n",
       " '1929',\n",
       " '1933',\n",
       " '1937',\n",
       " '1941',\n",
       " '1945',\n",
       " '1949',\n",
       " '1953',\n",
       " '1957',\n",
       " '1961',\n",
       " '1965',\n",
       " '1969',\n",
       " '1973',\n",
       " '1977',\n",
       " '1981',\n",
       " '1985',\n",
       " '1989',\n",
       " '1993',\n",
       " '1997',\n",
       " '2001',\n",
       " '2005',\n",
       " '2009',\n",
       " '2013',\n",
       " '2017']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[fileid[:4] for fileid in inaugural.fileids()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import ConditionalFreqDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfd = ConditionalFreqDist(\n",
    "    (target,fileid[:4])\n",
    "    for fileid in inaugural.fileids()\n",
    "    for w in inaugural.words()\n",
    "    for target in ['america','citizen']\n",
    "    if w.lower().startswith(target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1274e5290>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfd.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "带注释的文本语料库\n",
    "\n",
    "许多文本语料库包含语言注释，它们表示POS标签，命名实体，句法结构，语义角色等。 NLTK提供了访问其中一些语料库的便捷方法，并且具有包含语料库和语料库样本的数据包，可免费下载以用于教学和研究。 1.2列出了一些语料库。 有关下载它们的信息，请参见http://nltk.org/data。 有关如何访问NLTK语料库的更多示例，请访问http://nltk.org/howto的语料库HOWTO。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "：外语语料库：这些语料库中的最后一个语种是udhr，其中包含300多种语言的《世界人权宣言》。 该语料库的文件ID包含有关文件中使用的字符编码的信息，例如UTF8或Latin1。 让我们使用条件频率分布来检查udhr语料库中包括的多种语言的单词长度差异。 输出显示在1.2中（自己运行程序以查看颜色图）。 请注意，True和False是Python的内置布尔值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['El',\n",
       " 'grupo',\n",
       " 'estatal',\n",
       " 'Electricité_de_France',\n",
       " '-Fpa-',\n",
       " 'EDF',\n",
       " '-Fpt-',\n",
       " 'anunció',\n",
       " 'hoy',\n",
       " ',',\n",
       " 'jueves',\n",
       " ',',\n",
       " 'la',\n",
       " 'compra',\n",
       " 'del']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.corpus.cess_esp.words()[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Um', 'revivalismo', 'refrescante', 'O', '7_e_Meio', ...]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.corpus.floresta.words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['মহিষের', 'সন্তান', ':', 'তোড়া', 'উপজাতি', '৷', ...]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.corpus.indian.words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Abkhaz-Cyrillic+Abkh',\n",
       " 'Abkhaz-UTF8',\n",
       " 'Achehnese-Latin1',\n",
       " 'Achuar-Shiwiar-Latin1',\n",
       " 'Adja-UTF8',\n",
       " 'Afaan_Oromo_Oromiffa-Latin1',\n",
       " 'Afrikaans-Latin1',\n",
       " 'Aguaruna-Latin1',\n",
       " 'Akuapem_Twi-UTF8',\n",
       " 'Albanian_Shqip-Latin1',\n",
       " 'Amahuaca',\n",
       " 'Amahuaca-Latin1',\n",
       " 'Amarakaeri-Latin1',\n",
       " 'Amuesha-Yanesha-UTF8',\n",
       " 'Arabela-Latin1',\n",
       " 'Arabic_Alarabia-Arabic',\n",
       " 'Asante-UTF8',\n",
       " 'Ashaninca-Latin1',\n",
       " 'Asheninca-Latin1',\n",
       " 'Asturian_Bable-Latin1',\n",
       " 'Aymara-Latin1',\n",
       " 'Balinese-Latin1',\n",
       " 'Bambara-UTF8',\n",
       " 'Baoule-UTF8',\n",
       " 'Basque_Euskara-Latin1',\n",
       " 'Batonu_Bariba-UTF8',\n",
       " 'Belorus_Belaruski-Cyrillic',\n",
       " 'Belorus_Belaruski-UTF8',\n",
       " 'Bemba-Latin1',\n",
       " 'Bengali-UTF8',\n",
       " 'Beti-UTF8',\n",
       " 'Bichelamar-Latin1',\n",
       " 'Bikol_Bicolano-Latin1',\n",
       " 'Bora-Latin1',\n",
       " 'Bosnian_Bosanski-Cyrillic',\n",
       " 'Bosnian_Bosanski-Latin2',\n",
       " 'Bosnian_Bosanski-UTF8',\n",
       " 'Breton-Latin1',\n",
       " 'Bugisnese-Latin1',\n",
       " 'Bulgarian_Balgarski-Cyrillic',\n",
       " 'Bulgarian_Balgarski-UTF8',\n",
       " 'Cakchiquel-Latin1',\n",
       " 'Campa_Pajonalino-Latin1',\n",
       " 'Candoshi-Shapra-Latin1',\n",
       " 'Caquinte-Latin1',\n",
       " 'Cashibo-Cacataibo-Latin1',\n",
       " 'Cashinahua-Latin1',\n",
       " 'Catalan-Latin1',\n",
       " 'Catalan_Catala-Latin1',\n",
       " 'Cebuano-Latin1',\n",
       " 'Chamorro-Latin1',\n",
       " 'Chayahuita-Latin1',\n",
       " 'Chechewa_Nyanja-Latin1',\n",
       " 'Chickasaw-Latin1',\n",
       " 'Chinanteco-Ajitlan-Latin1',\n",
       " 'Chinanteco-UTF8',\n",
       " 'Chinese_Mandarin-GB2312',\n",
       " 'Chuuk_Trukese-Latin1',\n",
       " 'Cokwe-Latin1',\n",
       " 'Corsican-Latin1',\n",
       " 'Croatian_Hrvatski-Latin2',\n",
       " 'Czech-Latin2',\n",
       " 'Czech-UTF8',\n",
       " 'Czech_Cesky-Latin2',\n",
       " 'Czech_Cesky-UTF8',\n",
       " 'Dagaare-UTF8',\n",
       " 'Dagbani-UTF8',\n",
       " 'Dangme-UTF8',\n",
       " 'Danish_Dansk-Latin1',\n",
       " 'Dendi-UTF8',\n",
       " 'Ditammari-UTF8',\n",
       " 'Dutch_Nederlands-Latin1',\n",
       " 'Edo-Latin1',\n",
       " 'English-Latin1',\n",
       " 'Esperanto-UTF8',\n",
       " 'Estonian_Eesti-Latin1',\n",
       " 'Ewe_Eve-UTF8',\n",
       " 'Fante-UTF8',\n",
       " 'Faroese-Latin1',\n",
       " 'Farsi_Persian-UTF8',\n",
       " 'Farsi_Persian-v2-UTF8',\n",
       " 'Fijian-Latin1',\n",
       " 'Filipino_Tagalog-Latin1',\n",
       " 'Finnish_Suomi-Latin1',\n",
       " 'Fon-UTF8',\n",
       " 'French_Francais-Latin1',\n",
       " 'Frisian-Latin1',\n",
       " 'Friulian_Friulano-Latin1',\n",
       " 'Ga-UTF8',\n",
       " 'Gagauz_Gagauzi-UTF8',\n",
       " 'Galician_Galego-Latin1',\n",
       " 'Garifuna_Garifuna-Latin1',\n",
       " 'German_Deutsch-Latin1',\n",
       " 'Gonja-UTF8',\n",
       " 'Greek_Ellinika-Greek',\n",
       " 'Greek_Ellinika-UTF8',\n",
       " 'Greenlandic_Inuktikut-Latin1',\n",
       " 'Guarani-Latin1',\n",
       " 'Guen_Mina-UTF8',\n",
       " 'HaitianCreole_Kreyol-Latin1',\n",
       " 'HaitianCreole_Popular-Latin1',\n",
       " 'Hani-Latin1',\n",
       " 'Hausa_Haoussa-Latin1',\n",
       " 'Hawaiian-UTF8',\n",
       " 'Hebrew_Ivrit-Hebrew',\n",
       " 'Hebrew_Ivrit-UTF8',\n",
       " 'Hiligaynon-Latin1',\n",
       " 'Hindi-UTF8',\n",
       " 'Hindi_web-UTF8',\n",
       " 'Hmong_Miao-Sichuan-Guizhou-Yunnan-Latin1',\n",
       " 'Hmong_Miao-SouthernEast-Guizhou-Latin1',\n",
       " 'Hmong_Miao_Northern-East-Guizhou-Latin1',\n",
       " 'Hrvatski_Croatian-Latin2',\n",
       " 'Huasteco-Latin1',\n",
       " 'Huitoto_Murui-Latin1',\n",
       " 'Hungarian_Magyar-Latin1',\n",
       " 'Hungarian_Magyar-Latin2',\n",
       " 'Hungarian_Magyar-UTF8',\n",
       " 'Ibibio_Efik-Latin1',\n",
       " 'Icelandic_Yslenska-Latin1',\n",
       " 'Ido-Latin1',\n",
       " 'Igbo-UTF8',\n",
       " 'Iloko_Ilocano-Latin1',\n",
       " 'Indonesian-Latin1',\n",
       " 'Interlingua-Latin1',\n",
       " 'Inuktikut_Greenlandic-Latin1',\n",
       " 'IrishGaelic_Gaeilge-Latin1',\n",
       " 'Italian-Latin1',\n",
       " 'Italian_Italiano-Latin1',\n",
       " 'Japanese_Nihongo-EUC',\n",
       " 'Japanese_Nihongo-SJIS',\n",
       " 'Japanese_Nihongo-UTF8',\n",
       " 'Javanese-Latin1',\n",
       " 'Jola-Fogny_Diola-UTF8',\n",
       " 'Kabye-UTF8',\n",
       " 'Kannada-UTF8',\n",
       " 'Kaonde-Latin1',\n",
       " 'Kapampangan-Latin1',\n",
       " 'Kasem-UTF8',\n",
       " 'Kazakh-Cyrillic',\n",
       " 'Kazakh-UTF8',\n",
       " 'Kiche_Quiche-Latin1',\n",
       " 'Kicongo-Latin1',\n",
       " 'Kimbundu_Mbundu-Latin1',\n",
       " 'Kinyamwezi_Nyamwezi-Latin1',\n",
       " 'Kinyarwanda-Latin1',\n",
       " 'Kituba-Latin1',\n",
       " 'Korean_Hankuko-UTF8',\n",
       " 'Kpelewo-UTF8',\n",
       " 'Krio-UTF8',\n",
       " 'Kurdish-UTF8',\n",
       " 'Lamnso_Lam-nso-UTF8',\n",
       " 'Latin_Latina-Latin1',\n",
       " 'Latin_Latina-v2-Latin1',\n",
       " 'Latvian-Latin1',\n",
       " 'Limba-UTF8',\n",
       " 'Lingala-Latin1',\n",
       " 'Lithuanian_Lietuviskai-Baltic',\n",
       " 'Lozi-Latin1',\n",
       " 'Luba-Kasai_Tshiluba-Latin1',\n",
       " 'Luganda_Ganda-Latin1',\n",
       " 'Lunda_Chokwe-lunda-Latin1',\n",
       " 'Luvale-Latin1',\n",
       " 'Luxembourgish_Letzebuergeusch-Latin1',\n",
       " 'Macedonian-UTF8',\n",
       " 'Madurese-Latin1',\n",
       " 'Makonde-Latin1',\n",
       " 'Malagasy-Latin1',\n",
       " 'Malay_BahasaMelayu-Latin1',\n",
       " 'Maltese-UTF8',\n",
       " 'Mam-Latin1',\n",
       " 'Maninka-UTF8',\n",
       " 'Maori-Latin1',\n",
       " 'Mapudungun_Mapuzgun-Latin1',\n",
       " 'Mapudungun_Mapuzgun-UTF8',\n",
       " 'Marshallese-Latin1',\n",
       " 'Matses-Latin1',\n",
       " 'Mayan_Yucateco-Latin1',\n",
       " 'Mazahua_Jnatrjo-UTF8',\n",
       " 'Mazateco-Latin1',\n",
       " 'Mende-UTF8',\n",
       " 'Mikmaq_Micmac-Mikmaq-Latin1',\n",
       " 'Minangkabau-Latin1',\n",
       " 'Miskito_Miskito-Latin1',\n",
       " 'Mixteco-Latin1',\n",
       " 'Mongolian_Khalkha-Cyrillic',\n",
       " 'Mongolian_Khalkha-UTF8',\n",
       " 'Moore_More-UTF8',\n",
       " 'Nahuatl-Latin1',\n",
       " 'Ndebele-Latin1',\n",
       " 'Nepali-UTF8',\n",
       " 'Ngangela_Nyemba-Latin1',\n",
       " 'NigerianPidginEnglish-Latin1',\n",
       " 'Nomatsiguenga-Latin1',\n",
       " 'NorthernSotho_Pedi-Sepedi-Latin1',\n",
       " 'Norwegian-Latin1',\n",
       " 'Norwegian_Norsk-Bokmal-Latin1',\n",
       " 'Norwegian_Norsk-Nynorsk-Latin1',\n",
       " 'Nyanja_Chechewa-Latin1',\n",
       " 'Nyanja_Chinyanja-Latin1',\n",
       " 'Nzema-UTF8',\n",
       " 'OccitanAuvergnat-Latin1',\n",
       " 'OccitanLanguedocien-Latin1',\n",
       " 'Oromiffa_AfaanOromo-Latin1',\n",
       " 'Osetin_Ossetian-UTF8',\n",
       " 'Oshiwambo_Ndonga-Latin1',\n",
       " 'Otomi_Nahnu-Latin1',\n",
       " 'Paez-Latin1',\n",
       " 'Palauan-Latin1',\n",
       " 'Peuhl-UTF8',\n",
       " 'Picard-Latin1',\n",
       " 'Pipil-Latin1',\n",
       " 'Polish-Latin2',\n",
       " 'Polish_Polski-Latin2',\n",
       " 'Ponapean-Latin1',\n",
       " 'Portuguese_Portugues-Latin1',\n",
       " 'Pulaar-UTF8',\n",
       " 'Punjabi_Panjabi-UTF8',\n",
       " 'Purhepecha-UTF8',\n",
       " 'Qechi_Kekchi-Latin1',\n",
       " 'Quechua-Latin1',\n",
       " 'Quichua-Latin1',\n",
       " 'Rarotongan_MaoriCookIslands-Latin1',\n",
       " 'Rhaeto-Romance_Rumantsch-Latin1',\n",
       " 'Romani-Latin1',\n",
       " 'Romani-UTF8',\n",
       " 'Romanian-Latin2',\n",
       " 'Romanian_Romana-Latin2',\n",
       " 'Rukonzo_Konjo-Latin1',\n",
       " 'Rundi_Kirundi-Latin1',\n",
       " 'Runyankore-rukiga_Nkore-kiga-Latin1',\n",
       " 'Russian-Cyrillic',\n",
       " 'Russian-UTF8',\n",
       " 'Russian_Russky-Cyrillic',\n",
       " 'Russian_Russky-UTF8',\n",
       " 'Sami_Lappish-UTF8',\n",
       " 'Sammarinese-Latin1',\n",
       " 'Samoan-Latin1',\n",
       " 'Sango_Sangho-Latin1',\n",
       " 'Sanskrit-UTF8',\n",
       " 'Saraiki-UTF8',\n",
       " 'Sardinian-Latin1',\n",
       " 'ScottishGaelic_GaidhligAlbanach-Latin1',\n",
       " 'Seereer-UTF8',\n",
       " 'Serbian_Srpski-Cyrillic',\n",
       " 'Serbian_Srpski-Latin2',\n",
       " 'Serbian_Srpski-UTF8',\n",
       " 'Sharanahua-Latin1',\n",
       " 'Shipibo-Conibo-Latin1',\n",
       " 'Shona-Latin1',\n",
       " 'Sinhala-UTF8',\n",
       " 'Siswati-Latin1',\n",
       " 'Slovak-Latin2',\n",
       " 'Slovak_Slovencina-Latin2',\n",
       " 'Slovenian_Slovenscina-Latin2',\n",
       " 'SolomonsPidgin_Pijin-Latin1',\n",
       " 'Somali-Latin1',\n",
       " 'Soninke_Soninkanxaane-UTF8',\n",
       " 'Sorbian-Latin2',\n",
       " 'SouthernSotho_Sotho-Sesotho-Sutu-Sesutu-Latin1',\n",
       " 'Spanish-Latin1',\n",
       " 'Spanish_Espanol-Latin1',\n",
       " 'Sukuma-Latin1',\n",
       " 'Sundanese-Latin1',\n",
       " 'Sussu_Soussou-Sosso-Soso-Susu-UTF8',\n",
       " 'Swaheli-Latin1',\n",
       " 'Swahili_Kiswahili-Latin1',\n",
       " 'Swedish_Svenska-Latin1',\n",
       " 'Tahitian-UTF8',\n",
       " 'Tenek_Huasteco-Latin1',\n",
       " 'Tetum-Latin1',\n",
       " 'Themne_Temne-UTF8',\n",
       " 'Tiv-Latin1',\n",
       " 'Toba-UTF8',\n",
       " 'Tojol-abal-Latin1',\n",
       " 'TokPisin-Latin1',\n",
       " 'Tonga-Latin1',\n",
       " 'Tongan_Tonga-Latin1',\n",
       " 'Totonaco-Latin1',\n",
       " 'Trukese_Chuuk-Latin1',\n",
       " 'Turkish_Turkce-Turkish',\n",
       " 'Turkish_Turkce-UTF8',\n",
       " 'Tzeltal-Latin1',\n",
       " 'Tzotzil-Latin1',\n",
       " 'Uighur_Uyghur-Latin1',\n",
       " 'Uighur_Uyghur-UTF8',\n",
       " 'Ukrainian-Cyrillic',\n",
       " 'Ukrainian-UTF8',\n",
       " 'Umbundu-Latin1',\n",
       " 'Urarina-Latin1',\n",
       " 'Uzbek-Latin1',\n",
       " 'Vietnamese-ALRN-UTF8',\n",
       " 'Vietnamese-UTF8',\n",
       " 'Vlach-Latin1',\n",
       " 'Walloon_Wallon-Latin1',\n",
       " 'Wama-UTF8',\n",
       " 'Waray-Latin1',\n",
       " 'Wayuu-Latin1',\n",
       " 'Welsh_Cymraeg-Latin1',\n",
       " 'WesternSotho_Tswana-Setswana-Latin1',\n",
       " 'Wolof-Latin1',\n",
       " 'Xhosa-Latin1',\n",
       " 'Yagua-Latin1',\n",
       " 'Yao-Latin1',\n",
       " 'Yapese-Latin1',\n",
       " 'Yoruba-UTF8',\n",
       " 'Zapoteco-Latin1',\n",
       " 'Zapoteco-SanLucasQuiavini-Latin1',\n",
       " 'Zhuang-Latin1',\n",
       " 'Zulu-Latin1']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.corpus.udhr.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import udhr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "languages = ['Chickasaw','English','German_Deutsch','Greenlandic_Inuktikut', 'Hungarian_Magyar', 'Ibibio_Efik']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfd = nltk.ConditionalFreqDist(\n",
    "    (lang,len(word))\n",
    "    for lang in languages\n",
    "    for word in udhr.words(lang + '-Latin1'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x129b65610>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfd.plot(cumulative = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Abkhaz-Cyrillic+Abkh',\n",
       " 'Abkhaz-UTF8',\n",
       " 'Achehnese-Latin1',\n",
       " 'Achuar-Shiwiar-Latin1',\n",
       " 'Adja-UTF8',\n",
       " 'Afaan_Oromo_Oromiffa-Latin1',\n",
       " 'Afrikaans-Latin1',\n",
       " 'Aguaruna-Latin1',\n",
       " 'Akuapem_Twi-UTF8',\n",
       " 'Albanian_Shqip-Latin1',\n",
       " 'Amahuaca',\n",
       " 'Amahuaca-Latin1',\n",
       " 'Amarakaeri-Latin1',\n",
       " 'Amuesha-Yanesha-UTF8',\n",
       " 'Arabela-Latin1',\n",
       " 'Arabic_Alarabia-Arabic',\n",
       " 'Asante-UTF8',\n",
       " 'Ashaninca-Latin1',\n",
       " 'Asheninca-Latin1',\n",
       " 'Asturian_Bable-Latin1',\n",
       " 'Aymara-Latin1',\n",
       " 'Balinese-Latin1',\n",
       " 'Bambara-UTF8',\n",
       " 'Baoule-UTF8',\n",
       " 'Basque_Euskara-Latin1',\n",
       " 'Batonu_Bariba-UTF8',\n",
       " 'Belorus_Belaruski-Cyrillic',\n",
       " 'Belorus_Belaruski-UTF8',\n",
       " 'Bemba-Latin1',\n",
       " 'Bengali-UTF8',\n",
       " 'Beti-UTF8',\n",
       " 'Bichelamar-Latin1',\n",
       " 'Bikol_Bicolano-Latin1',\n",
       " 'Bora-Latin1',\n",
       " 'Bosnian_Bosanski-Cyrillic',\n",
       " 'Bosnian_Bosanski-Latin2',\n",
       " 'Bosnian_Bosanski-UTF8',\n",
       " 'Breton-Latin1',\n",
       " 'Bugisnese-Latin1',\n",
       " 'Bulgarian_Balgarski-Cyrillic',\n",
       " 'Bulgarian_Balgarski-UTF8',\n",
       " 'Cakchiquel-Latin1',\n",
       " 'Campa_Pajonalino-Latin1',\n",
       " 'Candoshi-Shapra-Latin1',\n",
       " 'Caquinte-Latin1',\n",
       " 'Cashibo-Cacataibo-Latin1',\n",
       " 'Cashinahua-Latin1',\n",
       " 'Catalan-Latin1',\n",
       " 'Catalan_Catala-Latin1',\n",
       " 'Cebuano-Latin1',\n",
       " 'Chamorro-Latin1',\n",
       " 'Chayahuita-Latin1',\n",
       " 'Chechewa_Nyanja-Latin1',\n",
       " 'Chickasaw-Latin1',\n",
       " 'Chinanteco-Ajitlan-Latin1',\n",
       " 'Chinanteco-UTF8',\n",
       " 'Chinese_Mandarin-GB2312',\n",
       " 'Chuuk_Trukese-Latin1',\n",
       " 'Cokwe-Latin1',\n",
       " 'Corsican-Latin1',\n",
       " 'Croatian_Hrvatski-Latin2',\n",
       " 'Czech-Latin2',\n",
       " 'Czech-UTF8',\n",
       " 'Czech_Cesky-Latin2',\n",
       " 'Czech_Cesky-UTF8',\n",
       " 'Dagaare-UTF8',\n",
       " 'Dagbani-UTF8',\n",
       " 'Dangme-UTF8',\n",
       " 'Danish_Dansk-Latin1',\n",
       " 'Dendi-UTF8',\n",
       " 'Ditammari-UTF8',\n",
       " 'Dutch_Nederlands-Latin1',\n",
       " 'Edo-Latin1',\n",
       " 'English-Latin1',\n",
       " 'Esperanto-UTF8',\n",
       " 'Estonian_Eesti-Latin1',\n",
       " 'Ewe_Eve-UTF8',\n",
       " 'Fante-UTF8',\n",
       " 'Faroese-Latin1',\n",
       " 'Farsi_Persian-UTF8',\n",
       " 'Farsi_Persian-v2-UTF8',\n",
       " 'Fijian-Latin1',\n",
       " 'Filipino_Tagalog-Latin1',\n",
       " 'Finnish_Suomi-Latin1',\n",
       " 'Fon-UTF8',\n",
       " 'French_Francais-Latin1',\n",
       " 'Frisian-Latin1',\n",
       " 'Friulian_Friulano-Latin1',\n",
       " 'Ga-UTF8',\n",
       " 'Gagauz_Gagauzi-UTF8',\n",
       " 'Galician_Galego-Latin1',\n",
       " 'Garifuna_Garifuna-Latin1',\n",
       " 'German_Deutsch-Latin1',\n",
       " 'Gonja-UTF8',\n",
       " 'Greek_Ellinika-Greek',\n",
       " 'Greek_Ellinika-UTF8',\n",
       " 'Greenlandic_Inuktikut-Latin1',\n",
       " 'Guarani-Latin1',\n",
       " 'Guen_Mina-UTF8',\n",
       " 'HaitianCreole_Kreyol-Latin1',\n",
       " 'HaitianCreole_Popular-Latin1',\n",
       " 'Hani-Latin1',\n",
       " 'Hausa_Haoussa-Latin1',\n",
       " 'Hawaiian-UTF8',\n",
       " 'Hebrew_Ivrit-Hebrew',\n",
       " 'Hebrew_Ivrit-UTF8',\n",
       " 'Hiligaynon-Latin1',\n",
       " 'Hindi-UTF8',\n",
       " 'Hindi_web-UTF8',\n",
       " 'Hmong_Miao-Sichuan-Guizhou-Yunnan-Latin1',\n",
       " 'Hmong_Miao-SouthernEast-Guizhou-Latin1',\n",
       " 'Hmong_Miao_Northern-East-Guizhou-Latin1',\n",
       " 'Hrvatski_Croatian-Latin2',\n",
       " 'Huasteco-Latin1',\n",
       " 'Huitoto_Murui-Latin1',\n",
       " 'Hungarian_Magyar-Latin1',\n",
       " 'Hungarian_Magyar-Latin2',\n",
       " 'Hungarian_Magyar-UTF8',\n",
       " 'Ibibio_Efik-Latin1',\n",
       " 'Icelandic_Yslenska-Latin1',\n",
       " 'Ido-Latin1',\n",
       " 'Igbo-UTF8',\n",
       " 'Iloko_Ilocano-Latin1',\n",
       " 'Indonesian-Latin1',\n",
       " 'Interlingua-Latin1',\n",
       " 'Inuktikut_Greenlandic-Latin1',\n",
       " 'IrishGaelic_Gaeilge-Latin1',\n",
       " 'Italian-Latin1',\n",
       " 'Italian_Italiano-Latin1',\n",
       " 'Japanese_Nihongo-EUC',\n",
       " 'Japanese_Nihongo-SJIS',\n",
       " 'Japanese_Nihongo-UTF8',\n",
       " 'Javanese-Latin1',\n",
       " 'Jola-Fogny_Diola-UTF8',\n",
       " 'Kabye-UTF8',\n",
       " 'Kannada-UTF8',\n",
       " 'Kaonde-Latin1',\n",
       " 'Kapampangan-Latin1',\n",
       " 'Kasem-UTF8',\n",
       " 'Kazakh-Cyrillic',\n",
       " 'Kazakh-UTF8',\n",
       " 'Kiche_Quiche-Latin1',\n",
       " 'Kicongo-Latin1',\n",
       " 'Kimbundu_Mbundu-Latin1',\n",
       " 'Kinyamwezi_Nyamwezi-Latin1',\n",
       " 'Kinyarwanda-Latin1',\n",
       " 'Kituba-Latin1',\n",
       " 'Korean_Hankuko-UTF8',\n",
       " 'Kpelewo-UTF8',\n",
       " 'Krio-UTF8',\n",
       " 'Kurdish-UTF8',\n",
       " 'Lamnso_Lam-nso-UTF8',\n",
       " 'Latin_Latina-Latin1',\n",
       " 'Latin_Latina-v2-Latin1',\n",
       " 'Latvian-Latin1',\n",
       " 'Limba-UTF8',\n",
       " 'Lingala-Latin1',\n",
       " 'Lithuanian_Lietuviskai-Baltic',\n",
       " 'Lozi-Latin1',\n",
       " 'Luba-Kasai_Tshiluba-Latin1',\n",
       " 'Luganda_Ganda-Latin1',\n",
       " 'Lunda_Chokwe-lunda-Latin1',\n",
       " 'Luvale-Latin1',\n",
       " 'Luxembourgish_Letzebuergeusch-Latin1',\n",
       " 'Macedonian-UTF8',\n",
       " 'Madurese-Latin1',\n",
       " 'Makonde-Latin1',\n",
       " 'Malagasy-Latin1',\n",
       " 'Malay_BahasaMelayu-Latin1',\n",
       " 'Maltese-UTF8',\n",
       " 'Mam-Latin1',\n",
       " 'Maninka-UTF8',\n",
       " 'Maori-Latin1',\n",
       " 'Mapudungun_Mapuzgun-Latin1',\n",
       " 'Mapudungun_Mapuzgun-UTF8',\n",
       " 'Marshallese-Latin1',\n",
       " 'Matses-Latin1',\n",
       " 'Mayan_Yucateco-Latin1',\n",
       " 'Mazahua_Jnatrjo-UTF8',\n",
       " 'Mazateco-Latin1',\n",
       " 'Mende-UTF8',\n",
       " 'Mikmaq_Micmac-Mikmaq-Latin1',\n",
       " 'Minangkabau-Latin1',\n",
       " 'Miskito_Miskito-Latin1',\n",
       " 'Mixteco-Latin1',\n",
       " 'Mongolian_Khalkha-Cyrillic',\n",
       " 'Mongolian_Khalkha-UTF8',\n",
       " 'Moore_More-UTF8',\n",
       " 'Nahuatl-Latin1',\n",
       " 'Ndebele-Latin1',\n",
       " 'Nepali-UTF8',\n",
       " 'Ngangela_Nyemba-Latin1',\n",
       " 'NigerianPidginEnglish-Latin1',\n",
       " 'Nomatsiguenga-Latin1',\n",
       " 'NorthernSotho_Pedi-Sepedi-Latin1',\n",
       " 'Norwegian-Latin1',\n",
       " 'Norwegian_Norsk-Bokmal-Latin1',\n",
       " 'Norwegian_Norsk-Nynorsk-Latin1',\n",
       " 'Nyanja_Chechewa-Latin1',\n",
       " 'Nyanja_Chinyanja-Latin1',\n",
       " 'Nzema-UTF8',\n",
       " 'OccitanAuvergnat-Latin1',\n",
       " 'OccitanLanguedocien-Latin1',\n",
       " 'Oromiffa_AfaanOromo-Latin1',\n",
       " 'Osetin_Ossetian-UTF8',\n",
       " 'Oshiwambo_Ndonga-Latin1',\n",
       " 'Otomi_Nahnu-Latin1',\n",
       " 'Paez-Latin1',\n",
       " 'Palauan-Latin1',\n",
       " 'Peuhl-UTF8',\n",
       " 'Picard-Latin1',\n",
       " 'Pipil-Latin1',\n",
       " 'Polish-Latin2',\n",
       " 'Polish_Polski-Latin2',\n",
       " 'Ponapean-Latin1',\n",
       " 'Portuguese_Portugues-Latin1',\n",
       " 'Pulaar-UTF8',\n",
       " 'Punjabi_Panjabi-UTF8',\n",
       " 'Purhepecha-UTF8',\n",
       " 'Qechi_Kekchi-Latin1',\n",
       " 'Quechua-Latin1',\n",
       " 'Quichua-Latin1',\n",
       " 'Rarotongan_MaoriCookIslands-Latin1',\n",
       " 'Rhaeto-Romance_Rumantsch-Latin1',\n",
       " 'Romani-Latin1',\n",
       " 'Romani-UTF8',\n",
       " 'Romanian-Latin2',\n",
       " 'Romanian_Romana-Latin2',\n",
       " 'Rukonzo_Konjo-Latin1',\n",
       " 'Rundi_Kirundi-Latin1',\n",
       " 'Runyankore-rukiga_Nkore-kiga-Latin1',\n",
       " 'Russian-Cyrillic',\n",
       " 'Russian-UTF8',\n",
       " 'Russian_Russky-Cyrillic',\n",
       " 'Russian_Russky-UTF8',\n",
       " 'Sami_Lappish-UTF8',\n",
       " 'Sammarinese-Latin1',\n",
       " 'Samoan-Latin1',\n",
       " 'Sango_Sangho-Latin1',\n",
       " 'Sanskrit-UTF8',\n",
       " 'Saraiki-UTF8',\n",
       " 'Sardinian-Latin1',\n",
       " 'ScottishGaelic_GaidhligAlbanach-Latin1',\n",
       " 'Seereer-UTF8',\n",
       " 'Serbian_Srpski-Cyrillic',\n",
       " 'Serbian_Srpski-Latin2',\n",
       " 'Serbian_Srpski-UTF8',\n",
       " 'Sharanahua-Latin1',\n",
       " 'Shipibo-Conibo-Latin1',\n",
       " 'Shona-Latin1',\n",
       " 'Sinhala-UTF8',\n",
       " 'Siswati-Latin1',\n",
       " 'Slovak-Latin2',\n",
       " 'Slovak_Slovencina-Latin2',\n",
       " 'Slovenian_Slovenscina-Latin2',\n",
       " 'SolomonsPidgin_Pijin-Latin1',\n",
       " 'Somali-Latin1',\n",
       " 'Soninke_Soninkanxaane-UTF8',\n",
       " 'Sorbian-Latin2',\n",
       " 'SouthernSotho_Sotho-Sesotho-Sutu-Sesutu-Latin1',\n",
       " 'Spanish-Latin1',\n",
       " 'Spanish_Espanol-Latin1',\n",
       " 'Sukuma-Latin1',\n",
       " 'Sundanese-Latin1',\n",
       " 'Sussu_Soussou-Sosso-Soso-Susu-UTF8',\n",
       " 'Swaheli-Latin1',\n",
       " 'Swahili_Kiswahili-Latin1',\n",
       " 'Swedish_Svenska-Latin1',\n",
       " 'Tahitian-UTF8',\n",
       " 'Tenek_Huasteco-Latin1',\n",
       " 'Tetum-Latin1',\n",
       " 'Themne_Temne-UTF8',\n",
       " 'Tiv-Latin1',\n",
       " 'Toba-UTF8',\n",
       " 'Tojol-abal-Latin1',\n",
       " 'TokPisin-Latin1',\n",
       " 'Tonga-Latin1',\n",
       " 'Tongan_Tonga-Latin1',\n",
       " 'Totonaco-Latin1',\n",
       " 'Trukese_Chuuk-Latin1',\n",
       " 'Turkish_Turkce-Turkish',\n",
       " 'Turkish_Turkce-UTF8',\n",
       " 'Tzeltal-Latin1',\n",
       " 'Tzotzil-Latin1',\n",
       " 'Uighur_Uyghur-Latin1',\n",
       " 'Uighur_Uyghur-UTF8',\n",
       " 'Ukrainian-Cyrillic',\n",
       " 'Ukrainian-UTF8',\n",
       " 'Umbundu-Latin1',\n",
       " 'Urarina-Latin1',\n",
       " 'Uzbek-Latin1',\n",
       " 'Vietnamese-ALRN-UTF8',\n",
       " 'Vietnamese-UTF8',\n",
       " 'Vlach-Latin1',\n",
       " 'Walloon_Wallon-Latin1',\n",
       " 'Wama-UTF8',\n",
       " 'Waray-Latin1',\n",
       " 'Wayuu-Latin1',\n",
       " 'Welsh_Cymraeg-Latin1',\n",
       " 'WesternSotho_Tswana-Setswana-Latin1',\n",
       " 'Wolof-Latin1',\n",
       " 'Xhosa-Latin1',\n",
       " 'Yagua-Latin1',\n",
       " 'Yao-Latin1',\n",
       " 'Yapese-Latin1',\n",
       " 'Yoruba-UTF8',\n",
       " 'Zapoteco-Latin1',\n",
       " 'Zapoteco-SanLucasQuiavini-Latin1',\n",
       " 'Zhuang-Latin1',\n",
       " 'Zulu-Latin1']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "udhr.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['世界人权宣言',\n",
       " '联合国大会一九四八年十二月十日第217A',\n",
       " '(',\n",
       " 'III',\n",
       " ')',\n",
       " '号决议通过并颁布',\n",
       " '1948',\n",
       " '年',\n",
       " '12',\n",
       " '月',\n",
       " '10',\n",
       " '日',\n",
       " '，',\n",
       " '联',\n",
       " '合',\n",
       " '国',\n",
       " '大',\n",
       " '会',\n",
       " '通',\n",
       " '过',\n",
       " '并',\n",
       " '颁',\n",
       " '布',\n",
       " '《',\n",
       " '世',\n",
       " '界',\n",
       " '人',\n",
       " '权',\n",
       " '宣',\n",
       " '言',\n",
       " '》。',\n",
       " '这',\n",
       " '一',\n",
       " '具',\n",
       " '有',\n",
       " '历',\n",
       " '史',\n",
       " '意',\n",
       " '义',\n",
       " '的',\n",
       " '《',\n",
       " '宣',\n",
       " '言',\n",
       " '》',\n",
       " '颁',\n",
       " '布',\n",
       " '后',\n",
       " '，',\n",
       " '大',\n",
       " '会',\n",
       " '要',\n",
       " '求',\n",
       " '所',\n",
       " '有',\n",
       " '会',\n",
       " '员',\n",
       " '国',\n",
       " '广',\n",
       " '为',\n",
       " '宣',\n",
       " '传',\n",
       " '，',\n",
       " '并',\n",
       " '且',\n",
       " '“',\n",
       " '不',\n",
       " '分',\n",
       " '国',\n",
       " '家',\n",
       " '或',\n",
       " '领',\n",
       " '土',\n",
       " '的',\n",
       " '政',\n",
       " '治',\n",
       " '地',\n",
       " '位',\n",
       " ',',\n",
       " '主',\n",
       " '要',\n",
       " '在',\n",
       " '各',\n",
       " '级',\n",
       " '学',\n",
       " '校',\n",
       " '和',\n",
       " '其',\n",
       " '他',\n",
       " '教',\n",
       " '育',\n",
       " '机',\n",
       " '构',\n",
       " '加',\n",
       " '以',\n",
       " '传',\n",
       " '播',\n",
       " '、',\n",
       " '展',\n",
       " '示',\n",
       " '、']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "udhr.words('Chinese_Mandarin-GB2312')[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text = udhr.raw('Chinese_Mandarin-GB2312')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({' ': 2652, '的': 149, '\\n': 118, '人': 104, '和': 80, ',': 79, '有': 70, '权': 69, '。': 62, '国': 42, ...})"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.FreqDist(raw_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "文本语料库结构\n",
    "\n",
    "到目前为止，我们已经看到了各种语料库结构。 这些在1.3中进行了总结。 最简单的一种没有任何结构：它只是文本的集合。 通常，文本被分组为可能对应于体裁，来源，作者，语言等的类别。有时这些类别会重叠，特别是在主题类别的情况下，因为文本可能与多个主题相关。 有时，文本集合具有时间结构，新闻集合是最常见的示例。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以用help(nltk.corpus.reader) 查找到更多的语料库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package nltk.corpus.reader in nltk.corpus:\n",
      "\n",
      "NAME\n",
      "    nltk.corpus.reader\n",
      "\n",
      "DESCRIPTION\n",
      "    NLTK corpus readers.  The modules in this package provide functions\n",
      "    that can be used to read corpus fileids in a variety of formats.  These\n",
      "    functions can be used to read both the corpus fileids that are\n",
      "    distributed in the NLTK corpus package, and corpus fileids that are part\n",
      "    of external corpora.\n",
      "    \n",
      "    Corpus Reader Functions\n",
      "    =======================\n",
      "    Each corpus module defines one or more \"corpus reader functions\",\n",
      "    which can be used to read documents from that corpus.  These functions\n",
      "    take an argument, ``item``, which is used to indicate which document\n",
      "    should be read from the corpus:\n",
      "    \n",
      "    - If ``item`` is one of the unique identifiers listed in the corpus\n",
      "      module's ``items`` variable, then the corresponding document will\n",
      "      be loaded from the NLTK corpus package.\n",
      "    - If ``item`` is a fileid, then that file will be read.\n",
      "    \n",
      "    Additionally, corpus reader functions can be given lists of item\n",
      "    names; in which case, they will return a concatenation of the\n",
      "    corresponding documents.\n",
      "    \n",
      "    Corpus reader functions are named based on the type of information\n",
      "    they return.  Some common examples, and their return types, are:\n",
      "    \n",
      "    - words(): list of str\n",
      "    - sents(): list of (list of str)\n",
      "    - paras(): list of (list of (list of str))\n",
      "    - tagged_words(): list of (str,str) tuple\n",
      "    - tagged_sents(): list of (list of (str,str))\n",
      "    - tagged_paras(): list of (list of (list of (str,str)))\n",
      "    - chunked_sents(): list of (Tree w/ (str,str) leaves)\n",
      "    - parsed_sents(): list of (Tree with str leaves)\n",
      "    - parsed_paras(): list of (list of (Tree with str leaves))\n",
      "    - xml(): A single xml ElementTree\n",
      "    - raw(): unprocessed corpus contents\n",
      "    \n",
      "    For example, to read a list of the words in the Brown Corpus, use\n",
      "    ``nltk.corpus.brown.words()``:\n",
      "    \n",
      "        >>> from nltk.corpus import brown\n",
      "        >>> print(\", \".join(brown.words()))\n",
      "        The, Fulton, County, Grand, Jury, said, ...\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "    aligned\n",
      "    api\n",
      "    bnc\n",
      "    bracket_parse\n",
      "    categorized_sents\n",
      "    chasen\n",
      "    childes\n",
      "    chunked\n",
      "    cmudict\n",
      "    comparative_sents\n",
      "    conll\n",
      "    crubadan\n",
      "    dependency\n",
      "    framenet\n",
      "    ieer\n",
      "    indian\n",
      "    ipipan\n",
      "    knbc\n",
      "    lin\n",
      "    mte\n",
      "    nkjp\n",
      "    nombank\n",
      "    nps_chat\n",
      "    opinion_lexicon\n",
      "    panlex_lite\n",
      "    panlex_swadesh\n",
      "    pl196x\n",
      "    plaintext\n",
      "    ppattach\n",
      "    propbank\n",
      "    pros_cons\n",
      "    reviews\n",
      "    rte\n",
      "    semcor\n",
      "    senseval\n",
      "    sentiwordnet\n",
      "    sinica_treebank\n",
      "    string_category\n",
      "    switchboard\n",
      "    tagged\n",
      "    timit\n",
      "    toolbox\n",
      "    twitter\n",
      "    udhr\n",
      "    util\n",
      "    verbnet\n",
      "    wordlist\n",
      "    wordnet\n",
      "    xmldocs\n",
      "    ycoe\n",
      "\n",
      "CLASSES\n",
      "    builtins.object\n",
      "        nltk.corpus.reader.api.CategorizedCorpusReader\n",
      "            nltk.corpus.reader.bracket_parse.CategorizedBracketParseCorpusReader(nltk.corpus.reader.api.CategorizedCorpusReader, nltk.corpus.reader.bracket_parse.BracketParseCorpusReader)\n",
      "            nltk.corpus.reader.categorized_sents.CategorizedSentencesCorpusReader(nltk.corpus.reader.api.CategorizedCorpusReader, nltk.corpus.reader.api.CorpusReader)\n",
      "            nltk.corpus.reader.pl196x.Pl196xCorpusReader(nltk.corpus.reader.api.CategorizedCorpusReader, nltk.corpus.reader.xmldocs.XMLCorpusReader)\n",
      "            nltk.corpus.reader.plaintext.CategorizedPlaintextCorpusReader(nltk.corpus.reader.api.CategorizedCorpusReader, nltk.corpus.reader.plaintext.PlaintextCorpusReader)\n",
      "                nltk.corpus.reader.plaintext.PortugueseCategorizedPlaintextCorpusReader\n",
      "            nltk.corpus.reader.pros_cons.ProsConsCorpusReader(nltk.corpus.reader.api.CategorizedCorpusReader, nltk.corpus.reader.api.CorpusReader)\n",
      "            nltk.corpus.reader.tagged.CategorizedTaggedCorpusReader(nltk.corpus.reader.api.CategorizedCorpusReader, nltk.corpus.reader.tagged.TaggedCorpusReader)\n",
      "        nltk.corpus.reader.api.CorpusReader\n",
      "            nltk.corpus.reader.aligned.AlignedCorpusReader\n",
      "            nltk.corpus.reader.api.SyntaxCorpusReader\n",
      "                nltk.corpus.reader.bracket_parse.BracketParseCorpusReader\n",
      "                    nltk.corpus.reader.bracket_parse.AlpinoCorpusReader\n",
      "                nltk.corpus.reader.dependency.DependencyCorpusReader\n",
      "                nltk.corpus.reader.knbc.KNBCorpusReader\n",
      "                nltk.corpus.reader.sinica_treebank.SinicaTreebankCorpusReader\n",
      "            nltk.corpus.reader.chasen.ChasenCorpusReader\n",
      "            nltk.corpus.reader.chunked.ChunkedCorpusReader\n",
      "            nltk.corpus.reader.cmudict.CMUDictCorpusReader\n",
      "            nltk.corpus.reader.comparative_sents.ComparativeSentencesCorpusReader\n",
      "            nltk.corpus.reader.conll.ConllCorpusReader\n",
      "                nltk.corpus.reader.conll.ConllChunkCorpusReader\n",
      "            nltk.corpus.reader.crubadan.CrubadanCorpusReader\n",
      "            nltk.corpus.reader.ieer.IEERCorpusReader\n",
      "            nltk.corpus.reader.indian.IndianCorpusReader\n",
      "            nltk.corpus.reader.ipipan.IPIPANCorpusReader\n",
      "            nltk.corpus.reader.lin.LinThesaurusCorpusReader\n",
      "            nltk.corpus.reader.nombank.NombankCorpusReader\n",
      "            nltk.corpus.reader.panlex_lite.PanLexLiteCorpusReader\n",
      "            nltk.corpus.reader.plaintext.PlaintextCorpusReader\n",
      "                nltk.corpus.reader.plaintext.EuroparlCorpusReader\n",
      "                nltk.corpus.reader.udhr.UdhrCorpusReader\n",
      "            nltk.corpus.reader.ppattach.PPAttachmentCorpusReader\n",
      "            nltk.corpus.reader.propbank.PropbankCorpusReader\n",
      "            nltk.corpus.reader.reviews.ReviewsCorpusReader\n",
      "            nltk.corpus.reader.senseval.SensevalCorpusReader\n",
      "            nltk.corpus.reader.sentiwordnet.SentiWordNetCorpusReader\n",
      "            nltk.corpus.reader.string_category.StringCategoryCorpusReader\n",
      "            nltk.corpus.reader.switchboard.SwitchboardCorpusReader\n",
      "            nltk.corpus.reader.tagged.TaggedCorpusReader\n",
      "                nltk.corpus.reader.mte.MTECorpusReader\n",
      "                nltk.corpus.reader.tagged.MacMorphoCorpusReader\n",
      "                nltk.corpus.reader.tagged.TimitTaggedCorpusReader\n",
      "            nltk.corpus.reader.timit.TimitCorpusReader\n",
      "            nltk.corpus.reader.toolbox.ToolboxCorpusReader\n",
      "            nltk.corpus.reader.twitter.TwitterCorpusReader\n",
      "            nltk.corpus.reader.wordlist.WordListCorpusReader\n",
      "                nltk.corpus.reader.opinion_lexicon.OpinionLexiconCorpusReader\n",
      "                nltk.corpus.reader.panlex_swadesh.PanlexSwadeshCorpusReader\n",
      "                nltk.corpus.reader.wordlist.MWAPPDBCorpusReader\n",
      "                nltk.corpus.reader.wordlist.NonbreakingPrefixesCorpusReader\n",
      "                nltk.corpus.reader.wordlist.SwadeshCorpusReader\n",
      "                nltk.corpus.reader.wordlist.UnicharsCorpusReader\n",
      "            nltk.corpus.reader.wordnet.WordNetCorpusReader\n",
      "            nltk.corpus.reader.wordnet.WordNetICCorpusReader\n",
      "            nltk.corpus.reader.xmldocs.XMLCorpusReader\n",
      "                nltk.corpus.reader.bnc.BNCCorpusReader\n",
      "                nltk.corpus.reader.childes.CHILDESCorpusReader\n",
      "                nltk.corpus.reader.framenet.FramenetCorpusReader\n",
      "                nltk.corpus.reader.nkjp.NKJPCorpusReader\n",
      "                nltk.corpus.reader.nps_chat.NPSChatCorpusReader\n",
      "                nltk.corpus.reader.rte.RTECorpusReader\n",
      "                nltk.corpus.reader.semcor.SemcorCorpusReader\n",
      "                nltk.corpus.reader.verbnet.VerbnetCorpusReader\n",
      "            nltk.corpus.reader.ycoe.YCOECorpusReader\n",
      "        nltk.corpus.reader.sentiwordnet.SentiSynset\n",
      "    nltk.corpus.reader.util.StreamBackedCorpusView(nltk.collections.AbstractLazySequence)\n",
      "        nltk.corpus.reader.pl196x.TEICorpusView\n",
      "    \n",
      "    class AlignedCorpusReader(nltk.corpus.reader.api.CorpusReader)\n",
      "     |  AlignedCorpusReader(root, fileids, sep='/', word_tokenizer=WhitespaceTokenizer(pattern='\\\\s+', gaps=True, discard_empty=True, flags=<RegexFlag.UNICODE|DOTALL|MULTILINE: 56>), sent_tokenizer=RegexpTokenizer(pattern='\\n', gaps=True, discard_empty=True, flags=<RegexFlag.UNICODE|DOTALL|MULTILINE: 56>), alignedsent_block_reader=<function read_alignedsent_block at 0x1259113b0>, encoding='latin1')\n",
      "     |  \n",
      "     |  Reader for corpora of word-aligned sentences.  Tokens are assumed\n",
      "     |  to be separated by whitespace.  Sentences begin on separate lines.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      AlignedCorpusReader\n",
      "     |      nltk.corpus.reader.api.CorpusReader\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, root, fileids, sep='/', word_tokenizer=WhitespaceTokenizer(pattern='\\\\s+', gaps=True, discard_empty=True, flags=<RegexFlag.UNICODE|DOTALL|MULTILINE: 56>), sent_tokenizer=RegexpTokenizer(pattern='\\n', gaps=True, discard_empty=True, flags=<RegexFlag.UNICODE|DOTALL|MULTILINE: 56>), alignedsent_block_reader=<function read_alignedsent_block at 0x1259113b0>, encoding='latin1')\n",
      "     |      Construct a new Aligned Corpus reader for a set of documents\n",
      "     |      located at the given root directory.  Example usage:\n",
      "     |      \n",
      "     |          >>> root = '/...path to corpus.../'\n",
      "     |          >>> reader = AlignedCorpusReader(root, '.*', '.txt') # doctest: +SKIP\n",
      "     |      \n",
      "     |      :param root: The root directory for this corpus.\n",
      "     |      :param fileids: A list or regexp specifying the fileids in this corpus.\n",
      "     |  \n",
      "     |  aligned_sents(self, fileids=None)\n",
      "     |      :return: the given file(s) as a list of AlignedSent objects.\n",
      "     |      :rtype: list(AlignedSent)\n",
      "     |  \n",
      "     |  raw(self, fileids=None)\n",
      "     |      :return: the given file(s) as a single string.\n",
      "     |      :rtype: str\n",
      "     |  \n",
      "     |  sents(self, fileids=None)\n",
      "     |      :return: the given file(s) as a list of\n",
      "     |          sentences or utterances, each encoded as a list of word\n",
      "     |          strings.\n",
      "     |      :rtype: list(list(str))\n",
      "     |  \n",
      "     |  words(self, fileids=None)\n",
      "     |      :return: the given file(s) as a list of words\n",
      "     |          and punctuation symbols.\n",
      "     |      :rtype: list(str)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __unicode__ = __str__(self, /)\n",
      "     |  \n",
      "     |  abspath(self, fileid)\n",
      "     |      Return the absolute path for the given file.\n",
      "     |      \n",
      "     |      :type fileid: str\n",
      "     |      :param fileid: The file identifier for the file whose path\n",
      "     |          should be returned.\n",
      "     |      :rtype: PathPointer\n",
      "     |  \n",
      "     |  abspaths(self, fileids=None, include_encoding=False, include_fileid=False)\n",
      "     |      Return a list of the absolute paths for all fileids in this corpus;\n",
      "     |      or for the given list of fileids, if specified.\n",
      "     |      \n",
      "     |      :type fileids: None or str or list\n",
      "     |      :param fileids: Specifies the set of fileids for which paths should\n",
      "     |          be returned.  Can be None, for all fileids; a list of\n",
      "     |          file identifiers, for a specified set of fileids; or a single\n",
      "     |          file identifier, for a single file.  Note that the return\n",
      "     |          value is always a list of paths, even if ``fileids`` is a\n",
      "     |          single file identifier.\n",
      "     |      \n",
      "     |      :param include_encoding: If true, then return a list of\n",
      "     |          ``(path_pointer, encoding)`` tuples.\n",
      "     |      \n",
      "     |      :rtype: list(PathPointer)\n",
      "     |  \n",
      "     |  citation(self)\n",
      "     |      Return the contents of the corpus citation.bib file, if it exists.\n",
      "     |  \n",
      "     |  encoding(self, file)\n",
      "     |      Return the unicode encoding for the given corpus file, if known.\n",
      "     |      If the encoding is unknown, or if the given file should be\n",
      "     |      processed using byte strings (str), then return None.\n",
      "     |  \n",
      "     |  ensure_loaded(self)\n",
      "     |      Load this corpus (if it has not already been loaded).  This is\n",
      "     |      used by LazyCorpusLoader as a simple method that can be used to\n",
      "     |      make sure a corpus is loaded -- e.g., in case a user wants to\n",
      "     |      do help(some_corpus).\n",
      "     |  \n",
      "     |  fileids(self)\n",
      "     |      Return a list of file identifiers for the fileids that make up\n",
      "     |      this corpus.\n",
      "     |  \n",
      "     |  license(self)\n",
      "     |      Return the contents of the corpus LICENSE file, if it exists.\n",
      "     |  \n",
      "     |  open(self, file)\n",
      "     |      Return an open stream that can be used to read the given file.\n",
      "     |      If the file's encoding is not None, then the stream will\n",
      "     |      automatically decode the file's contents into unicode.\n",
      "     |      \n",
      "     |      :param file: The file identifier of the file to read.\n",
      "     |  \n",
      "     |  readme(self)\n",
      "     |      Return the contents of the corpus README file, if it exists.\n",
      "     |  \n",
      "     |  unicode_repr = __repr__(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  root\n",
      "     |      The directory where this corpus is stored.\n",
      "     |      \n",
      "     |      :type: PathPointer\n",
      "    \n",
      "    class AlpinoCorpusReader(BracketParseCorpusReader)\n",
      "     |  AlpinoCorpusReader(root, encoding='ISO-8859-1', tagset=None)\n",
      "     |  \n",
      "     |  Reader for the Alpino Dutch Treebank.\n",
      "     |  This corpus has a lexical breakdown structure embedded, as read by _parse\n",
      "     |  Unfortunately this puts punctuation and some other words out of the sentence\n",
      "     |  order in the xml element tree. This is no good for tag_ and word_\n",
      "     |  _tag and _word will be overridden to use a non-default new parameter 'ordered'\n",
      "     |  to the overridden _normalize function. The _parse function can then remain\n",
      "     |  untouched.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      AlpinoCorpusReader\n",
      "     |      BracketParseCorpusReader\n",
      "     |      nltk.corpus.reader.api.SyntaxCorpusReader\n",
      "     |      nltk.corpus.reader.api.CorpusReader\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, root, encoding='ISO-8859-1', tagset=None)\n",
      "     |      :param root: The root directory for this corpus.\n",
      "     |      :param fileids: A list or regexp specifying the fileids in this corpus.\n",
      "     |      :param comment_char: The character which can appear at the start of\n",
      "     |          a line to indicate that the rest of the line is a comment.\n",
      "     |      :param detect_blocks: The method that is used to find blocks\n",
      "     |        in the corpus; can be 'unindented_paren' (every unindented\n",
      "     |        parenthesis starts a new parse) or 'sexpr' (brackets are\n",
      "     |        matched).\n",
      "     |      :param tagset: The name of the tagset used by this corpus, to be used\n",
      "     |            for normalizing or converting the POS tags returned by the\n",
      "     |            tagged_...() methods.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from nltk.corpus.reader.api.SyntaxCorpusReader:\n",
      "     |  \n",
      "     |  parsed_sents(self, fileids=None)\n",
      "     |  \n",
      "     |  raw(self, fileids=None)\n",
      "     |  \n",
      "     |  sents(self, fileids=None)\n",
      "     |  \n",
      "     |  tagged_sents(self, fileids=None, tagset=None)\n",
      "     |  \n",
      "     |  tagged_words(self, fileids=None, tagset=None)\n",
      "     |  \n",
      "     |  words(self, fileids=None)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __unicode__ = __str__(self, /)\n",
      "     |  \n",
      "     |  abspath(self, fileid)\n",
      "     |      Return the absolute path for the given file.\n",
      "     |      \n",
      "     |      :type fileid: str\n",
      "     |      :param fileid: The file identifier for the file whose path\n",
      "     |          should be returned.\n",
      "     |      :rtype: PathPointer\n",
      "     |  \n",
      "     |  abspaths(self, fileids=None, include_encoding=False, include_fileid=False)\n",
      "     |      Return a list of the absolute paths for all fileids in this corpus;\n",
      "     |      or for the given list of fileids, if specified.\n",
      "     |      \n",
      "     |      :type fileids: None or str or list\n",
      "     |      :param fileids: Specifies the set of fileids for which paths should\n",
      "     |          be returned.  Can be None, for all fileids; a list of\n",
      "     |          file identifiers, for a specified set of fileids; or a single\n",
      "     |          file identifier, for a single file.  Note that the return\n",
      "     |          value is always a list of paths, even if ``fileids`` is a\n",
      "     |          single file identifier.\n",
      "     |      \n",
      "     |      :param include_encoding: If true, then return a list of\n",
      "     |          ``(path_pointer, encoding)`` tuples.\n",
      "     |      \n",
      "     |      :rtype: list(PathPointer)\n",
      "     |  \n",
      "     |  citation(self)\n",
      "     |      Return the contents of the corpus citation.bib file, if it exists.\n",
      "     |  \n",
      "     |  encoding(self, file)\n",
      "     |      Return the unicode encoding for the given corpus file, if known.\n",
      "     |      If the encoding is unknown, or if the given file should be\n",
      "     |      processed using byte strings (str), then return None.\n",
      "     |  \n",
      "     |  ensure_loaded(self)\n",
      "     |      Load this corpus (if it has not already been loaded).  This is\n",
      "     |      used by LazyCorpusLoader as a simple method that can be used to\n",
      "     |      make sure a corpus is loaded -- e.g., in case a user wants to\n",
      "     |      do help(some_corpus).\n",
      "     |  \n",
      "     |  fileids(self)\n",
      "     |      Return a list of file identifiers for the fileids that make up\n",
      "     |      this corpus.\n",
      "     |  \n",
      "     |  license(self)\n",
      "     |      Return the contents of the corpus LICENSE file, if it exists.\n",
      "     |  \n",
      "     |  open(self, file)\n",
      "     |      Return an open stream that can be used to read the given file.\n",
      "     |      If the file's encoding is not None, then the stream will\n",
      "     |      automatically decode the file's contents into unicode.\n",
      "     |      \n",
      "     |      :param file: The file identifier of the file to read.\n",
      "     |  \n",
      "     |  readme(self)\n",
      "     |      Return the contents of the corpus README file, if it exists.\n",
      "     |  \n",
      "     |  unicode_repr = __repr__(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  root\n",
      "     |      The directory where this corpus is stored.\n",
      "     |      \n",
      "     |      :type: PathPointer\n",
      "    \n",
      "    class BNCCorpusReader(nltk.corpus.reader.xmldocs.XMLCorpusReader)\n",
      "     |  BNCCorpusReader(root, fileids, lazy=True)\n",
      "     |  \n",
      "     |  Corpus reader for the XML version of the British National Corpus.\n",
      "     |  \n",
      "     |  For access to the complete XML data structure, use the ``xml()``\n",
      "     |  method.  For access to simple word lists and tagged word lists, use\n",
      "     |  ``words()``, ``sents()``, ``tagged_words()``, and ``tagged_sents()``.\n",
      "     |  \n",
      "     |  You can obtain the full version of the BNC corpus at\n",
      "     |  http://www.ota.ox.ac.uk/desc/2554\n",
      "     |  \n",
      "     |  If you extracted the archive to a directory called `BNC`, then you can\n",
      "     |  instantiate the reader as::\n",
      "     |  \n",
      "     |      BNCCorpusReader(root='BNC/Texts/', fileids=r'[A-K]/\\w*/\\w*\\.xml')\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      BNCCorpusReader\n",
      "     |      nltk.corpus.reader.xmldocs.XMLCorpusReader\n",
      "     |      nltk.corpus.reader.api.CorpusReader\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, root, fileids, lazy=True)\n",
      "     |      :type root: PathPointer or str\n",
      "     |      :param root: A path pointer identifying the root directory for\n",
      "     |          this corpus.  If a string is specified, then it will be\n",
      "     |          converted to a ``PathPointer`` automatically.\n",
      "     |      :param fileids: A list of the files that make up this corpus.\n",
      "     |          This list can either be specified explicitly, as a list of\n",
      "     |          strings; or implicitly, as a regular expression over file\n",
      "     |          paths.  The absolute path for each file will be constructed\n",
      "     |          by joining the reader's root to each file name.\n",
      "     |      :param encoding: The default unicode encoding for the files\n",
      "     |          that make up the corpus.  The value of ``encoding`` can be any\n",
      "     |          of the following:\n",
      "     |          - A string: ``encoding`` is the encoding name for all files.\n",
      "     |          - A dictionary: ``encoding[file_id]`` is the encoding\n",
      "     |            name for the file whose identifier is ``file_id``.  If\n",
      "     |            ``file_id`` is not in ``encoding``, then the file\n",
      "     |            contents will be processed using non-unicode byte strings.\n",
      "     |          - A list: ``encoding`` should be a list of ``(regexp, encoding)``\n",
      "     |            tuples.  The encoding for a file whose identifier is ``file_id``\n",
      "     |            will be the ``encoding`` value for the first tuple whose\n",
      "     |            ``regexp`` matches the ``file_id``.  If no tuple's ``regexp``\n",
      "     |            matches the ``file_id``, the file contents will be processed\n",
      "     |            using non-unicode byte strings.\n",
      "     |          - None: the file contents of all files will be\n",
      "     |            processed using non-unicode byte strings.\n",
      "     |      :param tagset: The name of the tagset used by this corpus, to be used\n",
      "     |            for normalizing or converting the POS tags returned by the\n",
      "     |            tagged_...() methods.\n",
      "     |  \n",
      "     |  sents(self, fileids=None, strip_space=True, stem=False)\n",
      "     |      :return: the given file(s) as a list of\n",
      "     |          sentences or utterances, each encoded as a list of word\n",
      "     |          strings.\n",
      "     |      :rtype: list(list(str))\n",
      "     |      \n",
      "     |      :param strip_space: If true, then strip trailing spaces from\n",
      "     |          word tokens.  Otherwise, leave the spaces on the tokens.\n",
      "     |      :param stem: If true, then use word stems instead of word strings.\n",
      "     |  \n",
      "     |  tagged_sents(self, fileids=None, c5=False, strip_space=True, stem=False)\n",
      "     |      :return: the given file(s) as a list of\n",
      "     |          sentences, each encoded as a list of ``(word,tag)`` tuples.\n",
      "     |      :rtype: list(list(tuple(str,str)))\n",
      "     |      \n",
      "     |      :param c5: If true, then the tags used will be the more detailed\n",
      "     |          c5 tags.  Otherwise, the simplified tags will be used.\n",
      "     |      :param strip_space: If true, then strip trailing spaces from\n",
      "     |          word tokens.  Otherwise, leave the spaces on the tokens.\n",
      "     |      :param stem: If true, then use word stems instead of word strings.\n",
      "     |  \n",
      "     |  tagged_words(self, fileids=None, c5=False, strip_space=True, stem=False)\n",
      "     |      :return: the given file(s) as a list of tagged\n",
      "     |          words and punctuation symbols, encoded as tuples\n",
      "     |          ``(word,tag)``.\n",
      "     |      :rtype: list(tuple(str,str))\n",
      "     |      \n",
      "     |      :param c5: If true, then the tags used will be the more detailed\n",
      "     |          c5 tags.  Otherwise, the simplified tags will be used.\n",
      "     |      :param strip_space: If true, then strip trailing spaces from\n",
      "     |          word tokens.  Otherwise, leave the spaces on the tokens.\n",
      "     |      :param stem: If true, then use word stems instead of word strings.\n",
      "     |  \n",
      "     |  words(self, fileids=None, strip_space=True, stem=False)\n",
      "     |      :return: the given file(s) as a list of words\n",
      "     |          and punctuation symbols.\n",
      "     |      :rtype: list(str)\n",
      "     |      \n",
      "     |      :param strip_space: If true, then strip trailing spaces from\n",
      "     |          word tokens.  Otherwise, leave the spaces on the tokens.\n",
      "     |      :param stem: If true, then use word stems instead of word strings.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from nltk.corpus.reader.xmldocs.XMLCorpusReader:\n",
      "     |  \n",
      "     |  raw(self, fileids=None)\n",
      "     |  \n",
      "     |  xml(self, fileid=None)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __unicode__ = __str__(self, /)\n",
      "     |  \n",
      "     |  abspath(self, fileid)\n",
      "     |      Return the absolute path for the given file.\n",
      "     |      \n",
      "     |      :type fileid: str\n",
      "     |      :param fileid: The file identifier for the file whose path\n",
      "     |          should be returned.\n",
      "     |      :rtype: PathPointer\n",
      "     |  \n",
      "     |  abspaths(self, fileids=None, include_encoding=False, include_fileid=False)\n",
      "     |      Return a list of the absolute paths for all fileids in this corpus;\n",
      "     |      or for the given list of fileids, if specified.\n",
      "     |      \n",
      "     |      :type fileids: None or str or list\n",
      "     |      :param fileids: Specifies the set of fileids for which paths should\n",
      "     |          be returned.  Can be None, for all fileids; a list of\n",
      "     |          file identifiers, for a specified set of fileids; or a single\n",
      "     |          file identifier, for a single file.  Note that the return\n",
      "     |          value is always a list of paths, even if ``fileids`` is a\n",
      "     |          single file identifier.\n",
      "     |      \n",
      "     |      :param include_encoding: If true, then return a list of\n",
      "     |          ``(path_pointer, encoding)`` tuples.\n",
      "     |      \n",
      "     |      :rtype: list(PathPointer)\n",
      "     |  \n",
      "     |  citation(self)\n",
      "     |      Return the contents of the corpus citation.bib file, if it exists.\n",
      "     |  \n",
      "     |  encoding(self, file)\n",
      "     |      Return the unicode encoding for the given corpus file, if known.\n",
      "     |      If the encoding is unknown, or if the given file should be\n",
      "     |      processed using byte strings (str), then return None.\n",
      "     |  \n",
      "     |  ensure_loaded(self)\n",
      "     |      Load this corpus (if it has not already been loaded).  This is\n",
      "     |      used by LazyCorpusLoader as a simple method that can be used to\n",
      "     |      make sure a corpus is loaded -- e.g., in case a user wants to\n",
      "     |      do help(some_corpus).\n",
      "     |  \n",
      "     |  fileids(self)\n",
      "     |      Return a list of file identifiers for the fileids that make up\n",
      "     |      this corpus.\n",
      "     |  \n",
      "     |  license(self)\n",
      "     |      Return the contents of the corpus LICENSE file, if it exists.\n",
      "     |  \n",
      "     |  open(self, file)\n",
      "     |      Return an open stream that can be used to read the given file.\n",
      "     |      If the file's encoding is not None, then the stream will\n",
      "     |      automatically decode the file's contents into unicode.\n",
      "     |      \n",
      "     |      :param file: The file identifier of the file to read.\n",
      "     |  \n",
      "     |  readme(self)\n",
      "     |      Return the contents of the corpus README file, if it exists.\n",
      "     |  \n",
      "     |  unicode_repr = __repr__(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  root\n",
      "     |      The directory where this corpus is stored.\n",
      "     |      \n",
      "     |      :type: PathPointer\n",
      "    \n",
      "    class BracketParseCorpusReader(nltk.corpus.reader.api.SyntaxCorpusReader)\n",
      "     |  BracketParseCorpusReader(root, fileids, comment_char=None, detect_blocks='unindented_paren', encoding='utf8', tagset=None)\n",
      "     |  \n",
      "     |  Reader for corpora that consist of parenthesis-delineated parse trees,\n",
      "     |  like those found in the \"combined\" section of the Penn Treebank,\n",
      "     |  e.g. \"(S (NP (DT the) (JJ little) (NN dog)) (VP (VBD barked)))\".\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      BracketParseCorpusReader\n",
      "     |      nltk.corpus.reader.api.SyntaxCorpusReader\n",
      "     |      nltk.corpus.reader.api.CorpusReader\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, root, fileids, comment_char=None, detect_blocks='unindented_paren', encoding='utf8', tagset=None)\n",
      "     |      :param root: The root directory for this corpus.\n",
      "     |      :param fileids: A list or regexp specifying the fileids in this corpus.\n",
      "     |      :param comment_char: The character which can appear at the start of\n",
      "     |          a line to indicate that the rest of the line is a comment.\n",
      "     |      :param detect_blocks: The method that is used to find blocks\n",
      "     |        in the corpus; can be 'unindented_paren' (every unindented\n",
      "     |        parenthesis starts a new parse) or 'sexpr' (brackets are\n",
      "     |        matched).\n",
      "     |      :param tagset: The name of the tagset used by this corpus, to be used\n",
      "     |            for normalizing or converting the POS tags returned by the\n",
      "     |            tagged_...() methods.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from nltk.corpus.reader.api.SyntaxCorpusReader:\n",
      "     |  \n",
      "     |  parsed_sents(self, fileids=None)\n",
      "     |  \n",
      "     |  raw(self, fileids=None)\n",
      "     |  \n",
      "     |  sents(self, fileids=None)\n",
      "     |  \n",
      "     |  tagged_sents(self, fileids=None, tagset=None)\n",
      "     |  \n",
      "     |  tagged_words(self, fileids=None, tagset=None)\n",
      "     |  \n",
      "     |  words(self, fileids=None)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __unicode__ = __str__(self, /)\n",
      "     |  \n",
      "     |  abspath(self, fileid)\n",
      "     |      Return the absolute path for the given file.\n",
      "     |      \n",
      "     |      :type fileid: str\n",
      "     |      :param fileid: The file identifier for the file whose path\n",
      "     |          should be returned.\n",
      "     |      :rtype: PathPointer\n",
      "     |  \n",
      "     |  abspaths(self, fileids=None, include_encoding=False, include_fileid=False)\n",
      "     |      Return a list of the absolute paths for all fileids in this corpus;\n",
      "     |      or for the given list of fileids, if specified.\n",
      "     |      \n",
      "     |      :type fileids: None or str or list\n",
      "     |      :param fileids: Specifies the set of fileids for which paths should\n",
      "     |          be returned.  Can be None, for all fileids; a list of\n",
      "     |          file identifiers, for a specified set of fileids; or a single\n",
      "     |          file identifier, for a single file.  Note that the return\n",
      "     |          value is always a list of paths, even if ``fileids`` is a\n",
      "     |          single file identifier.\n",
      "     |      \n",
      "     |      :param include_encoding: If true, then return a list of\n",
      "     |          ``(path_pointer, encoding)`` tuples.\n",
      "     |      \n",
      "     |      :rtype: list(PathPointer)\n",
      "     |  \n",
      "     |  citation(self)\n",
      "     |      Return the contents of the corpus citation.bib file, if it exists.\n",
      "     |  \n",
      "     |  encoding(self, file)\n",
      "     |      Return the unicode encoding for the given corpus file, if known.\n",
      "     |      If the encoding is unknown, or if the given file should be\n",
      "     |      processed using byte strings (str), then return None.\n",
      "     |  \n",
      "     |  ensure_loaded(self)\n",
      "     |      Load this corpus (if it has not already been loaded).  This is\n",
      "     |      used by LazyCorpusLoader as a simple method that can be used to\n",
      "     |      make sure a corpus is loaded -- e.g., in case a user wants to\n",
      "     |      do help(some_corpus).\n",
      "     |  \n",
      "     |  fileids(self)\n",
      "     |      Return a list of file identifiers for the fileids that make up\n",
      "     |      this corpus.\n",
      "     |  \n",
      "     |  license(self)\n",
      "     |      Return the contents of the corpus LICENSE file, if it exists.\n",
      "     |  \n",
      "     |  open(self, file)\n",
      "     |      Return an open stream that can be used to read the given file.\n",
      "     |      If the file's encoding is not None, then the stream will\n",
      "     |      automatically decode the file's contents into unicode.\n",
      "     |      \n",
      "     |      :param file: The file identifier of the file to read.\n",
      "     |  \n",
      "     |  readme(self)\n",
      "     |      Return the contents of the corpus README file, if it exists.\n",
      "     |  \n",
      "     |  unicode_repr = __repr__(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  root\n",
      "     |      The directory where this corpus is stored.\n",
      "     |      \n",
      "     |      :type: PathPointer\n",
      "    \n",
      "    class CHILDESCorpusReader(nltk.corpus.reader.xmldocs.XMLCorpusReader)\n",
      "     |  CHILDESCorpusReader(root, fileids, lazy=True)\n",
      "     |  \n",
      "     |  Corpus reader for the XML version of the CHILDES corpus.\n",
      "     |  The CHILDES corpus is available at ``https://childes.talkbank.org/``. The XML\n",
      "     |  version of CHILDES is located at ``https://childes.talkbank.org/data-xml/``.\n",
      "     |  Copy the needed parts of the CHILDES XML corpus into the NLTK data directory\n",
      "     |  (``nltk_data/corpora/CHILDES/``).\n",
      "     |  \n",
      "     |  For access to the file text use the usual nltk functions,\n",
      "     |  ``words()``, ``sents()``, ``tagged_words()`` and ``tagged_sents()``.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      CHILDESCorpusReader\n",
      "     |      nltk.corpus.reader.xmldocs.XMLCorpusReader\n",
      "     |      nltk.corpus.reader.api.CorpusReader\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  MLU(self, fileids=None, speaker='CHI')\n",
      "     |      :return: the given file(s) as a floating number\n",
      "     |      :rtype: list(float)\n",
      "     |  \n",
      "     |  __init__(self, root, fileids, lazy=True)\n",
      "     |      :type root: PathPointer or str\n",
      "     |      :param root: A path pointer identifying the root directory for\n",
      "     |          this corpus.  If a string is specified, then it will be\n",
      "     |          converted to a ``PathPointer`` automatically.\n",
      "     |      :param fileids: A list of the files that make up this corpus.\n",
      "     |          This list can either be specified explicitly, as a list of\n",
      "     |          strings; or implicitly, as a regular expression over file\n",
      "     |          paths.  The absolute path for each file will be constructed\n",
      "     |          by joining the reader's root to each file name.\n",
      "     |      :param encoding: The default unicode encoding for the files\n",
      "     |          that make up the corpus.  The value of ``encoding`` can be any\n",
      "     |          of the following:\n",
      "     |          - A string: ``encoding`` is the encoding name for all files.\n",
      "     |          - A dictionary: ``encoding[file_id]`` is the encoding\n",
      "     |            name for the file whose identifier is ``file_id``.  If\n",
      "     |            ``file_id`` is not in ``encoding``, then the file\n",
      "     |            contents will be processed using non-unicode byte strings.\n",
      "     |          - A list: ``encoding`` should be a list of ``(regexp, encoding)``\n",
      "     |            tuples.  The encoding for a file whose identifier is ``file_id``\n",
      "     |            will be the ``encoding`` value for the first tuple whose\n",
      "     |            ``regexp`` matches the ``file_id``.  If no tuple's ``regexp``\n",
      "     |            matches the ``file_id``, the file contents will be processed\n",
      "     |            using non-unicode byte strings.\n",
      "     |          - None: the file contents of all files will be\n",
      "     |            processed using non-unicode byte strings.\n",
      "     |      :param tagset: The name of the tagset used by this corpus, to be used\n",
      "     |            for normalizing or converting the POS tags returned by the\n",
      "     |            tagged_...() methods.\n",
      "     |  \n",
      "     |  age(self, fileids=None, speaker='CHI', month=False)\n",
      "     |      :return: the given file(s) as string or int\n",
      "     |      :rtype: list or int\n",
      "     |      \n",
      "     |      :param month: If true, return months instead of year-month-date\n",
      "     |  \n",
      "     |  convert_age(self, age_year)\n",
      "     |      Caclculate age in months from a string in CHILDES format\n",
      "     |  \n",
      "     |  corpus(self, fileids=None)\n",
      "     |      :return: the given file(s) as a dict of ``(corpus_property_key, value)``\n",
      "     |      :rtype: list(dict)\n",
      "     |  \n",
      "     |  participants(self, fileids=None)\n",
      "     |      :return: the given file(s) as a dict of\n",
      "     |          ``(participant_property_key, value)``\n",
      "     |      :rtype: list(dict)\n",
      "     |  \n",
      "     |  sents(self, fileids=None, speaker='ALL', stem=False, relation=None, strip_space=True, replace=False)\n",
      "     |      :return: the given file(s) as a list of sentences or utterances, each\n",
      "     |          encoded as a list of word strings.\n",
      "     |      :rtype: list(list(str))\n",
      "     |      \n",
      "     |      :param speaker: If specified, select specific speaker(s) defined\n",
      "     |          in the corpus. Default is 'ALL' (all participants). Common choices\n",
      "     |          are 'CHI' (the child), 'MOT' (mother), ['CHI','MOT'] (exclude\n",
      "     |          researchers)\n",
      "     |      :param stem: If true, then use word stems instead of word strings.\n",
      "     |      :param relation: If true, then return tuples of ``(str,pos,relation_list)``.\n",
      "     |          If there is manually-annotated relation info, it will return\n",
      "     |          tuples of ``(str,pos,test_relation_list,str,pos,gold_relation_list)``\n",
      "     |      :param strip_space: If true, then strip trailing spaces from word\n",
      "     |          tokens. Otherwise, leave the spaces on the tokens.\n",
      "     |      :param replace: If true, then use the replaced (intended) word instead\n",
      "     |          of the original word (e.g., 'wat' will be replaced with 'watch')\n",
      "     |  \n",
      "     |  tagged_sents(self, fileids=None, speaker='ALL', stem=False, relation=None, strip_space=True, replace=False)\n",
      "     |      :return: the given file(s) as a list of\n",
      "     |          sentences, each encoded as a list of ``(word,tag)`` tuples.\n",
      "     |      :rtype: list(list(tuple(str,str)))\n",
      "     |      \n",
      "     |      :param speaker: If specified, select specific speaker(s) defined\n",
      "     |          in the corpus. Default is 'ALL' (all participants). Common choices\n",
      "     |          are 'CHI' (the child), 'MOT' (mother), ['CHI','MOT'] (exclude\n",
      "     |          researchers)\n",
      "     |      :param stem: If true, then use word stems instead of word strings.\n",
      "     |      :param relation: If true, then return tuples of ``(str,pos,relation_list)``.\n",
      "     |          If there is manually-annotated relation info, it will return\n",
      "     |          tuples of ``(str,pos,test_relation_list,str,pos,gold_relation_list)``\n",
      "     |      :param strip_space: If true, then strip trailing spaces from word\n",
      "     |          tokens. Otherwise, leave the spaces on the tokens.\n",
      "     |      :param replace: If true, then use the replaced (intended) word instead\n",
      "     |          of the original word (e.g., 'wat' will be replaced with 'watch')\n",
      "     |  \n",
      "     |  tagged_words(self, fileids=None, speaker='ALL', stem=False, relation=False, strip_space=True, replace=False)\n",
      "     |      :return: the given file(s) as a list of tagged\n",
      "     |          words and punctuation symbols, encoded as tuples\n",
      "     |          ``(word,tag)``.\n",
      "     |      :rtype: list(tuple(str,str))\n",
      "     |      \n",
      "     |      :param speaker: If specified, select specific speaker(s) defined\n",
      "     |          in the corpus. Default is 'ALL' (all participants). Common choices\n",
      "     |          are 'CHI' (the child), 'MOT' (mother), ['CHI','MOT'] (exclude\n",
      "     |          researchers)\n",
      "     |      :param stem: If true, then use word stems instead of word strings.\n",
      "     |      :param relation: If true, then return tuples of (stem, index,\n",
      "     |          dependent_index)\n",
      "     |      :param strip_space: If true, then strip trailing spaces from word\n",
      "     |          tokens. Otherwise, leave the spaces on the tokens.\n",
      "     |      :param replace: If true, then use the replaced (intended) word instead\n",
      "     |          of the original word (e.g., 'wat' will be replaced with 'watch')\n",
      "     |  \n",
      "     |  webview_file(self, fileid, urlbase=None)\n",
      "     |      Map a corpus file to its web version on the CHILDES website,\n",
      "     |      and open it in a web browser.\n",
      "     |      \n",
      "     |      The complete URL to be used is:\n",
      "     |          childes.childes_url_base + urlbase + fileid.replace('.xml', '.cha')\n",
      "     |      \n",
      "     |      If no urlbase is passed, we try to calculate it.  This\n",
      "     |      requires that the childes corpus was set up to mirror the\n",
      "     |      folder hierarchy under childes.psy.cmu.edu/data-xml/, e.g.:\n",
      "     |      nltk_data/corpora/childes/Eng-USA/Cornell/??? or\n",
      "     |      nltk_data/corpora/childes/Romance/Spanish/Aguirre/???\n",
      "     |      \n",
      "     |      The function first looks (as a special case) if \"Eng-USA\" is\n",
      "     |      on the path consisting of <corpus root>+fileid; then if\n",
      "     |      \"childes\", possibly followed by \"data-xml\", appears. If neither\n",
      "     |      one is found, we use the unmodified fileid and hope for the best.\n",
      "     |      If this is not right, specify urlbase explicitly, e.g., if the\n",
      "     |      corpus root points to the Cornell folder, urlbase='Eng-USA/Cornell'.\n",
      "     |  \n",
      "     |  words(self, fileids=None, speaker='ALL', stem=False, relation=False, strip_space=True, replace=False)\n",
      "     |      :return: the given file(s) as a list of words\n",
      "     |      :rtype: list(str)\n",
      "     |      \n",
      "     |      :param speaker: If specified, select specific speaker(s) defined\n",
      "     |          in the corpus. Default is 'ALL' (all participants). Common choices\n",
      "     |          are 'CHI' (the child), 'MOT' (mother), ['CHI','MOT'] (exclude\n",
      "     |          researchers)\n",
      "     |      :param stem: If true, then use word stems instead of word strings.\n",
      "     |      :param relation: If true, then return tuples of (stem, index,\n",
      "     |          dependent_index)\n",
      "     |      :param strip_space: If true, then strip trailing spaces from word\n",
      "     |          tokens. Otherwise, leave the spaces on the tokens.\n",
      "     |      :param replace: If true, then use the replaced (intended) word instead\n",
      "     |          of the original word (e.g., 'wat' will be replaced with 'watch')\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  childes_url_base = 'https://childes.talkbank.org/browser/index.php?url...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from nltk.corpus.reader.xmldocs.XMLCorpusReader:\n",
      "     |  \n",
      "     |  raw(self, fileids=None)\n",
      "     |  \n",
      "     |  xml(self, fileid=None)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __unicode__ = __str__(self, /)\n",
      "     |  \n",
      "     |  abspath(self, fileid)\n",
      "     |      Return the absolute path for the given file.\n",
      "     |      \n",
      "     |      :type fileid: str\n",
      "     |      :param fileid: The file identifier for the file whose path\n",
      "     |          should be returned.\n",
      "     |      :rtype: PathPointer\n",
      "     |  \n",
      "     |  abspaths(self, fileids=None, include_encoding=False, include_fileid=False)\n",
      "     |      Return a list of the absolute paths for all fileids in this corpus;\n",
      "     |      or for the given list of fileids, if specified.\n",
      "     |      \n",
      "     |      :type fileids: None or str or list\n",
      "     |      :param fileids: Specifies the set of fileids for which paths should\n",
      "     |          be returned.  Can be None, for all fileids; a list of\n",
      "     |          file identifiers, for a specified set of fileids; or a single\n",
      "     |          file identifier, for a single file.  Note that the return\n",
      "     |          value is always a list of paths, even if ``fileids`` is a\n",
      "     |          single file identifier.\n",
      "     |      \n",
      "     |      :param include_encoding: If true, then return a list of\n",
      "     |          ``(path_pointer, encoding)`` tuples.\n",
      "     |      \n",
      "     |      :rtype: list(PathPointer)\n",
      "     |  \n",
      "     |  citation(self)\n",
      "     |      Return the contents of the corpus citation.bib file, if it exists.\n",
      "     |  \n",
      "     |  encoding(self, file)\n",
      "     |      Return the unicode encoding for the given corpus file, if known.\n",
      "     |      If the encoding is unknown, or if the given file should be\n",
      "     |      processed using byte strings (str), then return None.\n",
      "     |  \n",
      "     |  ensure_loaded(self)\n",
      "     |      Load this corpus (if it has not already been loaded).  This is\n",
      "     |      used by LazyCorpusLoader as a simple method that can be used to\n",
      "     |      make sure a corpus is loaded -- e.g., in case a user wants to\n",
      "     |      do help(some_corpus).\n",
      "     |  \n",
      "     |  fileids(self)\n",
      "     |      Return a list of file identifiers for the fileids that make up\n",
      "     |      this corpus.\n",
      "     |  \n",
      "     |  license(self)\n",
      "     |      Return the contents of the corpus LICENSE file, if it exists.\n",
      "     |  \n",
      "     |  open(self, file)\n",
      "     |      Return an open stream that can be used to read the given file.\n",
      "     |      If the file's encoding is not None, then the stream will\n",
      "     |      automatically decode the file's contents into unicode.\n",
      "     |      \n",
      "     |      :param file: The file identifier of the file to read.\n",
      "     |  \n",
      "     |  readme(self)\n",
      "     |      Return the contents of the corpus README file, if it exists.\n",
      "     |  \n",
      "     |  unicode_repr = __repr__(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  root\n",
      "     |      The directory where this corpus is stored.\n",
      "     |      \n",
      "     |      :type: PathPointer\n",
      "    \n",
      "    class CMUDictCorpusReader(nltk.corpus.reader.api.CorpusReader)\n",
      "     |  CMUDictCorpusReader(root, fileids, encoding='utf8', tagset=None)\n",
      "     |  \n",
      "     |  A base class for \"corpus reader\" classes, each of which can be\n",
      "     |  used to read a specific corpus format.  Each individual corpus\n",
      "     |  reader instance is used to read a specific corpus, consisting of\n",
      "     |  one or more files under a common root directory.  Each file is\n",
      "     |  identified by its ``file identifier``, which is the relative path\n",
      "     |  to the file from the root directory.\n",
      "     |  \n",
      "     |  A separate subclass is defined for each corpus format.  These\n",
      "     |  subclasses define one or more methods that provide 'views' on the\n",
      "     |  corpus contents, such as ``words()`` (for a list of words) and\n",
      "     |  ``parsed_sents()`` (for a list of parsed sentences).  Called with\n",
      "     |  no arguments, these methods will return the contents of the entire\n",
      "     |  corpus.  For most corpora, these methods define one or more\n",
      "     |  selection arguments, such as ``fileids`` or ``categories``, which can\n",
      "     |  be used to select which portion of the corpus should be returned.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      CMUDictCorpusReader\n",
      "     |      nltk.corpus.reader.api.CorpusReader\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  dict(self)\n",
      "     |      :return: the cmudict lexicon as a dictionary, whose keys are\n",
      "     |      lowercase words and whose values are lists of pronunciations.\n",
      "     |  \n",
      "     |  entries(self)\n",
      "     |      :return: the cmudict lexicon as a list of entries\n",
      "     |      containing (word, transcriptions) tuples.\n",
      "     |  \n",
      "     |  raw(self)\n",
      "     |      :return: the cmudict lexicon as a raw string.\n",
      "     |  \n",
      "     |  words(self)\n",
      "     |      :return: a list of all words defined in the cmudict lexicon.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __init__(self, root, fileids, encoding='utf8', tagset=None)\n",
      "     |      :type root: PathPointer or str\n",
      "     |      :param root: A path pointer identifying the root directory for\n",
      "     |          this corpus.  If a string is specified, then it will be\n",
      "     |          converted to a ``PathPointer`` automatically.\n",
      "     |      :param fileids: A list of the files that make up this corpus.\n",
      "     |          This list can either be specified explicitly, as a list of\n",
      "     |          strings; or implicitly, as a regular expression over file\n",
      "     |          paths.  The absolute path for each file will be constructed\n",
      "     |          by joining the reader's root to each file name.\n",
      "     |      :param encoding: The default unicode encoding for the files\n",
      "     |          that make up the corpus.  The value of ``encoding`` can be any\n",
      "     |          of the following:\n",
      "     |          - A string: ``encoding`` is the encoding name for all files.\n",
      "     |          - A dictionary: ``encoding[file_id]`` is the encoding\n",
      "     |            name for the file whose identifier is ``file_id``.  If\n",
      "     |            ``file_id`` is not in ``encoding``, then the file\n",
      "     |            contents will be processed using non-unicode byte strings.\n",
      "     |          - A list: ``encoding`` should be a list of ``(regexp, encoding)``\n",
      "     |            tuples.  The encoding for a file whose identifier is ``file_id``\n",
      "     |            will be the ``encoding`` value for the first tuple whose\n",
      "     |            ``regexp`` matches the ``file_id``.  If no tuple's ``regexp``\n",
      "     |            matches the ``file_id``, the file contents will be processed\n",
      "     |            using non-unicode byte strings.\n",
      "     |          - None: the file contents of all files will be\n",
      "     |            processed using non-unicode byte strings.\n",
      "     |      :param tagset: The name of the tagset used by this corpus, to be used\n",
      "     |            for normalizing or converting the POS tags returned by the\n",
      "     |            tagged_...() methods.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __unicode__ = __str__(self, /)\n",
      "     |  \n",
      "     |  abspath(self, fileid)\n",
      "     |      Return the absolute path for the given file.\n",
      "     |      \n",
      "     |      :type fileid: str\n",
      "     |      :param fileid: The file identifier for the file whose path\n",
      "     |          should be returned.\n",
      "     |      :rtype: PathPointer\n",
      "     |  \n",
      "     |  abspaths(self, fileids=None, include_encoding=False, include_fileid=False)\n",
      "     |      Return a list of the absolute paths for all fileids in this corpus;\n",
      "     |      or for the given list of fileids, if specified.\n",
      "     |      \n",
      "     |      :type fileids: None or str or list\n",
      "     |      :param fileids: Specifies the set of fileids for which paths should\n",
      "     |          be returned.  Can be None, for all fileids; a list of\n",
      "     |          file identifiers, for a specified set of fileids; or a single\n",
      "     |          file identifier, for a single file.  Note that the return\n",
      "     |          value is always a list of paths, even if ``fileids`` is a\n",
      "     |          single file identifier.\n",
      "     |      \n",
      "     |      :param include_encoding: If true, then return a list of\n",
      "     |          ``(path_pointer, encoding)`` tuples.\n",
      "     |      \n",
      "     |      :rtype: list(PathPointer)\n",
      "     |  \n",
      "     |  citation(self)\n",
      "     |      Return the contents of the corpus citation.bib file, if it exists.\n",
      "     |  \n",
      "     |  encoding(self, file)\n",
      "     |      Return the unicode encoding for the given corpus file, if known.\n",
      "     |      If the encoding is unknown, or if the given file should be\n",
      "     |      processed using byte strings (str), then return None.\n",
      "     |  \n",
      "     |  ensure_loaded(self)\n",
      "     |      Load this corpus (if it has not already been loaded).  This is\n",
      "     |      used by LazyCorpusLoader as a simple method that can be used to\n",
      "     |      make sure a corpus is loaded -- e.g., in case a user wants to\n",
      "     |      do help(some_corpus).\n",
      "     |  \n",
      "     |  fileids(self)\n",
      "     |      Return a list of file identifiers for the fileids that make up\n",
      "     |      this corpus.\n",
      "     |  \n",
      "     |  license(self)\n",
      "     |      Return the contents of the corpus LICENSE file, if it exists.\n",
      "     |  \n",
      "     |  open(self, file)\n",
      "     |      Return an open stream that can be used to read the given file.\n",
      "     |      If the file's encoding is not None, then the stream will\n",
      "     |      automatically decode the file's contents into unicode.\n",
      "     |      \n",
      "     |      :param file: The file identifier of the file to read.\n",
      "     |  \n",
      "     |  readme(self)\n",
      "     |      Return the contents of the corpus README file, if it exists.\n",
      "     |  \n",
      "     |  unicode_repr = __repr__(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  root\n",
      "     |      The directory where this corpus is stored.\n",
      "     |      \n",
      "     |      :type: PathPointer\n",
      "    \n",
      "    class CategorizedBracketParseCorpusReader(nltk.corpus.reader.api.CategorizedCorpusReader, BracketParseCorpusReader)\n",
      "     |  CategorizedBracketParseCorpusReader(*args, **kwargs)\n",
      "     |  \n",
      "     |  A reader for parsed corpora whose documents are\n",
      "     |  divided into categories based on their file identifiers.\n",
      "     |  @author: Nathan Schneider <nschneid@cs.cmu.edu>\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      CategorizedBracketParseCorpusReader\n",
      "     |      nltk.corpus.reader.api.CategorizedCorpusReader\n",
      "     |      BracketParseCorpusReader\n",
      "     |      nltk.corpus.reader.api.SyntaxCorpusReader\n",
      "     |      nltk.corpus.reader.api.CorpusReader\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, *args, **kwargs)\n",
      "     |      Initialize the corpus reader.  Categorization arguments\n",
      "     |      (C{cat_pattern}, C{cat_map}, and C{cat_file}) are passed to\n",
      "     |      the L{CategorizedCorpusReader constructor\n",
      "     |      <CategorizedCorpusReader.__init__>}.  The remaining arguments\n",
      "     |      are passed to the L{BracketParseCorpusReader constructor\n",
      "     |      <BracketParseCorpusReader.__init__>}.\n",
      "     |  \n",
      "     |  paras(self, fileids=None, categories=None)\n",
      "     |  \n",
      "     |  parsed_paras(self, fileids=None, categories=None)\n",
      "     |  \n",
      "     |  parsed_sents(self, fileids=None, categories=None)\n",
      "     |  \n",
      "     |  parsed_words(self, fileids=None, categories=None)\n",
      "     |  \n",
      "     |  raw(self, fileids=None, categories=None)\n",
      "     |  \n",
      "     |  sents(self, fileids=None, categories=None)\n",
      "     |  \n",
      "     |  tagged_paras(self, fileids=None, categories=None, tagset=None)\n",
      "     |  \n",
      "     |  tagged_sents(self, fileids=None, categories=None, tagset=None)\n",
      "     |  \n",
      "     |  tagged_words(self, fileids=None, categories=None, tagset=None)\n",
      "     |  \n",
      "     |  words(self, fileids=None, categories=None)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from nltk.corpus.reader.api.CategorizedCorpusReader:\n",
      "     |  \n",
      "     |  categories(self, fileids=None)\n",
      "     |      Return a list of the categories that are defined for this corpus,\n",
      "     |      or for the file(s) if it is given.\n",
      "     |  \n",
      "     |  fileids(self, categories=None)\n",
      "     |      Return a list of file identifiers for the files that make up\n",
      "     |      this corpus, or that make up the given category(s) if specified.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from nltk.corpus.reader.api.CategorizedCorpusReader:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __unicode__ = __str__(self, /)\n",
      "     |  \n",
      "     |  abspath(self, fileid)\n",
      "     |      Return the absolute path for the given file.\n",
      "     |      \n",
      "     |      :type fileid: str\n",
      "     |      :param fileid: The file identifier for the file whose path\n",
      "     |          should be returned.\n",
      "     |      :rtype: PathPointer\n",
      "     |  \n",
      "     |  abspaths(self, fileids=None, include_encoding=False, include_fileid=False)\n",
      "     |      Return a list of the absolute paths for all fileids in this corpus;\n",
      "     |      or for the given list of fileids, if specified.\n",
      "     |      \n",
      "     |      :type fileids: None or str or list\n",
      "     |      :param fileids: Specifies the set of fileids for which paths should\n",
      "     |          be returned.  Can be None, for all fileids; a list of\n",
      "     |          file identifiers, for a specified set of fileids; or a single\n",
      "     |          file identifier, for a single file.  Note that the return\n",
      "     |          value is always a list of paths, even if ``fileids`` is a\n",
      "     |          single file identifier.\n",
      "     |      \n",
      "     |      :param include_encoding: If true, then return a list of\n",
      "     |          ``(path_pointer, encoding)`` tuples.\n",
      "     |      \n",
      "     |      :rtype: list(PathPointer)\n",
      "     |  \n",
      "     |  citation(self)\n",
      "     |      Return the contents of the corpus citation.bib file, if it exists.\n",
      "     |  \n",
      "     |  encoding(self, file)\n",
      "     |      Return the unicode encoding for the given corpus file, if known.\n",
      "     |      If the encoding is unknown, or if the given file should be\n",
      "     |      processed using byte strings (str), then return None.\n",
      "     |  \n",
      "     |  ensure_loaded(self)\n",
      "     |      Load this corpus (if it has not already been loaded).  This is\n",
      "     |      used by LazyCorpusLoader as a simple method that can be used to\n",
      "     |      make sure a corpus is loaded -- e.g., in case a user wants to\n",
      "     |      do help(some_corpus).\n",
      "     |  \n",
      "     |  license(self)\n",
      "     |      Return the contents of the corpus LICENSE file, if it exists.\n",
      "     |  \n",
      "     |  open(self, file)\n",
      "     |      Return an open stream that can be used to read the given file.\n",
      "     |      If the file's encoding is not None, then the stream will\n",
      "     |      automatically decode the file's contents into unicode.\n",
      "     |      \n",
      "     |      :param file: The file identifier of the file to read.\n",
      "     |  \n",
      "     |  readme(self)\n",
      "     |      Return the contents of the corpus README file, if it exists.\n",
      "     |  \n",
      "     |  unicode_repr = __repr__(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  root\n",
      "     |      The directory where this corpus is stored.\n",
      "     |      \n",
      "     |      :type: PathPointer\n",
      "    \n",
      "    class CategorizedCorpusReader(builtins.object)\n",
      "     |  CategorizedCorpusReader(kwargs)\n",
      "     |  \n",
      "     |  A mixin class used to aid in the implementation of corpus readers\n",
      "     |  for categorized corpora.  This class defines the method\n",
      "     |  ``categories()``, which returns a list of the categories for the\n",
      "     |  corpus or for a specified set of fileids; and overrides ``fileids()``\n",
      "     |  to take a ``categories`` argument, restricting the set of fileids to\n",
      "     |  be returned.\n",
      "     |  \n",
      "     |  Subclasses are expected to:\n",
      "     |  \n",
      "     |    - Call ``__init__()`` to set up the mapping.\n",
      "     |  \n",
      "     |    - Override all view methods to accept a ``categories`` parameter,\n",
      "     |      which can be used *instead* of the ``fileids`` parameter, to\n",
      "     |      select which fileids should be included in the returned view.\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, kwargs)\n",
      "     |      Initialize this mapping based on keyword arguments, as\n",
      "     |      follows:\n",
      "     |      \n",
      "     |        - cat_pattern: A regular expression pattern used to find the\n",
      "     |          category for each file identifier.  The pattern will be\n",
      "     |          applied to each file identifier, and the first matching\n",
      "     |          group will be used as the category label for that file.\n",
      "     |      \n",
      "     |        - cat_map: A dictionary, mapping from file identifiers to\n",
      "     |          category labels.\n",
      "     |      \n",
      "     |        - cat_file: The name of a file that contains the mapping\n",
      "     |          from file identifiers to categories.  The argument\n",
      "     |          ``cat_delimiter`` can be used to specify a delimiter.\n",
      "     |      \n",
      "     |      The corresponding argument will be deleted from ``kwargs``.  If\n",
      "     |      more than one argument is specified, an exception will be\n",
      "     |      raised.\n",
      "     |  \n",
      "     |  categories(self, fileids=None)\n",
      "     |      Return a list of the categories that are defined for this corpus,\n",
      "     |      or for the file(s) if it is given.\n",
      "     |  \n",
      "     |  fileids(self, categories=None)\n",
      "     |      Return a list of file identifiers for the files that make up\n",
      "     |      this corpus, or that make up the given category(s) if specified.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class CategorizedPlaintextCorpusReader(nltk.corpus.reader.api.CategorizedCorpusReader, PlaintextCorpusReader)\n",
      "     |  CategorizedPlaintextCorpusReader(*args, **kwargs)\n",
      "     |  \n",
      "     |  A reader for plaintext corpora whose documents are divided into\n",
      "     |  categories based on their file identifiers.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      CategorizedPlaintextCorpusReader\n",
      "     |      nltk.corpus.reader.api.CategorizedCorpusReader\n",
      "     |      PlaintextCorpusReader\n",
      "     |      nltk.corpus.reader.api.CorpusReader\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, *args, **kwargs)\n",
      "     |      Initialize the corpus reader.  Categorization arguments\n",
      "     |      (``cat_pattern``, ``cat_map``, and ``cat_file``) are passed to\n",
      "     |      the ``CategorizedCorpusReader`` constructor.  The remaining arguments\n",
      "     |      are passed to the ``PlaintextCorpusReader`` constructor.\n",
      "     |  \n",
      "     |  paras(self, fileids=None, categories=None)\n",
      "     |      :return: the given file(s) as a list of\n",
      "     |          paragraphs, each encoded as a list of sentences, which are\n",
      "     |          in turn encoded as lists of word strings.\n",
      "     |      :rtype: list(list(list(str)))\n",
      "     |  \n",
      "     |  raw(self, fileids=None, categories=None)\n",
      "     |      :return: the given file(s) as a single string.\n",
      "     |      :rtype: str\n",
      "     |  \n",
      "     |  sents(self, fileids=None, categories=None)\n",
      "     |      :return: the given file(s) as a list of\n",
      "     |          sentences or utterances, each encoded as a list of word\n",
      "     |          strings.\n",
      "     |      :rtype: list(list(str))\n",
      "     |  \n",
      "     |  words(self, fileids=None, categories=None)\n",
      "     |      :return: the given file(s) as a list of words\n",
      "     |          and punctuation symbols.\n",
      "     |      :rtype: list(str)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from nltk.corpus.reader.api.CategorizedCorpusReader:\n",
      "     |  \n",
      "     |  categories(self, fileids=None)\n",
      "     |      Return a list of the categories that are defined for this corpus,\n",
      "     |      or for the file(s) if it is given.\n",
      "     |  \n",
      "     |  fileids(self, categories=None)\n",
      "     |      Return a list of file identifiers for the files that make up\n",
      "     |      this corpus, or that make up the given category(s) if specified.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from nltk.corpus.reader.api.CategorizedCorpusReader:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from PlaintextCorpusReader:\n",
      "     |  \n",
      "     |  CorpusView = <class 'nltk.corpus.reader.util.StreamBackedCorpusView'>\n",
      "     |      A 'view' of a corpus file, which acts like a sequence of tokens:\n",
      "     |      it can be accessed by index, iterated over, etc.  However, the\n",
      "     |      tokens are only constructed as-needed -- the entire corpus is\n",
      "     |      never stored in memory at once.\n",
      "     |      \n",
      "     |      The constructor to ``StreamBackedCorpusView`` takes two arguments:\n",
      "     |      a corpus fileid (specified as a string or as a ``PathPointer``);\n",
      "     |      and a block reader.  A \"block reader\" is a function that reads\n",
      "     |      zero or more tokens from a stream, and returns them as a list.  A\n",
      "     |      very simple example of a block reader is:\n",
      "     |      \n",
      "     |          >>> def simple_block_reader(stream):\n",
      "     |          ...     return stream.readline().split()\n",
      "     |      \n",
      "     |      This simple block reader reads a single line at a time, and\n",
      "     |      returns a single token (consisting of a string) for each\n",
      "     |      whitespace-separated substring on the line.\n",
      "     |      \n",
      "     |      When deciding how to define the block reader for a given\n",
      "     |      corpus, careful consideration should be given to the size of\n",
      "     |      blocks handled by the block reader.  Smaller block sizes will\n",
      "     |      increase the memory requirements of the corpus view's internal\n",
      "     |      data structures (by 2 integers per block).  On the other hand,\n",
      "     |      larger block sizes may decrease performance for random access to\n",
      "     |      the corpus.  (But note that larger block sizes will *not*\n",
      "     |      decrease performance for iteration.)\n",
      "     |      \n",
      "     |      Internally, ``CorpusView`` maintains a partial mapping from token\n",
      "     |      index to file position, with one entry per block.  When a token\n",
      "     |      with a given index *i* is requested, the ``CorpusView`` constructs\n",
      "     |      it as follows:\n",
      "     |      \n",
      "     |        1. First, it searches the toknum/filepos mapping for the token\n",
      "     |           index closest to (but less than or equal to) *i*.\n",
      "     |      \n",
      "     |        2. Then, starting at the file position corresponding to that\n",
      "     |           index, it reads one block at a time using the block reader\n",
      "     |           until it reaches the requested token.\n",
      "     |      \n",
      "     |      The toknum/filepos mapping is created lazily: it is initially\n",
      "     |      empty, but every time a new block is read, the block's\n",
      "     |      initial token is added to the mapping.  (Thus, the toknum/filepos\n",
      "     |      map has one entry per block.)\n",
      "     |      \n",
      "     |      In order to increase efficiency for random access patterns that\n",
      "     |      have high degrees of locality, the corpus view may cache one or\n",
      "     |      more blocks.\n",
      "     |      \n",
      "     |      :note: Each ``CorpusView`` object internally maintains an open file\n",
      "     |          object for its underlying corpus file.  This file should be\n",
      "     |          automatically closed when the ``CorpusView`` is garbage collected,\n",
      "     |          but if you wish to close it manually, use the ``close()``\n",
      "     |          method.  If you access a ``CorpusView``'s items after it has been\n",
      "     |          closed, the file object will be automatically re-opened.\n",
      "     |      \n",
      "     |      :warning: If the contents of the file are modified during the\n",
      "     |          lifetime of the ``CorpusView``, then the ``CorpusView``'s behavior\n",
      "     |          is undefined.\n",
      "     |      \n",
      "     |      :warning: If a unicode encoding is specified when constructing a\n",
      "     |          ``CorpusView``, then the block reader may only call\n",
      "     |          ``stream.seek()`` with offsets that have been returned by\n",
      "     |          ``stream.tell()``; in particular, calling ``stream.seek()`` with\n",
      "     |          relative offsets, or with offsets based on string lengths, may\n",
      "     |          lead to incorrect behavior.\n",
      "     |      \n",
      "     |      :ivar _block_reader: The function used to read\n",
      "     |          a single block from the underlying file stream.\n",
      "     |      :ivar _toknum: A list containing the token index of each block\n",
      "     |          that has been processed.  In particular, ``_toknum[i]`` is the\n",
      "     |          token index of the first token in block ``i``.  Together\n",
      "     |          with ``_filepos``, this forms a partial mapping between token\n",
      "     |          indices and file positions.\n",
      "     |      :ivar _filepos: A list containing the file position of each block\n",
      "     |          that has been processed.  In particular, ``_toknum[i]`` is the\n",
      "     |          file position of the first character in block ``i``.  Together\n",
      "     |          with ``_toknum``, this forms a partial mapping between token\n",
      "     |          indices and file positions.\n",
      "     |      :ivar _stream: The stream used to access the underlying corpus file.\n",
      "     |      :ivar _len: The total number of tokens in the corpus, if known;\n",
      "     |          or None, if the number of tokens is not yet known.\n",
      "     |      :ivar _eofpos: The character position of the last character in the\n",
      "     |          file.  This is calculated when the corpus view is initialized,\n",
      "     |          and is used to decide when the end of file has been reached.\n",
      "     |      :ivar _cache: A cache of the most recently read block.  It\n",
      "     |         is encoded as a tuple (start_toknum, end_toknum, tokens), where\n",
      "     |         start_toknum is the token index of the first token in the block;\n",
      "     |         end_toknum is the token index of the first token not in the\n",
      "     |         block; and tokens is a list of the tokens in the block.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __unicode__ = __str__(self, /)\n",
      "     |  \n",
      "     |  abspath(self, fileid)\n",
      "     |      Return the absolute path for the given file.\n",
      "     |      \n",
      "     |      :type fileid: str\n",
      "     |      :param fileid: The file identifier for the file whose path\n",
      "     |          should be returned.\n",
      "     |      :rtype: PathPointer\n",
      "     |  \n",
      "     |  abspaths(self, fileids=None, include_encoding=False, include_fileid=False)\n",
      "     |      Return a list of the absolute paths for all fileids in this corpus;\n",
      "     |      or for the given list of fileids, if specified.\n",
      "     |      \n",
      "     |      :type fileids: None or str or list\n",
      "     |      :param fileids: Specifies the set of fileids for which paths should\n",
      "     |          be returned.  Can be None, for all fileids; a list of\n",
      "     |          file identifiers, for a specified set of fileids; or a single\n",
      "     |          file identifier, for a single file.  Note that the return\n",
      "     |          value is always a list of paths, even if ``fileids`` is a\n",
      "     |          single file identifier.\n",
      "     |      \n",
      "     |      :param include_encoding: If true, then return a list of\n",
      "     |          ``(path_pointer, encoding)`` tuples.\n",
      "     |      \n",
      "     |      :rtype: list(PathPointer)\n",
      "     |  \n",
      "     |  citation(self)\n",
      "     |      Return the contents of the corpus citation.bib file, if it exists.\n",
      "     |  \n",
      "     |  encoding(self, file)\n",
      "     |      Return the unicode encoding for the given corpus file, if known.\n",
      "     |      If the encoding is unknown, or if the given file should be\n",
      "     |      processed using byte strings (str), then return None.\n",
      "     |  \n",
      "     |  ensure_loaded(self)\n",
      "     |      Load this corpus (if it has not already been loaded).  This is\n",
      "     |      used by LazyCorpusLoader as a simple method that can be used to\n",
      "     |      make sure a corpus is loaded -- e.g., in case a user wants to\n",
      "     |      do help(some_corpus).\n",
      "     |  \n",
      "     |  license(self)\n",
      "     |      Return the contents of the corpus LICENSE file, if it exists.\n",
      "     |  \n",
      "     |  open(self, file)\n",
      "     |      Return an open stream that can be used to read the given file.\n",
      "     |      If the file's encoding is not None, then the stream will\n",
      "     |      automatically decode the file's contents into unicode.\n",
      "     |      \n",
      "     |      :param file: The file identifier of the file to read.\n",
      "     |  \n",
      "     |  readme(self)\n",
      "     |      Return the contents of the corpus README file, if it exists.\n",
      "     |  \n",
      "     |  unicode_repr = __repr__(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  root\n",
      "     |      The directory where this corpus is stored.\n",
      "     |      \n",
      "     |      :type: PathPointer\n",
      "    \n",
      "    class CategorizedSentencesCorpusReader(nltk.corpus.reader.api.CategorizedCorpusReader, nltk.corpus.reader.api.CorpusReader)\n",
      "     |  CategorizedSentencesCorpusReader(root, fileids, word_tokenizer=WhitespaceTokenizer(pattern='\\\\s+', gaps=True, discard_empty=True, flags=<RegexFlag.UNICODE|DOTALL|MULTILINE: 56>), sent_tokenizer=None, encoding='utf8', **kwargs)\n",
      "     |  \n",
      "     |  A reader for corpora in which each row represents a single instance, mainly\n",
      "     |  a sentence. Istances are divided into categories based on their file identifiers\n",
      "     |  (see CategorizedCorpusReader).\n",
      "     |  Since many corpora allow rows that contain more than one sentence, it is\n",
      "     |  possible to specify a sentence tokenizer to retrieve all sentences instead\n",
      "     |  than all rows.\n",
      "     |  \n",
      "     |  Examples using the Subjectivity Dataset:\n",
      "     |  \n",
      "     |  >>> from nltk.corpus import subjectivity\n",
      "     |  >>> subjectivity.sents()[23]\n",
      "     |  ['television', 'made', 'him', 'famous', ',', 'but', 'his', 'biggest', 'hits',\n",
      "     |  'happened', 'off', 'screen', '.']\n",
      "     |  >>> subjectivity.categories()\n",
      "     |  ['obj', 'subj']\n",
      "     |  >>> subjectivity.words(categories='subj')\n",
      "     |  ['smart', 'and', 'alert', ',', 'thirteen', ...]\n",
      "     |  \n",
      "     |  Examples using the Sentence Polarity Dataset:\n",
      "     |  \n",
      "     |  >>> from nltk.corpus import sentence_polarity\n",
      "     |  >>> sentence_polarity.sents()\n",
      "     |  [['simplistic', ',', 'silly', 'and', 'tedious', '.'], [\"it's\", 'so', 'laddish',\n",
      "     |  'and', 'juvenile', ',', 'only', 'teenage', 'boys', 'could', 'possibly', 'find',\n",
      "     |  'it', 'funny', '.'], ...]\n",
      "     |  >>> sentence_polarity.categories()\n",
      "     |  ['neg', 'pos']\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      CategorizedSentencesCorpusReader\n",
      "     |      nltk.corpus.reader.api.CategorizedCorpusReader\n",
      "     |      nltk.corpus.reader.api.CorpusReader\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, root, fileids, word_tokenizer=WhitespaceTokenizer(pattern='\\\\s+', gaps=True, discard_empty=True, flags=<RegexFlag.UNICODE|DOTALL|MULTILINE: 56>), sent_tokenizer=None, encoding='utf8', **kwargs)\n",
      "     |      :param root: The root directory for the corpus.\n",
      "     |      :param fileids: a list or regexp specifying the fileids in the corpus.\n",
      "     |      :param word_tokenizer: a tokenizer for breaking sentences or paragraphs\n",
      "     |          into words. Default: `WhitespaceTokenizer`\n",
      "     |      :param sent_tokenizer: a tokenizer for breaking paragraphs into sentences.\n",
      "     |      :param encoding: the encoding that should be used to read the corpus.\n",
      "     |      :param kwargs: additional parameters passed to CategorizedCorpusReader.\n",
      "     |  \n",
      "     |  raw(self, fileids=None, categories=None)\n",
      "     |      :param fileids: a list or regexp specifying the fileids that have to be\n",
      "     |          returned as a raw string.\n",
      "     |      :param categories: a list specifying the categories whose files have to\n",
      "     |          be returned as a raw string.\n",
      "     |      :return: the given file(s) as a single string.\n",
      "     |      :rtype: str\n",
      "     |  \n",
      "     |  readme(self)\n",
      "     |      Return the contents of the corpus Readme.txt file.\n",
      "     |  \n",
      "     |  sents(self, fileids=None, categories=None)\n",
      "     |      Return all sentences in the corpus or in the specified file(s).\n",
      "     |      \n",
      "     |      :param fileids: a list or regexp specifying the ids of the files whose\n",
      "     |          sentences have to be returned.\n",
      "     |      :param categories: a list specifying the categories whose sentences have\n",
      "     |          to be returned.\n",
      "     |      :return: the given file(s) as a list of sentences.\n",
      "     |          Each sentence is tokenized using the specified word_tokenizer.\n",
      "     |      :rtype: list(list(str))\n",
      "     |  \n",
      "     |  words(self, fileids=None, categories=None)\n",
      "     |      Return all words and punctuation symbols in the corpus or in the specified\n",
      "     |      file(s).\n",
      "     |      \n",
      "     |      :param fileids: a list or regexp specifying the ids of the files whose\n",
      "     |          words have to be returned.\n",
      "     |      :param categories: a list specifying the categories whose words have to\n",
      "     |          be returned.\n",
      "     |      :return: the given file(s) as a list of words and punctuation symbols.\n",
      "     |      :rtype: list(str)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  CorpusView = <class 'nltk.corpus.reader.util.StreamBackedCorpusView'>\n",
      "     |      A 'view' of a corpus file, which acts like a sequence of tokens:\n",
      "     |      it can be accessed by index, iterated over, etc.  However, the\n",
      "     |      tokens are only constructed as-needed -- the entire corpus is\n",
      "     |      never stored in memory at once.\n",
      "     |      \n",
      "     |      The constructor to ``StreamBackedCorpusView`` takes two arguments:\n",
      "     |      a corpus fileid (specified as a string or as a ``PathPointer``);\n",
      "     |      and a block reader.  A \"block reader\" is a function that reads\n",
      "     |      zero or more tokens from a stream, and returns them as a list.  A\n",
      "     |      very simple example of a block reader is:\n",
      "     |      \n",
      "     |          >>> def simple_block_reader(stream):\n",
      "     |          ...     return stream.readline().split()\n",
      "     |      \n",
      "     |      This simple block reader reads a single line at a time, and\n",
      "     |      returns a single token (consisting of a string) for each\n",
      "     |      whitespace-separated substring on the line.\n",
      "     |      \n",
      "     |      When deciding how to define the block reader for a given\n",
      "     |      corpus, careful consideration should be given to the size of\n",
      "     |      blocks handled by the block reader.  Smaller block sizes will\n",
      "     |      increase the memory requirements of the corpus view's internal\n",
      "     |      data structures (by 2 integers per block).  On the other hand,\n",
      "     |      larger block sizes may decrease performance for random access to\n",
      "     |      the corpus.  (But note that larger block sizes will *not*\n",
      "     |      decrease performance for iteration.)\n",
      "     |      \n",
      "     |      Internally, ``CorpusView`` maintains a partial mapping from token\n",
      "     |      index to file position, with one entry per block.  When a token\n",
      "     |      with a given index *i* is requested, the ``CorpusView`` constructs\n",
      "     |      it as follows:\n",
      "     |      \n",
      "     |        1. First, it searches the toknum/filepos mapping for the token\n",
      "     |           index closest to (but less than or equal to) *i*.\n",
      "     |      \n",
      "     |        2. Then, starting at the file position corresponding to that\n",
      "     |           index, it reads one block at a time using the block reader\n",
      "     |           until it reaches the requested token.\n",
      "     |      \n",
      "     |      The toknum/filepos mapping is created lazily: it is initially\n",
      "     |      empty, but every time a new block is read, the block's\n",
      "     |      initial token is added to the mapping.  (Thus, the toknum/filepos\n",
      "     |      map has one entry per block.)\n",
      "     |      \n",
      "     |      In order to increase efficiency for random access patterns that\n",
      "     |      have high degrees of locality, the corpus view may cache one or\n",
      "     |      more blocks.\n",
      "     |      \n",
      "     |      :note: Each ``CorpusView`` object internally maintains an open file\n",
      "     |          object for its underlying corpus file.  This file should be\n",
      "     |          automatically closed when the ``CorpusView`` is garbage collected,\n",
      "     |          but if you wish to close it manually, use the ``close()``\n",
      "     |          method.  If you access a ``CorpusView``'s items after it has been\n",
      "     |          closed, the file object will be automatically re-opened.\n",
      "     |      \n",
      "     |      :warning: If the contents of the file are modified during the\n",
      "     |          lifetime of the ``CorpusView``, then the ``CorpusView``'s behavior\n",
      "     |          is undefined.\n",
      "     |      \n",
      "     |      :warning: If a unicode encoding is specified when constructing a\n",
      "     |          ``CorpusView``, then the block reader may only call\n",
      "     |          ``stream.seek()`` with offsets that have been returned by\n",
      "     |          ``stream.tell()``; in particular, calling ``stream.seek()`` with\n",
      "     |          relative offsets, or with offsets based on string lengths, may\n",
      "     |          lead to incorrect behavior.\n",
      "     |      \n",
      "     |      :ivar _block_reader: The function used to read\n",
      "     |          a single block from the underlying file stream.\n",
      "     |      :ivar _toknum: A list containing the token index of each block\n",
      "     |          that has been processed.  In particular, ``_toknum[i]`` is the\n",
      "     |          token index of the first token in block ``i``.  Together\n",
      "     |          with ``_filepos``, this forms a partial mapping between token\n",
      "     |          indices and file positions.\n",
      "     |      :ivar _filepos: A list containing the file position of each block\n",
      "     |          that has been processed.  In particular, ``_toknum[i]`` is the\n",
      "     |          file position of the first character in block ``i``.  Together\n",
      "     |          with ``_toknum``, this forms a partial mapping between token\n",
      "     |          indices and file positions.\n",
      "     |      :ivar _stream: The stream used to access the underlying corpus file.\n",
      "     |      :ivar _len: The total number of tokens in the corpus, if known;\n",
      "     |          or None, if the number of tokens is not yet known.\n",
      "     |      :ivar _eofpos: The character position of the last character in the\n",
      "     |          file.  This is calculated when the corpus view is initialized,\n",
      "     |          and is used to decide when the end of file has been reached.\n",
      "     |      :ivar _cache: A cache of the most recently read block.  It\n",
      "     |         is encoded as a tuple (start_toknum, end_toknum, tokens), where\n",
      "     |         start_toknum is the token index of the first token in the block;\n",
      "     |         end_toknum is the token index of the first token not in the\n",
      "     |         block; and tokens is a list of the tokens in the block.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from nltk.corpus.reader.api.CategorizedCorpusReader:\n",
      "     |  \n",
      "     |  categories(self, fileids=None)\n",
      "     |      Return a list of the categories that are defined for this corpus,\n",
      "     |      or for the file(s) if it is given.\n",
      "     |  \n",
      "     |  fileids(self, categories=None)\n",
      "     |      Return a list of file identifiers for the files that make up\n",
      "     |      this corpus, or that make up the given category(s) if specified.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from nltk.corpus.reader.api.CategorizedCorpusReader:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __unicode__ = __str__(self, /)\n",
      "     |  \n",
      "     |  abspath(self, fileid)\n",
      "     |      Return the absolute path for the given file.\n",
      "     |      \n",
      "     |      :type fileid: str\n",
      "     |      :param fileid: The file identifier for the file whose path\n",
      "     |          should be returned.\n",
      "     |      :rtype: PathPointer\n",
      "     |  \n",
      "     |  abspaths(self, fileids=None, include_encoding=False, include_fileid=False)\n",
      "     |      Return a list of the absolute paths for all fileids in this corpus;\n",
      "     |      or for the given list of fileids, if specified.\n",
      "     |      \n",
      "     |      :type fileids: None or str or list\n",
      "     |      :param fileids: Specifies the set of fileids for which paths should\n",
      "     |          be returned.  Can be None, for all fileids; a list of\n",
      "     |          file identifiers, for a specified set of fileids; or a single\n",
      "     |          file identifier, for a single file.  Note that the return\n",
      "     |          value is always a list of paths, even if ``fileids`` is a\n",
      "     |          single file identifier.\n",
      "     |      \n",
      "     |      :param include_encoding: If true, then return a list of\n",
      "     |          ``(path_pointer, encoding)`` tuples.\n",
      "     |      \n",
      "     |      :rtype: list(PathPointer)\n",
      "     |  \n",
      "     |  citation(self)\n",
      "     |      Return the contents of the corpus citation.bib file, if it exists.\n",
      "     |  \n",
      "     |  encoding(self, file)\n",
      "     |      Return the unicode encoding for the given corpus file, if known.\n",
      "     |      If the encoding is unknown, or if the given file should be\n",
      "     |      processed using byte strings (str), then return None.\n",
      "     |  \n",
      "     |  ensure_loaded(self)\n",
      "     |      Load this corpus (if it has not already been loaded).  This is\n",
      "     |      used by LazyCorpusLoader as a simple method that can be used to\n",
      "     |      make sure a corpus is loaded -- e.g., in case a user wants to\n",
      "     |      do help(some_corpus).\n",
      "     |  \n",
      "     |  license(self)\n",
      "     |      Return the contents of the corpus LICENSE file, if it exists.\n",
      "     |  \n",
      "     |  open(self, file)\n",
      "     |      Return an open stream that can be used to read the given file.\n",
      "     |      If the file's encoding is not None, then the stream will\n",
      "     |      automatically decode the file's contents into unicode.\n",
      "     |      \n",
      "     |      :param file: The file identifier of the file to read.\n",
      "     |  \n",
      "     |  unicode_repr = __repr__(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  root\n",
      "     |      The directory where this corpus is stored.\n",
      "     |      \n",
      "     |      :type: PathPointer\n",
      "    \n",
      "    class CategorizedTaggedCorpusReader(nltk.corpus.reader.api.CategorizedCorpusReader, TaggedCorpusReader)\n",
      "     |  CategorizedTaggedCorpusReader(*args, **kwargs)\n",
      "     |  \n",
      "     |  A reader for part-of-speech tagged corpora whose documents are\n",
      "     |  divided into categories based on their file identifiers.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      CategorizedTaggedCorpusReader\n",
      "     |      nltk.corpus.reader.api.CategorizedCorpusReader\n",
      "     |      TaggedCorpusReader\n",
      "     |      nltk.corpus.reader.api.CorpusReader\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, *args, **kwargs)\n",
      "     |      Initialize the corpus reader.  Categorization arguments\n",
      "     |      (``cat_pattern``, ``cat_map``, and ``cat_file``) are passed to\n",
      "     |      the ``CategorizedCorpusReader`` constructor.  The remaining arguments\n",
      "     |      are passed to the ``TaggedCorpusReader``.\n",
      "     |  \n",
      "     |  paras(self, fileids=None, categories=None)\n",
      "     |      :return: the given file(s) as a list of\n",
      "     |          paragraphs, each encoded as a list of sentences, which are\n",
      "     |          in turn encoded as lists of word strings.\n",
      "     |      :rtype: list(list(list(str)))\n",
      "     |  \n",
      "     |  raw(self, fileids=None, categories=None)\n",
      "     |      :return: the given file(s) as a single string.\n",
      "     |      :rtype: str\n",
      "     |  \n",
      "     |  sents(self, fileids=None, categories=None)\n",
      "     |      :return: the given file(s) as a list of\n",
      "     |          sentences or utterances, each encoded as a list of word\n",
      "     |          strings.\n",
      "     |      :rtype: list(list(str))\n",
      "     |  \n",
      "     |  tagged_paras(self, fileids=None, categories=None, tagset=None)\n",
      "     |      :return: the given file(s) as a list of\n",
      "     |          paragraphs, each encoded as a list of sentences, which are\n",
      "     |          in turn encoded as lists of ``(word,tag)`` tuples.\n",
      "     |      :rtype: list(list(list(tuple(str,str))))\n",
      "     |  \n",
      "     |  tagged_sents(self, fileids=None, categories=None, tagset=None)\n",
      "     |      :return: the given file(s) as a list of\n",
      "     |          sentences, each encoded as a list of ``(word,tag)`` tuples.\n",
      "     |      \n",
      "     |      :rtype: list(list(tuple(str,str)))\n",
      "     |  \n",
      "     |  tagged_words(self, fileids=None, categories=None, tagset=None)\n",
      "     |      :return: the given file(s) as a list of tagged\n",
      "     |          words and punctuation symbols, encoded as tuples\n",
      "     |          ``(word,tag)``.\n",
      "     |      :rtype: list(tuple(str,str))\n",
      "     |  \n",
      "     |  words(self, fileids=None, categories=None)\n",
      "     |      :return: the given file(s) as a list of words\n",
      "     |          and punctuation symbols.\n",
      "     |      :rtype: list(str)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from nltk.corpus.reader.api.CategorizedCorpusReader:\n",
      "     |  \n",
      "     |  categories(self, fileids=None)\n",
      "     |      Return a list of the categories that are defined for this corpus,\n",
      "     |      or for the file(s) if it is given.\n",
      "     |  \n",
      "     |  fileids(self, categories=None)\n",
      "     |      Return a list of file identifiers for the files that make up\n",
      "     |      this corpus, or that make up the given category(s) if specified.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from nltk.corpus.reader.api.CategorizedCorpusReader:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __unicode__ = __str__(self, /)\n",
      "     |  \n",
      "     |  abspath(self, fileid)\n",
      "     |      Return the absolute path for the given file.\n",
      "     |      \n",
      "     |      :type fileid: str\n",
      "     |      :param fileid: The file identifier for the file whose path\n",
      "     |          should be returned.\n",
      "     |      :rtype: PathPointer\n",
      "     |  \n",
      "     |  abspaths(self, fileids=None, include_encoding=False, include_fileid=False)\n",
      "     |      Return a list of the absolute paths for all fileids in this corpus;\n",
      "     |      or for the given list of fileids, if specified.\n",
      "     |      \n",
      "     |      :type fileids: None or str or list\n",
      "     |      :param fileids: Specifies the set of fileids for which paths should\n",
      "     |          be returned.  Can be None, for all fileids; a list of\n",
      "     |          file identifiers, for a specified set of fileids; or a single\n",
      "     |          file identifier, for a single file.  Note that the return\n",
      "     |          value is always a list of paths, even if ``fileids`` is a\n",
      "     |          single file identifier.\n",
      "     |      \n",
      "     |      :param include_encoding: If true, then return a list of\n",
      "     |          ``(path_pointer, encoding)`` tuples.\n",
      "     |      \n",
      "     |      :rtype: list(PathPointer)\n",
      "     |  \n",
      "     |  citation(self)\n",
      "     |      Return the contents of the corpus citation.bib file, if it exists.\n",
      "     |  \n",
      "     |  encoding(self, file)\n",
      "     |      Return the unicode encoding for the given corpus file, if known.\n",
      "     |      If the encoding is unknown, or if the given file should be\n",
      "     |      processed using byte strings (str), then return None.\n",
      "     |  \n",
      "     |  ensure_loaded(self)\n",
      "     |      Load this corpus (if it has not already been loaded).  This is\n",
      "     |      used by LazyCorpusLoader as a simple method that can be used to\n",
      "     |      make sure a corpus is loaded -- e.g., in case a user wants to\n",
      "     |      do help(some_corpus).\n",
      "     |  \n",
      "     |  license(self)\n",
      "     |      Return the contents of the corpus LICENSE file, if it exists.\n",
      "     |  \n",
      "     |  open(self, file)\n",
      "     |      Return an open stream that can be used to read the given file.\n",
      "     |      If the file's encoding is not None, then the stream will\n",
      "     |      automatically decode the file's contents into unicode.\n",
      "     |      \n",
      "     |      :param file: The file identifier of the file to read.\n",
      "     |  \n",
      "     |  readme(self)\n",
      "     |      Return the contents of the corpus README file, if it exists.\n",
      "     |  \n",
      "     |  unicode_repr = __repr__(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  root\n",
      "     |      The directory where this corpus is stored.\n",
      "     |      \n",
      "     |      :type: PathPointer\n",
      "    \n",
      "    class ChasenCorpusReader(nltk.corpus.reader.api.CorpusReader)\n",
      "     |  ChasenCorpusReader(root, fileids, encoding='utf8', sent_splitter=None)\n",
      "     |  \n",
      "     |  A base class for \"corpus reader\" classes, each of which can be\n",
      "     |  used to read a specific corpus format.  Each individual corpus\n",
      "     |  reader instance is used to read a specific corpus, consisting of\n",
      "     |  one or more files under a common root directory.  Each file is\n",
      "     |  identified by its ``file identifier``, which is the relative path\n",
      "     |  to the file from the root directory.\n",
      "     |  \n",
      "     |  A separate subclass is defined for each corpus format.  These\n",
      "     |  subclasses define one or more methods that provide 'views' on the\n",
      "     |  corpus contents, such as ``words()`` (for a list of words) and\n",
      "     |  ``parsed_sents()`` (for a list of parsed sentences).  Called with\n",
      "     |  no arguments, these methods will return the contents of the entire\n",
      "     |  corpus.  For most corpora, these methods define one or more\n",
      "     |  selection arguments, such as ``fileids`` or ``categories``, which can\n",
      "     |  be used to select which portion of the corpus should be returned.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      ChasenCorpusReader\n",
      "     |      nltk.corpus.reader.api.CorpusReader\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, root, fileids, encoding='utf8', sent_splitter=None)\n",
      "     |      :type root: PathPointer or str\n",
      "     |      :param root: A path pointer identifying the root directory for\n",
      "     |          this corpus.  If a string is specified, then it will be\n",
      "     |          converted to a ``PathPointer`` automatically.\n",
      "     |      :param fileids: A list of the files that make up this corpus.\n",
      "     |          This list can either be specified explicitly, as a list of\n",
      "     |          strings; or implicitly, as a regular expression over file\n",
      "     |          paths.  The absolute path for each file will be constructed\n",
      "     |          by joining the reader's root to each file name.\n",
      "     |      :param encoding: The default unicode encoding for the files\n",
      "     |          that make up the corpus.  The value of ``encoding`` can be any\n",
      "     |          of the following:\n",
      "     |          - A string: ``encoding`` is the encoding name for all files.\n",
      "     |          - A dictionary: ``encoding[file_id]`` is the encoding\n",
      "     |            name for the file whose identifier is ``file_id``.  If\n",
      "     |            ``file_id`` is not in ``encoding``, then the file\n",
      "     |            contents will be processed using non-unicode byte strings.\n",
      "     |          - A list: ``encoding`` should be a list of ``(regexp, encoding)``\n",
      "     |            tuples.  The encoding for a file whose identifier is ``file_id``\n",
      "     |            will be the ``encoding`` value for the first tuple whose\n",
      "     |            ``regexp`` matches the ``file_id``.  If no tuple's ``regexp``\n",
      "     |            matches the ``file_id``, the file contents will be processed\n",
      "     |            using non-unicode byte strings.\n",
      "     |          - None: the file contents of all files will be\n",
      "     |            processed using non-unicode byte strings.\n",
      "     |      :param tagset: The name of the tagset used by this corpus, to be used\n",
      "     |            for normalizing or converting the POS tags returned by the\n",
      "     |            tagged_...() methods.\n",
      "     |  \n",
      "     |  paras(self, fileids=None)\n",
      "     |  \n",
      "     |  raw(self, fileids=None)\n",
      "     |  \n",
      "     |  sents(self, fileids=None)\n",
      "     |  \n",
      "     |  tagged_paras(self, fileids=None)\n",
      "     |  \n",
      "     |  tagged_sents(self, fileids=None)\n",
      "     |  \n",
      "     |  tagged_words(self, fileids=None)\n",
      "     |  \n",
      "     |  words(self, fileids=None)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __unicode__ = __str__(self, /)\n",
      "     |  \n",
      "     |  abspath(self, fileid)\n",
      "     |      Return the absolute path for the given file.\n",
      "     |      \n",
      "     |      :type fileid: str\n",
      "     |      :param fileid: The file identifier for the file whose path\n",
      "     |          should be returned.\n",
      "     |      :rtype: PathPointer\n",
      "     |  \n",
      "     |  abspaths(self, fileids=None, include_encoding=False, include_fileid=False)\n",
      "     |      Return a list of the absolute paths for all fileids in this corpus;\n",
      "     |      or for the given list of fileids, if specified.\n",
      "     |      \n",
      "     |      :type fileids: None or str or list\n",
      "     |      :param fileids: Specifies the set of fileids for which paths should\n",
      "     |          be returned.  Can be None, for all fileids; a list of\n",
      "     |          file identifiers, for a specified set of fileids; or a single\n",
      "     |          file identifier, for a single file.  Note that the return\n",
      "     |          value is always a list of paths, even if ``fileids`` is a\n",
      "     |          single file identifier.\n",
      "     |      \n",
      "     |      :param include_encoding: If true, then return a list of\n",
      "     |          ``(path_pointer, encoding)`` tuples.\n",
      "     |      \n",
      "     |      :rtype: list(PathPointer)\n",
      "     |  \n",
      "     |  citation(self)\n",
      "     |      Return the contents of the corpus citation.bib file, if it exists.\n",
      "     |  \n",
      "     |  encoding(self, file)\n",
      "     |      Return the unicode encoding for the given corpus file, if known.\n",
      "     |      If the encoding is unknown, or if the given file should be\n",
      "     |      processed using byte strings (str), then return None.\n",
      "     |  \n",
      "     |  ensure_loaded(self)\n",
      "     |      Load this corpus (if it has not already been loaded).  This is\n",
      "     |      used by LazyCorpusLoader as a simple method that can be used to\n",
      "     |      make sure a corpus is loaded -- e.g., in case a user wants to\n",
      "     |      do help(some_corpus).\n",
      "     |  \n",
      "     |  fileids(self)\n",
      "     |      Return a list of file identifiers for the fileids that make up\n",
      "     |      this corpus.\n",
      "     |  \n",
      "     |  license(self)\n",
      "     |      Return the contents of the corpus LICENSE file, if it exists.\n",
      "     |  \n",
      "     |  open(self, file)\n",
      "     |      Return an open stream that can be used to read the given file.\n",
      "     |      If the file's encoding is not None, then the stream will\n",
      "     |      automatically decode the file's contents into unicode.\n",
      "     |      \n",
      "     |      :param file: The file identifier of the file to read.\n",
      "     |  \n",
      "     |  readme(self)\n",
      "     |      Return the contents of the corpus README file, if it exists.\n",
      "     |  \n",
      "     |  unicode_repr = __repr__(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  root\n",
      "     |      The directory where this corpus is stored.\n",
      "     |      \n",
      "     |      :type: PathPointer\n",
      "    \n",
      "    class ChunkedCorpusReader(nltk.corpus.reader.api.CorpusReader)\n",
      "     |  ChunkedCorpusReader(root, fileids, extension='', str2chunktree=<function tagstr2tree at 0x1257d0560>, sent_tokenizer=RegexpTokenizer(pattern='\\n', gaps=True, discard_empty=True, flags=<RegexFlag.UNICODE|DOTALL|MULTILINE: 56>), para_block_reader=<function read_blankline_block at 0x125911320>, encoding='utf8', tagset=None)\n",
      "     |  \n",
      "     |  Reader for chunked (and optionally tagged) corpora.  Paragraphs\n",
      "     |  are split using a block reader.  They are then tokenized into\n",
      "     |  sentences using a sentence tokenizer.  Finally, these sentences\n",
      "     |  are parsed into chunk trees using a string-to-chunktree conversion\n",
      "     |  function.  Each of these steps can be performed using a default\n",
      "     |  function or a custom function.  By default, paragraphs are split\n",
      "     |  on blank lines; sentences are listed one per line; and sentences\n",
      "     |  are parsed into chunk trees using ``nltk.chunk.tagstr2tree``.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      ChunkedCorpusReader\n",
      "     |      nltk.corpus.reader.api.CorpusReader\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, root, fileids, extension='', str2chunktree=<function tagstr2tree at 0x1257d0560>, sent_tokenizer=RegexpTokenizer(pattern='\\n', gaps=True, discard_empty=True, flags=<RegexFlag.UNICODE|DOTALL|MULTILINE: 56>), para_block_reader=<function read_blankline_block at 0x125911320>, encoding='utf8', tagset=None)\n",
      "     |      :param root: The root directory for this corpus.\n",
      "     |      :param fileids: A list or regexp specifying the fileids in this corpus.\n",
      "     |  \n",
      "     |  chunked_paras(self, fileids=None, tagset=None)\n",
      "     |      :return: the given file(s) as a list of\n",
      "     |          paragraphs, each encoded as a list of sentences, which are\n",
      "     |          in turn encoded as a shallow Tree.  The leaves of these\n",
      "     |          trees are encoded as ``(word, tag)`` tuples (if the corpus\n",
      "     |          has tags) or word strings (if the corpus has no tags).\n",
      "     |      :rtype: list(list(Tree))\n",
      "     |  \n",
      "     |  chunked_sents(self, fileids=None, tagset=None)\n",
      "     |      :return: the given file(s) as a list of\n",
      "     |          sentences, each encoded as a shallow Tree.  The leaves\n",
      "     |          of these trees are encoded as ``(word, tag)`` tuples (if\n",
      "     |          the corpus has tags) or word strings (if the corpus has no\n",
      "     |          tags).\n",
      "     |      :rtype: list(Tree)\n",
      "     |  \n",
      "     |  chunked_words(self, fileids=None, tagset=None)\n",
      "     |      :return: the given file(s) as a list of tagged\n",
      "     |          words and chunks.  Words are encoded as ``(word, tag)``\n",
      "     |          tuples (if the corpus has tags) or word strings (if the\n",
      "     |          corpus has no tags).  Chunks are encoded as depth-one\n",
      "     |          trees over ``(word,tag)`` tuples or word strings.\n",
      "     |      :rtype: list(tuple(str,str) and Tree)\n",
      "     |  \n",
      "     |  paras(self, fileids=None)\n",
      "     |      :return: the given file(s) as a list of\n",
      "     |          paragraphs, each encoded as a list of sentences, which are\n",
      "     |          in turn encoded as lists of word strings.\n",
      "     |      :rtype: list(list(list(str)))\n",
      "     |  \n",
      "     |  raw(self, fileids=None)\n",
      "     |      :return: the given file(s) as a single string.\n",
      "     |      :rtype: str\n",
      "     |  \n",
      "     |  sents(self, fileids=None)\n",
      "     |      :return: the given file(s) as a list of\n",
      "     |          sentences or utterances, each encoded as a list of word\n",
      "     |          strings.\n",
      "     |      :rtype: list(list(str))\n",
      "     |  \n",
      "     |  tagged_paras(self, fileids=None, tagset=None)\n",
      "     |      :return: the given file(s) as a list of\n",
      "     |          paragraphs, each encoded as a list of sentences, which are\n",
      "     |          in turn encoded as lists of ``(word,tag)`` tuples.\n",
      "     |      :rtype: list(list(list(tuple(str,str))))\n",
      "     |  \n",
      "     |  tagged_sents(self, fileids=None, tagset=None)\n",
      "     |      :return: the given file(s) as a list of\n",
      "     |          sentences, each encoded as a list of ``(word,tag)`` tuples.\n",
      "     |      \n",
      "     |      :rtype: list(list(tuple(str,str)))\n",
      "     |  \n",
      "     |  tagged_words(self, fileids=None, tagset=None)\n",
      "     |      :return: the given file(s) as a list of tagged\n",
      "     |          words and punctuation symbols, encoded as tuples\n",
      "     |          ``(word,tag)``.\n",
      "     |      :rtype: list(tuple(str,str))\n",
      "     |  \n",
      "     |  words(self, fileids=None)\n",
      "     |      :return: the given file(s) as a list of words\n",
      "     |          and punctuation symbols.\n",
      "     |      :rtype: list(str)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __unicode__ = __str__(self, /)\n",
      "     |  \n",
      "     |  abspath(self, fileid)\n",
      "     |      Return the absolute path for the given file.\n",
      "     |      \n",
      "     |      :type fileid: str\n",
      "     |      :param fileid: The file identifier for the file whose path\n",
      "     |          should be returned.\n",
      "     |      :rtype: PathPointer\n",
      "     |  \n",
      "     |  abspaths(self, fileids=None, include_encoding=False, include_fileid=False)\n",
      "     |      Return a list of the absolute paths for all fileids in this corpus;\n",
      "     |      or for the given list of fileids, if specified.\n",
      "     |      \n",
      "     |      :type fileids: None or str or list\n",
      "     |      :param fileids: Specifies the set of fileids for which paths should\n",
      "     |          be returned.  Can be None, for all fileids; a list of\n",
      "     |          file identifiers, for a specified set of fileids; or a single\n",
      "     |          file identifier, for a single file.  Note that the return\n",
      "     |          value is always a list of paths, even if ``fileids`` is a\n",
      "     |          single file identifier.\n",
      "     |      \n",
      "     |      :param include_encoding: If true, then return a list of\n",
      "     |          ``(path_pointer, encoding)`` tuples.\n",
      "     |      \n",
      "     |      :rtype: list(PathPointer)\n",
      "     |  \n",
      "     |  citation(self)\n",
      "     |      Return the contents of the corpus citation.bib file, if it exists.\n",
      "     |  \n",
      "     |  encoding(self, file)\n",
      "     |      Return the unicode encoding for the given corpus file, if known.\n",
      "     |      If the encoding is unknown, or if the given file should be\n",
      "     |      processed using byte strings (str), then return None.\n",
      "     |  \n",
      "     |  ensure_loaded(self)\n",
      "     |      Load this corpus (if it has not already been loaded).  This is\n",
      "     |      used by LazyCorpusLoader as a simple method that can be used to\n",
      "     |      make sure a corpus is loaded -- e.g., in case a user wants to\n",
      "     |      do help(some_corpus).\n",
      "     |  \n",
      "     |  fileids(self)\n",
      "     |      Return a list of file identifiers for the fileids that make up\n",
      "     |      this corpus.\n",
      "     |  \n",
      "     |  license(self)\n",
      "     |      Return the contents of the corpus LICENSE file, if it exists.\n",
      "     |  \n",
      "     |  open(self, file)\n",
      "     |      Return an open stream that can be used to read the given file.\n",
      "     |      If the file's encoding is not None, then the stream will\n",
      "     |      automatically decode the file's contents into unicode.\n",
      "     |      \n",
      "     |      :param file: The file identifier of the file to read.\n",
      "     |  \n",
      "     |  readme(self)\n",
      "     |      Return the contents of the corpus README file, if it exists.\n",
      "     |  \n",
      "     |  unicode_repr = __repr__(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  root\n",
      "     |      The directory where this corpus is stored.\n",
      "     |      \n",
      "     |      :type: PathPointer\n",
      "    \n",
      "    class ComparativeSentencesCorpusReader(nltk.corpus.reader.api.CorpusReader)\n",
      "     |  ComparativeSentencesCorpusReader(root, fileids, word_tokenizer=WhitespaceTokenizer(pattern='\\\\s+', gaps=True, discard_empty=True, flags=<RegexFlag.UNICODE|DOTALL|MULTILINE: 56>), sent_tokenizer=None, encoding='utf8')\n",
      "     |  \n",
      "     |  Reader for the Comparative Sentence Dataset by Jindal and Liu (2006).\n",
      "     |  \n",
      "     |      >>> from nltk.corpus import comparative_sentences\n",
      "     |      >>> comparison = comparative_sentences.comparisons()[0]\n",
      "     |      >>> comparison.text\n",
      "     |      ['its', 'fast-forward', 'and', 'rewind', 'work', 'much', 'more', 'smoothly',\n",
      "     |      'and', 'consistently', 'than', 'those', 'of', 'other', 'models', 'i', \"'ve\",\n",
      "     |      'had', '.']\n",
      "     |      >>> comparison.entity_2\n",
      "     |      'models'\n",
      "     |      >>> (comparison.feature, comparison.keyword)\n",
      "     |      ('rewind', 'more')\n",
      "     |      >>> len(comparative_sentences.comparisons())\n",
      "     |      853\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      ComparativeSentencesCorpusReader\n",
      "     |      nltk.corpus.reader.api.CorpusReader\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, root, fileids, word_tokenizer=WhitespaceTokenizer(pattern='\\\\s+', gaps=True, discard_empty=True, flags=<RegexFlag.UNICODE|DOTALL|MULTILINE: 56>), sent_tokenizer=None, encoding='utf8')\n",
      "     |      :param root: The root directory for this corpus.\n",
      "     |      :param fileids: a list or regexp specifying the fileids in this corpus.\n",
      "     |      :param word_tokenizer: tokenizer for breaking sentences or paragraphs\n",
      "     |          into words. Default: `WhitespaceTokenizer`\n",
      "     |      :param sent_tokenizer: tokenizer for breaking paragraphs into sentences.\n",
      "     |      :param encoding: the encoding that should be used to read the corpus.\n",
      "     |  \n",
      "     |  comparisons(self, fileids=None)\n",
      "     |      Return all comparisons in the corpus.\n",
      "     |      \n",
      "     |      :param fileids: a list or regexp specifying the ids of the files whose\n",
      "     |          comparisons have to be returned.\n",
      "     |      :return: the given file(s) as a list of Comparison objects.\n",
      "     |      :rtype: list(Comparison)\n",
      "     |  \n",
      "     |  keywords(self, fileids=None)\n",
      "     |      Return a set of all keywords used in the corpus.\n",
      "     |      \n",
      "     |      :param fileids: a list or regexp specifying the ids of the files whose\n",
      "     |          keywords have to be returned.\n",
      "     |      :return: the set of keywords and comparative phrases used in the corpus.\n",
      "     |      :rtype: set(str)\n",
      "     |  \n",
      "     |  keywords_readme(self)\n",
      "     |      Return the list of words and constituents considered as clues of a\n",
      "     |      comparison (from listOfkeywords.txt).\n",
      "     |  \n",
      "     |  raw(self, fileids=None)\n",
      "     |      :param fileids: a list or regexp specifying the fileids that have to be\n",
      "     |          returned as a raw string.\n",
      "     |      :return: the given file(s) as a single string.\n",
      "     |      :rtype: str\n",
      "     |  \n",
      "     |  readme(self)\n",
      "     |      Return the contents of the corpus readme file.\n",
      "     |  \n",
      "     |  sents(self, fileids=None)\n",
      "     |      Return all sentences in the corpus.\n",
      "     |      \n",
      "     |      :param fileids: a list or regexp specifying the ids of the files whose\n",
      "     |          sentences have to be returned.\n",
      "     |      :return: all sentences of the corpus as lists of tokens (or as plain\n",
      "     |          strings, if no word tokenizer is specified).\n",
      "     |      :rtype: list(list(str)) or list(str)\n",
      "     |  \n",
      "     |  words(self, fileids=None)\n",
      "     |      Return all words and punctuation symbols in the corpus.\n",
      "     |      \n",
      "     |      :param fileids: a list or regexp specifying the ids of the files whose\n",
      "     |          words have to be returned.\n",
      "     |      :return: the given file(s) as a list of words and punctuation symbols.\n",
      "     |      :rtype: list(str)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  CorpusView = <class 'nltk.corpus.reader.util.StreamBackedCorpusView'>\n",
      "     |      A 'view' of a corpus file, which acts like a sequence of tokens:\n",
      "     |      it can be accessed by index, iterated over, etc.  However, the\n",
      "     |      tokens are only constructed as-needed -- the entire corpus is\n",
      "     |      never stored in memory at once.\n",
      "     |      \n",
      "     |      The constructor to ``StreamBackedCorpusView`` takes two arguments:\n",
      "     |      a corpus fileid (specified as a string or as a ``PathPointer``);\n",
      "     |      and a block reader.  A \"block reader\" is a function that reads\n",
      "     |      zero or more tokens from a stream, and returns them as a list.  A\n",
      "     |      very simple example of a block reader is:\n",
      "     |      \n",
      "     |          >>> def simple_block_reader(stream):\n",
      "     |          ...     return stream.readline().split()\n",
      "     |      \n",
      "     |      This simple block reader reads a single line at a time, and\n",
      "     |      returns a single token (consisting of a string) for each\n",
      "     |      whitespace-separated substring on the line.\n",
      "     |      \n",
      "     |      When deciding how to define the block reader for a given\n",
      "     |      corpus, careful consideration should be given to the size of\n",
      "     |      blocks handled by the block reader.  Smaller block sizes will\n",
      "     |      increase the memory requirements of the corpus view's internal\n",
      "     |      data structures (by 2 integers per block).  On the other hand,\n",
      "     |      larger block sizes may decrease performance for random access to\n",
      "     |      the corpus.  (But note that larger block sizes will *not*\n",
      "     |      decrease performance for iteration.)\n",
      "     |      \n",
      "     |      Internally, ``CorpusView`` maintains a partial mapping from token\n",
      "     |      index to file position, with one entry per block.  When a token\n",
      "     |      with a given index *i* is requested, the ``CorpusView`` constructs\n",
      "     |      it as follows:\n",
      "     |      \n",
      "     |        1. First, it searches the toknum/filepos mapping for the token\n",
      "     |           index closest to (but less than or equal to) *i*.\n",
      "     |      \n",
      "     |        2. Then, starting at the file position corresponding to that\n",
      "     |           index, it reads one block at a time using the block reader\n",
      "     |           until it reaches the requested token.\n",
      "     |      \n",
      "     |      The toknum/filepos mapping is created lazily: it is initially\n",
      "     |      empty, but every time a new block is read, the block's\n",
      "     |      initial token is added to the mapping.  (Thus, the toknum/filepos\n",
      "     |      map has one entry per block.)\n",
      "     |      \n",
      "     |      In order to increase efficiency for random access patterns that\n",
      "     |      have high degrees of locality, the corpus view may cache one or\n",
      "     |      more blocks.\n",
      "     |      \n",
      "     |      :note: Each ``CorpusView`` object internally maintains an open file\n",
      "     |          object for its underlying corpus file.  This file should be\n",
      "     |          automatically closed when the ``CorpusView`` is garbage collected,\n",
      "     |          but if you wish to close it manually, use the ``close()``\n",
      "     |          method.  If you access a ``CorpusView``'s items after it has been\n",
      "     |          closed, the file object will be automatically re-opened.\n",
      "     |      \n",
      "     |      :warning: If the contents of the file are modified during the\n",
      "     |          lifetime of the ``CorpusView``, then the ``CorpusView``'s behavior\n",
      "     |          is undefined.\n",
      "     |      \n",
      "     |      :warning: If a unicode encoding is specified when constructing a\n",
      "     |          ``CorpusView``, then the block reader may only call\n",
      "     |          ``stream.seek()`` with offsets that have been returned by\n",
      "     |          ``stream.tell()``; in particular, calling ``stream.seek()`` with\n",
      "     |          relative offsets, or with offsets based on string lengths, may\n",
      "     |          lead to incorrect behavior.\n",
      "     |      \n",
      "     |      :ivar _block_reader: The function used to read\n",
      "     |          a single block from the underlying file stream.\n",
      "     |      :ivar _toknum: A list containing the token index of each block\n",
      "     |          that has been processed.  In particular, ``_toknum[i]`` is the\n",
      "     |          token index of the first token in block ``i``.  Together\n",
      "     |          with ``_filepos``, this forms a partial mapping between token\n",
      "     |          indices and file positions.\n",
      "     |      :ivar _filepos: A list containing the file position of each block\n",
      "     |          that has been processed.  In particular, ``_toknum[i]`` is the\n",
      "     |          file position of the first character in block ``i``.  Together\n",
      "     |          with ``_toknum``, this forms a partial mapping between token\n",
      "     |          indices and file positions.\n",
      "     |      :ivar _stream: The stream used to access the underlying corpus file.\n",
      "     |      :ivar _len: The total number of tokens in the corpus, if known;\n",
      "     |          or None, if the number of tokens is not yet known.\n",
      "     |      :ivar _eofpos: The character position of the last character in the\n",
      "     |          file.  This is calculated when the corpus view is initialized,\n",
      "     |          and is used to decide when the end of file has been reached.\n",
      "     |      :ivar _cache: A cache of the most recently read block.  It\n",
      "     |         is encoded as a tuple (start_toknum, end_toknum, tokens), where\n",
      "     |         start_toknum is the token index of the first token in the block;\n",
      "     |         end_toknum is the token index of the first token not in the\n",
      "     |         block; and tokens is a list of the tokens in the block.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __unicode__ = __str__(self, /)\n",
      "     |  \n",
      "     |  abspath(self, fileid)\n",
      "     |      Return the absolute path for the given file.\n",
      "     |      \n",
      "     |      :type fileid: str\n",
      "     |      :param fileid: The file identifier for the file whose path\n",
      "     |          should be returned.\n",
      "     |      :rtype: PathPointer\n",
      "     |  \n",
      "     |  abspaths(self, fileids=None, include_encoding=False, include_fileid=False)\n",
      "     |      Return a list of the absolute paths for all fileids in this corpus;\n",
      "     |      or for the given list of fileids, if specified.\n",
      "     |      \n",
      "     |      :type fileids: None or str or list\n",
      "     |      :param fileids: Specifies the set of fileids for which paths should\n",
      "     |          be returned.  Can be None, for all fileids; a list of\n",
      "     |          file identifiers, for a specified set of fileids; or a single\n",
      "     |          file identifier, for a single file.  Note that the return\n",
      "     |          value is always a list of paths, even if ``fileids`` is a\n",
      "     |          single file identifier.\n",
      "     |      \n",
      "     |      :param include_encoding: If true, then return a list of\n",
      "     |          ``(path_pointer, encoding)`` tuples.\n",
      "     |      \n",
      "     |      :rtype: list(PathPointer)\n",
      "     |  \n",
      "     |  citation(self)\n",
      "     |      Return the contents of the corpus citation.bib file, if it exists.\n",
      "     |  \n",
      "     |  encoding(self, file)\n",
      "     |      Return the unicode encoding for the given corpus file, if known.\n",
      "     |      If the encoding is unknown, or if the given file should be\n",
      "     |      processed using byte strings (str), then return None.\n",
      "     |  \n",
      "     |  ensure_loaded(self)\n",
      "     |      Load this corpus (if it has not already been loaded).  This is\n",
      "     |      used by LazyCorpusLoader as a simple method that can be used to\n",
      "     |      make sure a corpus is loaded -- e.g., in case a user wants to\n",
      "     |      do help(some_corpus).\n",
      "     |  \n",
      "     |  fileids(self)\n",
      "     |      Return a list of file identifiers for the fileids that make up\n",
      "     |      this corpus.\n",
      "     |  \n",
      "     |  license(self)\n",
      "     |      Return the contents of the corpus LICENSE file, if it exists.\n",
      "     |  \n",
      "     |  open(self, file)\n",
      "     |      Return an open stream that can be used to read the given file.\n",
      "     |      If the file's encoding is not None, then the stream will\n",
      "     |      automatically decode the file's contents into unicode.\n",
      "     |      \n",
      "     |      :param file: The file identifier of the file to read.\n",
      "     |  \n",
      "     |  unicode_repr = __repr__(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  root\n",
      "     |      The directory where this corpus is stored.\n",
      "     |      \n",
      "     |      :type: PathPointer\n",
      "    \n",
      "    class ConllChunkCorpusReader(ConllCorpusReader)\n",
      "     |  ConllChunkCorpusReader(root, fileids, chunk_types, encoding='utf8', tagset=None, separator=None)\n",
      "     |  \n",
      "     |  A ConllCorpusReader whose data file contains three columns: words,\n",
      "     |  pos, and chunk.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      ConllChunkCorpusReader\n",
      "     |      ConllCorpusReader\n",
      "     |      nltk.corpus.reader.api.CorpusReader\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, root, fileids, chunk_types, encoding='utf8', tagset=None, separator=None)\n",
      "     |      :type root: PathPointer or str\n",
      "     |      :param root: A path pointer identifying the root directory for\n",
      "     |          this corpus.  If a string is specified, then it will be\n",
      "     |          converted to a ``PathPointer`` automatically.\n",
      "     |      :param fileids: A list of the files that make up this corpus.\n",
      "     |          This list can either be specified explicitly, as a list of\n",
      "     |          strings; or implicitly, as a regular expression over file\n",
      "     |          paths.  The absolute path for each file will be constructed\n",
      "     |          by joining the reader's root to each file name.\n",
      "     |      :param encoding: The default unicode encoding for the files\n",
      "     |          that make up the corpus.  The value of ``encoding`` can be any\n",
      "     |          of the following:\n",
      "     |          - A string: ``encoding`` is the encoding name for all files.\n",
      "     |          - A dictionary: ``encoding[file_id]`` is the encoding\n",
      "     |            name for the file whose identifier is ``file_id``.  If\n",
      "     |            ``file_id`` is not in ``encoding``, then the file\n",
      "     |            contents will be processed using non-unicode byte strings.\n",
      "     |          - A list: ``encoding`` should be a list of ``(regexp, encoding)``\n",
      "     |            tuples.  The encoding for a file whose identifier is ``file_id``\n",
      "     |            will be the ``encoding`` value for the first tuple whose\n",
      "     |            ``regexp`` matches the ``file_id``.  If no tuple's ``regexp``\n",
      "     |            matches the ``file_id``, the file contents will be processed\n",
      "     |            using non-unicode byte strings.\n",
      "     |          - None: the file contents of all files will be\n",
      "     |            processed using non-unicode byte strings.\n",
      "     |      :param tagset: The name of the tagset used by this corpus, to be used\n",
      "     |            for normalizing or converting the POS tags returned by the\n",
      "     |            tagged_...() methods.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from ConllCorpusReader:\n",
      "     |  \n",
      "     |  chunked_sents(self, fileids=None, chunk_types=None, tagset=None)\n",
      "     |  \n",
      "     |  chunked_words(self, fileids=None, chunk_types=None, tagset=None)\n",
      "     |  \n",
      "     |  iob_sents(self, fileids=None, tagset=None)\n",
      "     |      :return: a list of lists of word/tag/IOB tuples\n",
      "     |      :rtype: list(list)\n",
      "     |      :param fileids: the list of fileids that make up this corpus\n",
      "     |      :type fileids: None or str or list\n",
      "     |  \n",
      "     |  iob_words(self, fileids=None, tagset=None)\n",
      "     |      :return: a list of word/tag/IOB tuples\n",
      "     |      :rtype: list(tuple)\n",
      "     |      :param fileids: the list of fileids that make up this corpus\n",
      "     |      :type fileids: None or str or list\n",
      "     |  \n",
      "     |  parsed_sents(self, fileids=None, pos_in_tree=None, tagset=None)\n",
      "     |  \n",
      "     |  raw(self, fileids=None)\n",
      "     |  \n",
      "     |  sents(self, fileids=None)\n",
      "     |  \n",
      "     |  srl_instances(self, fileids=None, pos_in_tree=None, flatten=True)\n",
      "     |  \n",
      "     |  srl_spans(self, fileids=None)\n",
      "     |  \n",
      "     |  tagged_sents(self, fileids=None, tagset=None)\n",
      "     |  \n",
      "     |  tagged_words(self, fileids=None, tagset=None)\n",
      "     |  \n",
      "     |  words(self, fileids=None)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from ConllCorpusReader:\n",
      "     |  \n",
      "     |  CHUNK = 'chunk'\n",
      "     |  \n",
      "     |  COLUMN_TYPES = ('words', 'pos', 'tree', 'chunk', 'ne', 'srl', 'ignore'...\n",
      "     |  \n",
      "     |  IGNORE = 'ignore'\n",
      "     |  \n",
      "     |  NE = 'ne'\n",
      "     |  \n",
      "     |  POS = 'pos'\n",
      "     |  \n",
      "     |  SRL = 'srl'\n",
      "     |  \n",
      "     |  TREE = 'tree'\n",
      "     |  \n",
      "     |  WORDS = 'words'\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __unicode__ = __str__(self, /)\n",
      "     |  \n",
      "     |  abspath(self, fileid)\n",
      "     |      Return the absolute path for the given file.\n",
      "     |      \n",
      "     |      :type fileid: str\n",
      "     |      :param fileid: The file identifier for the file whose path\n",
      "     |          should be returned.\n",
      "     |      :rtype: PathPointer\n",
      "     |  \n",
      "     |  abspaths(self, fileids=None, include_encoding=False, include_fileid=False)\n",
      "     |      Return a list of the absolute paths for all fileids in this corpus;\n",
      "     |      or for the given list of fileids, if specified.\n",
      "     |      \n",
      "     |      :type fileids: None or str or list\n",
      "     |      :param fileids: Specifies the set of fileids for which paths should\n",
      "     |          be returned.  Can be None, for all fileids; a list of\n",
      "     |          file identifiers, for a specified set of fileids; or a single\n",
      "     |          file identifier, for a single file.  Note that the return\n",
      "     |          value is always a list of paths, even if ``fileids`` is a\n",
      "     |          single file identifier.\n",
      "     |      \n",
      "     |      :param include_encoding: If true, then return a list of\n",
      "     |          ``(path_pointer, encoding)`` tuples.\n",
      "     |      \n",
      "     |      :rtype: list(PathPointer)\n",
      "     |  \n",
      "     |  citation(self)\n",
      "     |      Return the contents of the corpus citation.bib file, if it exists.\n",
      "     |  \n",
      "     |  encoding(self, file)\n",
      "     |      Return the unicode encoding for the given corpus file, if known.\n",
      "     |      If the encoding is unknown, or if the given file should be\n",
      "     |      processed using byte strings (str), then return None.\n",
      "     |  \n",
      "     |  ensure_loaded(self)\n",
      "     |      Load this corpus (if it has not already been loaded).  This is\n",
      "     |      used by LazyCorpusLoader as a simple method that can be used to\n",
      "     |      make sure a corpus is loaded -- e.g., in case a user wants to\n",
      "     |      do help(some_corpus).\n",
      "     |  \n",
      "     |  fileids(self)\n",
      "     |      Return a list of file identifiers for the fileids that make up\n",
      "     |      this corpus.\n",
      "     |  \n",
      "     |  license(self)\n",
      "     |      Return the contents of the corpus LICENSE file, if it exists.\n",
      "     |  \n",
      "     |  open(self, file)\n",
      "     |      Return an open stream that can be used to read the given file.\n",
      "     |      If the file's encoding is not None, then the stream will\n",
      "     |      automatically decode the file's contents into unicode.\n",
      "     |      \n",
      "     |      :param file: The file identifier of the file to read.\n",
      "     |  \n",
      "     |  readme(self)\n",
      "     |      Return the contents of the corpus README file, if it exists.\n",
      "     |  \n",
      "     |  unicode_repr = __repr__(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  root\n",
      "     |      The directory where this corpus is stored.\n",
      "     |      \n",
      "     |      :type: PathPointer\n",
      "    \n",
      "    class ConllCorpusReader(nltk.corpus.reader.api.CorpusReader)\n",
      "     |  ConllCorpusReader(root, fileids, columntypes, chunk_types=None, root_label='S', pos_in_tree=False, srl_includes_roleset=True, encoding='utf8', tree_class=<class 'nltk.tree.Tree'>, tagset=None, separator=None)\n",
      "     |  \n",
      "     |  A corpus reader for CoNLL-style files.  These files consist of a\n",
      "     |  series of sentences, separated by blank lines.  Each sentence is\n",
      "     |  encoded using a table (or \"grid\") of values, where each line\n",
      "     |  corresponds to a single word, and each column corresponds to an\n",
      "     |  annotation type.  The set of columns used by CoNLL-style files can\n",
      "     |  vary from corpus to corpus; the ``ConllCorpusReader`` constructor\n",
      "     |  therefore takes an argument, ``columntypes``, which is used to\n",
      "     |  specify the columns that are used by a given corpus. By default\n",
      "     |  columns are split by consecutive whitespaces, with the\n",
      "     |  ``separator`` argument you can set a string to split by (e.g.\n",
      "     |  ``' '``).\n",
      "     |  \n",
      "     |  \n",
      "     |  @todo: Add support for reading from corpora where different\n",
      "     |      parallel files contain different columns.\n",
      "     |  @todo: Possibly add caching of the grid corpus view?  This would\n",
      "     |      allow the same grid view to be used by different data access\n",
      "     |      methods (eg words() and parsed_sents() could both share the\n",
      "     |      same grid corpus view object).\n",
      "     |  @todo: Better support for -DOCSTART-.  Currently, we just ignore\n",
      "     |      it, but it could be used to define methods that retrieve a\n",
      "     |      document at a time (eg parsed_documents()).\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      ConllCorpusReader\n",
      "     |      nltk.corpus.reader.api.CorpusReader\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, root, fileids, columntypes, chunk_types=None, root_label='S', pos_in_tree=False, srl_includes_roleset=True, encoding='utf8', tree_class=<class 'nltk.tree.Tree'>, tagset=None, separator=None)\n",
      "     |      :type root: PathPointer or str\n",
      "     |      :param root: A path pointer identifying the root directory for\n",
      "     |          this corpus.  If a string is specified, then it will be\n",
      "     |          converted to a ``PathPointer`` automatically.\n",
      "     |      :param fileids: A list of the files that make up this corpus.\n",
      "     |          This list can either be specified explicitly, as a list of\n",
      "     |          strings; or implicitly, as a regular expression over file\n",
      "     |          paths.  The absolute path for each file will be constructed\n",
      "     |          by joining the reader's root to each file name.\n",
      "     |      :param encoding: The default unicode encoding for the files\n",
      "     |          that make up the corpus.  The value of ``encoding`` can be any\n",
      "     |          of the following:\n",
      "     |          - A string: ``encoding`` is the encoding name for all files.\n",
      "     |          - A dictionary: ``encoding[file_id]`` is the encoding\n",
      "     |            name for the file whose identifier is ``file_id``.  If\n",
      "     |            ``file_id`` is not in ``encoding``, then the file\n",
      "     |            contents will be processed using non-unicode byte strings.\n",
      "     |          - A list: ``encoding`` should be a list of ``(regexp, encoding)``\n",
      "     |            tuples.  The encoding for a file whose identifier is ``file_id``\n",
      "     |            will be the ``encoding`` value for the first tuple whose\n",
      "     |            ``regexp`` matches the ``file_id``.  If no tuple's ``regexp``\n",
      "     |            matches the ``file_id``, the file contents will be processed\n",
      "     |            using non-unicode byte strings.\n",
      "     |          - None: the file contents of all files will be\n",
      "     |            processed using non-unicode byte strings.\n",
      "     |      :param tagset: The name of the tagset used by this corpus, to be used\n",
      "     |            for normalizing or converting the POS tags returned by the\n",
      "     |            tagged_...() methods.\n",
      "     |  \n",
      "     |  chunked_sents(self, fileids=None, chunk_types=None, tagset=None)\n",
      "     |  \n",
      "     |  chunked_words(self, fileids=None, chunk_types=None, tagset=None)\n",
      "     |  \n",
      "     |  iob_sents(self, fileids=None, tagset=None)\n",
      "     |      :return: a list of lists of word/tag/IOB tuples\n",
      "     |      :rtype: list(list)\n",
      "     |      :param fileids: the list of fileids that make up this corpus\n",
      "     |      :type fileids: None or str or list\n",
      "     |  \n",
      "     |  iob_words(self, fileids=None, tagset=None)\n",
      "     |      :return: a list of word/tag/IOB tuples\n",
      "     |      :rtype: list(tuple)\n",
      "     |      :param fileids: the list of fileids that make up this corpus\n",
      "     |      :type fileids: None or str or list\n",
      "     |  \n",
      "     |  parsed_sents(self, fileids=None, pos_in_tree=None, tagset=None)\n",
      "     |  \n",
      "     |  raw(self, fileids=None)\n",
      "     |  \n",
      "     |  sents(self, fileids=None)\n",
      "     |  \n",
      "     |  srl_instances(self, fileids=None, pos_in_tree=None, flatten=True)\n",
      "     |  \n",
      "     |  srl_spans(self, fileids=None)\n",
      "     |  \n",
      "     |  tagged_sents(self, fileids=None, tagset=None)\n",
      "     |  \n",
      "     |  tagged_words(self, fileids=None, tagset=None)\n",
      "     |  \n",
      "     |  words(self, fileids=None)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  CHUNK = 'chunk'\n",
      "     |  \n",
      "     |  COLUMN_TYPES = ('words', 'pos', 'tree', 'chunk', 'ne', 'srl', 'ignore'...\n",
      "     |  \n",
      "     |  IGNORE = 'ignore'\n",
      "     |  \n",
      "     |  NE = 'ne'\n",
      "     |  \n",
      "     |  POS = 'pos'\n",
      "     |  \n",
      "     |  SRL = 'srl'\n",
      "     |  \n",
      "     |  TREE = 'tree'\n",
      "     |  \n",
      "     |  WORDS = 'words'\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __unicode__ = __str__(self, /)\n",
      "     |  \n",
      "     |  abspath(self, fileid)\n",
      "     |      Return the absolute path for the given file.\n",
      "     |      \n",
      "     |      :type fileid: str\n",
      "     |      :param fileid: The file identifier for the file whose path\n",
      "     |          should be returned.\n",
      "     |      :rtype: PathPointer\n",
      "     |  \n",
      "     |  abspaths(self, fileids=None, include_encoding=False, include_fileid=False)\n",
      "     |      Return a list of the absolute paths for all fileids in this corpus;\n",
      "     |      or for the given list of fileids, if specified.\n",
      "     |      \n",
      "     |      :type fileids: None or str or list\n",
      "     |      :param fileids: Specifies the set of fileids for which paths should\n",
      "     |          be returned.  Can be None, for all fileids; a list of\n",
      "     |          file identifiers, for a specified set of fileids; or a single\n",
      "     |          file identifier, for a single file.  Note that the return\n",
      "     |          value is always a list of paths, even if ``fileids`` is a\n",
      "     |          single file identifier.\n",
      "     |      \n",
      "     |      :param include_encoding: If true, then return a list of\n",
      "     |          ``(path_pointer, encoding)`` tuples.\n",
      "     |      \n",
      "     |      :rtype: list(PathPointer)\n",
      "     |  \n",
      "     |  citation(self)\n",
      "     |      Return the contents of the corpus citation.bib file, if it exists.\n",
      "     |  \n",
      "     |  encoding(self, file)\n",
      "     |      Return the unicode encoding for the given corpus file, if known.\n",
      "     |      If the encoding is unknown, or if the given file should be\n",
      "     |      processed using byte strings (str), then return None.\n",
      "     |  \n",
      "     |  ensure_loaded(self)\n",
      "     |      Load this corpus (if it has not already been loaded).  This is\n",
      "     |      used by LazyCorpusLoader as a simple method that can be used to\n",
      "     |      make sure a corpus is loaded -- e.g., in case a user wants to\n",
      "     |      do help(some_corpus).\n",
      "     |  \n",
      "     |  fileids(self)\n",
      "     |      Return a list of file identifiers for the fileids that make up\n",
      "     |      this corpus.\n",
      "     |  \n",
      "     |  license(self)\n",
      "     |      Return the contents of the corpus LICENSE file, if it exists.\n",
      "     |  \n",
      "     |  open(self, file)\n",
      "     |      Return an open stream that can be used to read the given file.\n",
      "     |      If the file's encoding is not None, then the stream will\n",
      "     |      automatically decode the file's contents into unicode.\n",
      "     |      \n",
      "     |      :param file: The file identifier of the file to read.\n",
      "     |  \n",
      "     |  readme(self)\n",
      "     |      Return the contents of the corpus README file, if it exists.\n",
      "     |  \n",
      "     |  unicode_repr = __repr__(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  root\n",
      "     |      The directory where this corpus is stored.\n",
      "     |      \n",
      "     |      :type: PathPointer\n",
      "    \n",
      "    class CorpusReader(builtins.object)\n",
      "     |  CorpusReader(root, fileids, encoding='utf8', tagset=None)\n",
      "     |  \n",
      "     |  A base class for \"corpus reader\" classes, each of which can be\n",
      "     |  used to read a specific corpus format.  Each individual corpus\n",
      "     |  reader instance is used to read a specific corpus, consisting of\n",
      "     |  one or more files under a common root directory.  Each file is\n",
      "     |  identified by its ``file identifier``, which is the relative path\n",
      "     |  to the file from the root directory.\n",
      "     |  \n",
      "     |  A separate subclass is defined for each corpus format.  These\n",
      "     |  subclasses define one or more methods that provide 'views' on the\n",
      "     |  corpus contents, such as ``words()`` (for a list of words) and\n",
      "     |  ``parsed_sents()`` (for a list of parsed sentences).  Called with\n",
      "     |  no arguments, these methods will return the contents of the entire\n",
      "     |  corpus.  For most corpora, these methods define one or more\n",
      "     |  selection arguments, such as ``fileids`` or ``categories``, which can\n",
      "     |  be used to select which portion of the corpus should be returned.\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, root, fileids, encoding='utf8', tagset=None)\n",
      "     |      :type root: PathPointer or str\n",
      "     |      :param root: A path pointer identifying the root directory for\n",
      "     |          this corpus.  If a string is specified, then it will be\n",
      "     |          converted to a ``PathPointer`` automatically.\n",
      "     |      :param fileids: A list of the files that make up this corpus.\n",
      "     |          This list can either be specified explicitly, as a list of\n",
      "     |          strings; or implicitly, as a regular expression over file\n",
      "     |          paths.  The absolute path for each file will be constructed\n",
      "     |          by joining the reader's root to each file name.\n",
      "     |      :param encoding: The default unicode encoding for the files\n",
      "     |          that make up the corpus.  The value of ``encoding`` can be any\n",
      "     |          of the following:\n",
      "     |          - A string: ``encoding`` is the encoding name for all files.\n",
      "     |          - A dictionary: ``encoding[file_id]`` is the encoding\n",
      "     |            name for the file whose identifier is ``file_id``.  If\n",
      "     |            ``file_id`` is not in ``encoding``, then the file\n",
      "     |            contents will be processed using non-unicode byte strings.\n",
      "     |          - A list: ``encoding`` should be a list of ``(regexp, encoding)``\n",
      "     |            tuples.  The encoding for a file whose identifier is ``file_id``\n",
      "     |            will be the ``encoding`` value for the first tuple whose\n",
      "     |            ``regexp`` matches the ``file_id``.  If no tuple's ``regexp``\n",
      "     |            matches the ``file_id``, the file contents will be processed\n",
      "     |            using non-unicode byte strings.\n",
      "     |          - None: the file contents of all files will be\n",
      "     |            processed using non-unicode byte strings.\n",
      "     |      :param tagset: The name of the tagset used by this corpus, to be used\n",
      "     |            for normalizing or converting the POS tags returned by the\n",
      "     |            tagged_...() methods.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __unicode__ = __str__(self, /)\n",
      "     |  \n",
      "     |  abspath(self, fileid)\n",
      "     |      Return the absolute path for the given file.\n",
      "     |      \n",
      "     |      :type fileid: str\n",
      "     |      :param fileid: The file identifier for the file whose path\n",
      "     |          should be returned.\n",
      "     |      :rtype: PathPointer\n",
      "     |  \n",
      "     |  abspaths(self, fileids=None, include_encoding=False, include_fileid=False)\n",
      "     |      Return a list of the absolute paths for all fileids in this corpus;\n",
      "     |      or for the given list of fileids, if specified.\n",
      "     |      \n",
      "     |      :type fileids: None or str or list\n",
      "     |      :param fileids: Specifies the set of fileids for which paths should\n",
      "     |          be returned.  Can be None, for all fileids; a list of\n",
      "     |          file identifiers, for a specified set of fileids; or a single\n",
      "     |          file identifier, for a single file.  Note that the return\n",
      "     |          value is always a list of paths, even if ``fileids`` is a\n",
      "     |          single file identifier.\n",
      "     |      \n",
      "     |      :param include_encoding: If true, then return a list of\n",
      "     |          ``(path_pointer, encoding)`` tuples.\n",
      "     |      \n",
      "     |      :rtype: list(PathPointer)\n",
      "     |  \n",
      "     |  citation(self)\n",
      "     |      Return the contents of the corpus citation.bib file, if it exists.\n",
      "     |  \n",
      "     |  encoding(self, file)\n",
      "     |      Return the unicode encoding for the given corpus file, if known.\n",
      "     |      If the encoding is unknown, or if the given file should be\n",
      "     |      processed using byte strings (str), then return None.\n",
      "     |  \n",
      "     |  ensure_loaded(self)\n",
      "     |      Load this corpus (if it has not already been loaded).  This is\n",
      "     |      used by LazyCorpusLoader as a simple method that can be used to\n",
      "     |      make sure a corpus is loaded -- e.g., in case a user wants to\n",
      "     |      do help(some_corpus).\n",
      "     |  \n",
      "     |  fileids(self)\n",
      "     |      Return a list of file identifiers for the fileids that make up\n",
      "     |      this corpus.\n",
      "     |  \n",
      "     |  license(self)\n",
      "     |      Return the contents of the corpus LICENSE file, if it exists.\n",
      "     |  \n",
      "     |  open(self, file)\n",
      "     |      Return an open stream that can be used to read the given file.\n",
      "     |      If the file's encoding is not None, then the stream will\n",
      "     |      automatically decode the file's contents into unicode.\n",
      "     |      \n",
      "     |      :param file: The file identifier of the file to read.\n",
      "     |  \n",
      "     |  readme(self)\n",
      "     |      Return the contents of the corpus README file, if it exists.\n",
      "     |  \n",
      "     |  unicode_repr = __repr__(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  root\n",
      "     |      The directory where this corpus is stored.\n",
      "     |      \n",
      "     |      :type: PathPointer\n",
      "    \n",
      "    class CrubadanCorpusReader(nltk.corpus.reader.api.CorpusReader)\n",
      "     |  CrubadanCorpusReader(root, fileids, encoding='utf8', tagset=None)\n",
      "     |  \n",
      "     |  A corpus reader used to access language An Crubadan n-gram files.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      CrubadanCorpusReader\n",
      "     |      nltk.corpus.reader.api.CorpusReader\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, root, fileids, encoding='utf8', tagset=None)\n",
      "     |      :type root: PathPointer or str\n",
      "     |      :param root: A path pointer identifying the root directory for\n",
      "     |          this corpus.  If a string is specified, then it will be\n",
      "     |          converted to a ``PathPointer`` automatically.\n",
      "     |      :param fileids: A list of the files that make up this corpus.\n",
      "     |          This list can either be specified explicitly, as a list of\n",
      "     |          strings; or implicitly, as a regular expression over file\n",
      "     |          paths.  The absolute path for each file will be constructed\n",
      "     |          by joining the reader's root to each file name.\n",
      "     |      :param encoding: The default unicode encoding for the files\n",
      "     |          that make up the corpus.  The value of ``encoding`` can be any\n",
      "     |          of the following:\n",
      "     |          - A string: ``encoding`` is the encoding name for all files.\n",
      "     |          - A dictionary: ``encoding[file_id]`` is the encoding\n",
      "     |            name for the file whose identifier is ``file_id``.  If\n",
      "     |            ``file_id`` is not in ``encoding``, then the file\n",
      "     |            contents will be processed using non-unicode byte strings.\n",
      "     |          - A list: ``encoding`` should be a list of ``(regexp, encoding)``\n",
      "     |            tuples.  The encoding for a file whose identifier is ``file_id``\n",
      "     |            will be the ``encoding`` value for the first tuple whose\n",
      "     |            ``regexp`` matches the ``file_id``.  If no tuple's ``regexp``\n",
      "     |            matches the ``file_id``, the file contents will be processed\n",
      "     |            using non-unicode byte strings.\n",
      "     |          - None: the file contents of all files will be\n",
      "     |            processed using non-unicode byte strings.\n",
      "     |      :param tagset: The name of the tagset used by this corpus, to be used\n",
      "     |            for normalizing or converting the POS tags returned by the\n",
      "     |            tagged_...() methods.\n",
      "     |  \n",
      "     |  crubadan_to_iso(self, lang)\n",
      "     |      Return ISO 639-3 code given internal Crubadan code\n",
      "     |  \n",
      "     |  iso_to_crubadan(self, lang)\n",
      "     |      Return internal Crubadan code based on ISO 639-3 code\n",
      "     |  \n",
      "     |  lang_freq(self, lang)\n",
      "     |      Return n-gram FreqDist for a specific language\n",
      "     |      given ISO 639-3 language code\n",
      "     |  \n",
      "     |  langs(self)\n",
      "     |      Return a list of supported languages as ISO 639-3 codes\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __unicode__ = __str__(self, /)\n",
      "     |  \n",
      "     |  abspath(self, fileid)\n",
      "     |      Return the absolute path for the given file.\n",
      "     |      \n",
      "     |      :type fileid: str\n",
      "     |      :param fileid: The file identifier for the file whose path\n",
      "     |          should be returned.\n",
      "     |      :rtype: PathPointer\n",
      "     |  \n",
      "     |  abspaths(self, fileids=None, include_encoding=False, include_fileid=False)\n",
      "     |      Return a list of the absolute paths for all fileids in this corpus;\n",
      "     |      or for the given list of fileids, if specified.\n",
      "     |      \n",
      "     |      :type fileids: None or str or list\n",
      "     |      :param fileids: Specifies the set of fileids for which paths should\n",
      "     |          be returned.  Can be None, for all fileids; a list of\n",
      "     |          file identifiers, for a specified set of fileids; or a single\n",
      "     |          file identifier, for a single file.  Note that the return\n",
      "     |          value is always a list of paths, even if ``fileids`` is a\n",
      "     |          single file identifier.\n",
      "     |      \n",
      "     |      :param include_encoding: If true, then return a list of\n",
      "     |          ``(path_pointer, encoding)`` tuples.\n",
      "     |      \n",
      "     |      :rtype: list(PathPointer)\n",
      "     |  \n",
      "     |  citation(self)\n",
      "     |      Return the contents of the corpus citation.bib file, if it exists.\n",
      "     |  \n",
      "     |  encoding(self, file)\n",
      "     |      Return the unicode encoding for the given corpus file, if known.\n",
      "     |      If the encoding is unknown, or if the given file should be\n",
      "     |      processed using byte strings (str), then return None.\n",
      "     |  \n",
      "     |  ensure_loaded(self)\n",
      "     |      Load this corpus (if it has not already been loaded).  This is\n",
      "     |      used by LazyCorpusLoader as a simple method that can be used to\n",
      "     |      make sure a corpus is loaded -- e.g., in case a user wants to\n",
      "     |      do help(some_corpus).\n",
      "     |  \n",
      "     |  fileids(self)\n",
      "     |      Return a list of file identifiers for the fileids that make up\n",
      "     |      this corpus.\n",
      "     |  \n",
      "     |  license(self)\n",
      "     |      Return the contents of the corpus LICENSE file, if it exists.\n",
      "     |  \n",
      "     |  open(self, file)\n",
      "     |      Return an open stream that can be used to read the given file.\n",
      "     |      If the file's encoding is not None, then the stream will\n",
      "     |      automatically decode the file's contents into unicode.\n",
      "     |      \n",
      "     |      :param file: The file identifier of the file to read.\n",
      "     |  \n",
      "     |  readme(self)\n",
      "     |      Return the contents of the corpus README file, if it exists.\n",
      "     |  \n",
      "     |  unicode_repr = __repr__(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  root\n",
      "     |      The directory where this corpus is stored.\n",
      "     |      \n",
      "     |      :type: PathPointer\n",
      "    \n",
      "    class DependencyCorpusReader(nltk.corpus.reader.api.SyntaxCorpusReader)\n",
      "     |  DependencyCorpusReader(root, fileids, encoding='utf8', word_tokenizer=<nltk.tokenize.simple.TabTokenizer object at 0x1259d6cd0>, sent_tokenizer=RegexpTokenizer(pattern='\\n', gaps=True, discard_empty=True, flags=<RegexFlag.UNICODE|DOTALL|MULTILINE: 56>), para_block_reader=<function read_blankline_block at 0x125911320>)\n",
      "     |  \n",
      "     |  An abstract base class for reading corpora consisting of\n",
      "     |  syntactically parsed text.  Subclasses should define:\n",
      "     |  \n",
      "     |    - ``__init__``, which specifies the location of the corpus\n",
      "     |      and a method for detecting the sentence blocks in corpus files.\n",
      "     |    - ``_read_block``, which reads a block from the input stream.\n",
      "     |    - ``_word``, which takes a block and returns a list of list of words.\n",
      "     |    - ``_tag``, which takes a block and returns a list of list of tagged\n",
      "     |      words.\n",
      "     |    - ``_parse``, which takes a block and returns a list of parsed\n",
      "     |      sentences.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      DependencyCorpusReader\n",
      "     |      nltk.corpus.reader.api.SyntaxCorpusReader\n",
      "     |      nltk.corpus.reader.api.CorpusReader\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, root, fileids, encoding='utf8', word_tokenizer=<nltk.tokenize.simple.TabTokenizer object at 0x1259d6cd0>, sent_tokenizer=RegexpTokenizer(pattern='\\n', gaps=True, discard_empty=True, flags=<RegexFlag.UNICODE|DOTALL|MULTILINE: 56>), para_block_reader=<function read_blankline_block at 0x125911320>)\n",
      "     |      :type root: PathPointer or str\n",
      "     |      :param root: A path pointer identifying the root directory for\n",
      "     |          this corpus.  If a string is specified, then it will be\n",
      "     |          converted to a ``PathPointer`` automatically.\n",
      "     |      :param fileids: A list of the files that make up this corpus.\n",
      "     |          This list can either be specified explicitly, as a list of\n",
      "     |          strings; or implicitly, as a regular expression over file\n",
      "     |          paths.  The absolute path for each file will be constructed\n",
      "     |          by joining the reader's root to each file name.\n",
      "     |      :param encoding: The default unicode encoding for the files\n",
      "     |          that make up the corpus.  The value of ``encoding`` can be any\n",
      "     |          of the following:\n",
      "     |          - A string: ``encoding`` is the encoding name for all files.\n",
      "     |          - A dictionary: ``encoding[file_id]`` is the encoding\n",
      "     |            name for the file whose identifier is ``file_id``.  If\n",
      "     |            ``file_id`` is not in ``encoding``, then the file\n",
      "     |            contents will be processed using non-unicode byte strings.\n",
      "     |          - A list: ``encoding`` should be a list of ``(regexp, encoding)``\n",
      "     |            tuples.  The encoding for a file whose identifier is ``file_id``\n",
      "     |            will be the ``encoding`` value for the first tuple whose\n",
      "     |            ``regexp`` matches the ``file_id``.  If no tuple's ``regexp``\n",
      "     |            matches the ``file_id``, the file contents will be processed\n",
      "     |            using non-unicode byte strings.\n",
      "     |          - None: the file contents of all files will be\n",
      "     |            processed using non-unicode byte strings.\n",
      "     |      :param tagset: The name of the tagset used by this corpus, to be used\n",
      "     |            for normalizing or converting the POS tags returned by the\n",
      "     |            tagged_...() methods.\n",
      "     |  \n",
      "     |  parsed_sents(self, fileids=None)\n",
      "     |  \n",
      "     |  raw(self, fileids=None)\n",
      "     |      :return: the given file(s) as a single string.\n",
      "     |      :rtype: str\n",
      "     |  \n",
      "     |  sents(self, fileids=None)\n",
      "     |  \n",
      "     |  tagged_sents(self, fileids=None)\n",
      "     |  \n",
      "     |  tagged_words(self, fileids=None)\n",
      "     |  \n",
      "     |  words(self, fileids=None)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __unicode__ = __str__(self, /)\n",
      "     |  \n",
      "     |  abspath(self, fileid)\n",
      "     |      Return the absolute path for the given file.\n",
      "     |      \n",
      "     |      :type fileid: str\n",
      "     |      :param fileid: The file identifier for the file whose path\n",
      "     |          should be returned.\n",
      "     |      :rtype: PathPointer\n",
      "     |  \n",
      "     |  abspaths(self, fileids=None, include_encoding=False, include_fileid=False)\n",
      "     |      Return a list of the absolute paths for all fileids in this corpus;\n",
      "     |      or for the given list of fileids, if specified.\n",
      "     |      \n",
      "     |      :type fileids: None or str or list\n",
      "     |      :param fileids: Specifies the set of fileids for which paths should\n",
      "     |          be returned.  Can be None, for all fileids; a list of\n",
      "     |          file identifiers, for a specified set of fileids; or a single\n",
      "     |          file identifier, for a single file.  Note that the return\n",
      "     |          value is always a list of paths, even if ``fileids`` is a\n",
      "     |          single file identifier.\n",
      "     |      \n",
      "     |      :param include_encoding: If true, then return a list of\n",
      "     |          ``(path_pointer, encoding)`` tuples.\n",
      "     |      \n",
      "     |      :rtype: list(PathPointer)\n",
      "     |  \n",
      "     |  citation(self)\n",
      "     |      Return the contents of the corpus citation.bib file, if it exists.\n",
      "     |  \n",
      "     |  encoding(self, file)\n",
      "     |      Return the unicode encoding for the given corpus file, if known.\n",
      "     |      If the encoding is unknown, or if the given file should be\n",
      "     |      processed using byte strings (str), then return None.\n",
      "     |  \n",
      "     |  ensure_loaded(self)\n",
      "     |      Load this corpus (if it has not already been loaded).  This is\n",
      "     |      used by LazyCorpusLoader as a simple method that can be used to\n",
      "     |      make sure a corpus is loaded -- e.g., in case a user wants to\n",
      "     |      do help(some_corpus).\n",
      "     |  \n",
      "     |  fileids(self)\n",
      "     |      Return a list of file identifiers for the fileids that make up\n",
      "     |      this corpus.\n",
      "     |  \n",
      "     |  license(self)\n",
      "     |      Return the contents of the corpus LICENSE file, if it exists.\n",
      "     |  \n",
      "     |  open(self, file)\n",
      "     |      Return an open stream that can be used to read the given file.\n",
      "     |      If the file's encoding is not None, then the stream will\n",
      "     |      automatically decode the file's contents into unicode.\n",
      "     |      \n",
      "     |      :param file: The file identifier of the file to read.\n",
      "     |  \n",
      "     |  readme(self)\n",
      "     |      Return the contents of the corpus README file, if it exists.\n",
      "     |  \n",
      "     |  unicode_repr = __repr__(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  root\n",
      "     |      The directory where this corpus is stored.\n",
      "     |      \n",
      "     |      :type: PathPointer\n",
      "    \n",
      "    class EuroparlCorpusReader(PlaintextCorpusReader)\n",
      "     |  EuroparlCorpusReader(root, fileids, word_tokenizer=WordPunctTokenizer(pattern='\\\\w+|[^\\\\w\\\\s]+', gaps=False, discard_empty=True, flags=<RegexFlag.UNICODE|DOTALL|MULTILINE: 56>), sent_tokenizer=<nltk.tokenize.punkt.PunktSentenceTokenizer object at 0x125903c50>, para_block_reader=<function read_blankline_block at 0x125911320>, encoding='utf8')\n",
      "     |  \n",
      "     |  Reader for Europarl corpora that consist of plaintext documents.\n",
      "     |  Documents are divided into chapters instead of paragraphs as\n",
      "     |  for regular plaintext documents. Chapters are separated using blank\n",
      "     |  lines. Everything is inherited from ``PlaintextCorpusReader`` except\n",
      "     |  that:\n",
      "     |    - Since the corpus is pre-processed and pre-tokenized, the\n",
      "     |      word tokenizer should just split the line at whitespaces.\n",
      "     |    - For the same reason, the sentence tokenizer should just\n",
      "     |      split the paragraph at line breaks.\n",
      "     |    - There is a new 'chapters()' method that returns chapters instead\n",
      "     |      instead of paragraphs.\n",
      "     |    - The 'paras()' method inherited from PlaintextCorpusReader is\n",
      "     |      made non-functional to remove any confusion between chapters\n",
      "     |      and paragraphs for Europarl.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      EuroparlCorpusReader\n",
      "     |      PlaintextCorpusReader\n",
      "     |      nltk.corpus.reader.api.CorpusReader\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  chapters(self, fileids=None)\n",
      "     |      :return: the given file(s) as a list of\n",
      "     |          chapters, each encoded as a list of sentences, which are\n",
      "     |          in turn encoded as lists of word strings.\n",
      "     |      :rtype: list(list(list(str)))\n",
      "     |  \n",
      "     |  paras(self, fileids=None)\n",
      "     |      :return: the given file(s) as a list of\n",
      "     |          paragraphs, each encoded as a list of sentences, which are\n",
      "     |          in turn encoded as lists of word strings.\n",
      "     |      :rtype: list(list(list(str)))\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from PlaintextCorpusReader:\n",
      "     |  \n",
      "     |  __init__(self, root, fileids, word_tokenizer=WordPunctTokenizer(pattern='\\\\w+|[^\\\\w\\\\s]+', gaps=False, discard_empty=True, flags=<RegexFlag.UNICODE|DOTALL|MULTILINE: 56>), sent_tokenizer=<nltk.tokenize.punkt.PunktSentenceTokenizer object at 0x125903c50>, para_block_reader=<function read_blankline_block at 0x125911320>, encoding='utf8')\n",
      "     |      Construct a new plaintext corpus reader for a set of documents\n",
      "     |      located at the given root directory.  Example usage:\n",
      "     |      \n",
      "     |          >>> root = '/usr/local/share/nltk_data/corpora/webtext/'\n",
      "     |          >>> reader = PlaintextCorpusReader(root, '.*\\.txt') # doctest: +SKIP\n",
      "     |      \n",
      "     |      :param root: The root directory for this corpus.\n",
      "     |      :param fileids: A list or regexp specifying the fileids in this corpus.\n",
      "     |      :param word_tokenizer: Tokenizer for breaking sentences or\n",
      "     |          paragraphs into words.\n",
      "     |      :param sent_tokenizer: Tokenizer for breaking paragraphs\n",
      "     |          into words.\n",
      "     |      :param para_block_reader: The block reader used to divide the\n",
      "     |          corpus into paragraph blocks.\n",
      "     |  \n",
      "     |  raw(self, fileids=None)\n",
      "     |      :return: the given file(s) as a single string.\n",
      "     |      :rtype: str\n",
      "     |  \n",
      "     |  sents(self, fileids=None)\n",
      "     |      :return: the given file(s) as a list of\n",
      "     |          sentences or utterances, each encoded as a list of word\n",
      "     |          strings.\n",
      "     |      :rtype: list(list(str))\n",
      "     |  \n",
      "     |  words(self, fileids=None)\n",
      "     |      :return: the given file(s) as a list of words\n",
      "     |          and punctuation symbols.\n",
      "     |      :rtype: list(str)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from PlaintextCorpusReader:\n",
      "     |  \n",
      "     |  CorpusView = <class 'nltk.corpus.reader.util.StreamBackedCorpusView'>\n",
      "     |      A 'view' of a corpus file, which acts like a sequence of tokens:\n",
      "     |      it can be accessed by index, iterated over, etc.  However, the\n",
      "     |      tokens are only constructed as-needed -- the entire corpus is\n",
      "     |      never stored in memory at once.\n",
      "     |      \n",
      "     |      The constructor to ``StreamBackedCorpusView`` takes two arguments:\n",
      "     |      a corpus fileid (specified as a string or as a ``PathPointer``);\n",
      "     |      and a block reader.  A \"block reader\" is a function that reads\n",
      "     |      zero or more tokens from a stream, and returns them as a list.  A\n",
      "     |      very simple example of a block reader is:\n",
      "     |      \n",
      "     |          >>> def simple_block_reader(stream):\n",
      "     |          ...     return stream.readline().split()\n",
      "     |      \n",
      "     |      This simple block reader reads a single line at a time, and\n",
      "     |      returns a single token (consisting of a string) for each\n",
      "     |      whitespace-separated substring on the line.\n",
      "     |      \n",
      "     |      When deciding how to define the block reader for a given\n",
      "     |      corpus, careful consideration should be given to the size of\n",
      "     |      blocks handled by the block reader.  Smaller block sizes will\n",
      "     |      increase the memory requirements of the corpus view's internal\n",
      "     |      data structures (by 2 integers per block).  On the other hand,\n",
      "     |      larger block sizes may decrease performance for random access to\n",
      "     |      the corpus.  (But note that larger block sizes will *not*\n",
      "     |      decrease performance for iteration.)\n",
      "     |      \n",
      "     |      Internally, ``CorpusView`` maintains a partial mapping from token\n",
      "     |      index to file position, with one entry per block.  When a token\n",
      "     |      with a given index *i* is requested, the ``CorpusView`` constructs\n",
      "     |      it as follows:\n",
      "     |      \n",
      "     |        1. First, it searches the toknum/filepos mapping for the token\n",
      "     |           index closest to (but less than or equal to) *i*.\n",
      "     |      \n",
      "     |        2. Then, starting at the file position corresponding to that\n",
      "     |           index, it reads one block at a time using the block reader\n",
      "     |           until it reaches the requested token.\n",
      "     |      \n",
      "     |      The toknum/filepos mapping is created lazily: it is initially\n",
      "     |      empty, but every time a new block is read, the block's\n",
      "     |      initial token is added to the mapping.  (Thus, the toknum/filepos\n",
      "     |      map has one entry per block.)\n",
      "     |      \n",
      "     |      In order to increase efficiency for random access patterns that\n",
      "     |      have high degrees of locality, the corpus view may cache one or\n",
      "     |      more blocks.\n",
      "     |      \n",
      "     |      :note: Each ``CorpusView`` object internally maintains an open file\n",
      "     |          object for its underlying corpus file.  This file should be\n",
      "     |          automatically closed when the ``CorpusView`` is garbage collected,\n",
      "     |          but if you wish to close it manually, use the ``close()``\n",
      "     |          method.  If you access a ``CorpusView``'s items after it has been\n",
      "     |          closed, the file object will be automatically re-opened.\n",
      "     |      \n",
      "     |      :warning: If the contents of the file are modified during the\n",
      "     |          lifetime of the ``CorpusView``, then the ``CorpusView``'s behavior\n",
      "     |          is undefined.\n",
      "     |      \n",
      "     |      :warning: If a unicode encoding is specified when constructing a\n",
      "     |          ``CorpusView``, then the block reader may only call\n",
      "     |          ``stream.seek()`` with offsets that have been returned by\n",
      "     |          ``stream.tell()``; in particular, calling ``stream.seek()`` with\n",
      "     |          relative offsets, or with offsets based on string lengths, may\n",
      "     |          lead to incorrect behavior.\n",
      "     |      \n",
      "     |      :ivar _block_reader: The function used to read\n",
      "     |          a single block from the underlying file stream.\n",
      "     |      :ivar _toknum: A list containing the token index of each block\n",
      "     |          that has been processed.  In particular, ``_toknum[i]`` is the\n",
      "     |          token index of the first token in block ``i``.  Together\n",
      "     |          with ``_filepos``, this forms a partial mapping between token\n",
      "     |          indices and file positions.\n",
      "     |      :ivar _filepos: A list containing the file position of each block\n",
      "     |          that has been processed.  In particular, ``_toknum[i]`` is the\n",
      "     |          file position of the first character in block ``i``.  Together\n",
      "     |          with ``_toknum``, this forms a partial mapping between token\n",
      "     |          indices and file positions.\n",
      "     |      :ivar _stream: The stream used to access the underlying corpus file.\n",
      "     |      :ivar _len: The total number of tokens in the corpus, if known;\n",
      "     |          or None, if the number of tokens is not yet known.\n",
      "     |      :ivar _eofpos: The character position of the last character in the\n",
      "     |          file.  This is calculated when the corpus view is initialized,\n",
      "     |          and is used to decide when the end of file has been reached.\n",
      "     |      :ivar _cache: A cache of the most recently read block.  It\n",
      "     |         is encoded as a tuple (start_toknum, end_toknum, tokens), where\n",
      "     |         start_toknum is the token index of the first token in the block;\n",
      "     |         end_toknum is the token index of the first token not in the\n",
      "     |         block; and tokens is a list of the tokens in the block.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __unicode__ = __str__(self, /)\n",
      "     |  \n",
      "     |  abspath(self, fileid)\n",
      "     |      Return the absolute path for the given file.\n",
      "     |      \n",
      "     |      :type fileid: str\n",
      "     |      :param fileid: The file identifier for the file whose path\n",
      "     |          should be returned.\n",
      "     |      :rtype: PathPointer\n",
      "     |  \n",
      "     |  abspaths(self, fileids=None, include_encoding=False, include_fileid=False)\n",
      "     |      Return a list of the absolute paths for all fileids in this corpus;\n",
      "     |      or for the given list of fileids, if specified.\n",
      "     |      \n",
      "     |      :type fileids: None or str or list\n",
      "     |      :param fileids: Specifies the set of fileids for which paths should\n",
      "     |          be returned.  Can be None, for all fileids; a list of\n",
      "     |          file identifiers, for a specified set of fileids; or a single\n",
      "     |          file identifier, for a single file.  Note that the return\n",
      "     |          value is always a list of paths, even if ``fileids`` is a\n",
      "     |          single file identifier.\n",
      "     |      \n",
      "     |      :param include_encoding: If true, then return a list of\n",
      "     |          ``(path_pointer, encoding)`` tuples.\n",
      "     |      \n",
      "     |      :rtype: list(PathPointer)\n",
      "     |  \n",
      "     |  citation(self)\n",
      "     |      Return the contents of the corpus citation.bib file, if it exists.\n",
      "     |  \n",
      "     |  encoding(self, file)\n",
      "     |      Return the unicode encoding for the given corpus file, if known.\n",
      "     |      If the encoding is unknown, or if the given file should be\n",
      "     |      processed using byte strings (str), then return None.\n",
      "     |  \n",
      "     |  ensure_loaded(self)\n",
      "     |      Load this corpus (if it has not already been loaded).  This is\n",
      "     |      used by LazyCorpusLoader as a simple method that can be used to\n",
      "     |      make sure a corpus is loaded -- e.g., in case a user wants to\n",
      "     |      do help(some_corpus).\n",
      "     |  \n",
      "     |  fileids(self)\n",
      "     |      Return a list of file identifiers for the fileids that make up\n",
      "     |      this corpus.\n",
      "     |  \n",
      "     |  license(self)\n",
      "     |      Return the contents of the corpus LICENSE file, if it exists.\n",
      "     |  \n",
      "     |  open(self, file)\n",
      "     |      Return an open stream that can be used to read the given file.\n",
      "     |      If the file's encoding is not None, then the stream will\n",
      "     |      automatically decode the file's contents into unicode.\n",
      "     |      \n",
      "     |      :param file: The file identifier of the file to read.\n",
      "     |  \n",
      "     |  readme(self)\n",
      "     |      Return the contents of the corpus README file, if it exists.\n",
      "     |  \n",
      "     |  unicode_repr = __repr__(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  root\n",
      "     |      The directory where this corpus is stored.\n",
      "     |      \n",
      "     |      :type: PathPointer\n",
      "    \n",
      "    class FramenetCorpusReader(nltk.corpus.reader.xmldocs.XMLCorpusReader)\n",
      "     |  FramenetCorpusReader(root, fileids)\n",
      "     |  \n",
      "     |  A corpus reader for the Framenet Corpus.\n",
      "     |  \n",
      "     |  >>> from nltk.corpus import framenet as fn\n",
      "     |  >>> fn.lu(3238).frame.lexUnit['glint.v'] is fn.lu(3238)\n",
      "     |  True\n",
      "     |  >>> fn.frame_by_name('Replacing') is fn.lus('replace.v')[0].frame\n",
      "     |  True\n",
      "     |  >>> fn.lus('prejudice.n')[0].frame.frameRelations == fn.frame_relations('Partiality')\n",
      "     |  True\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      FramenetCorpusReader\n",
      "     |      nltk.corpus.reader.xmldocs.XMLCorpusReader\n",
      "     |      nltk.corpus.reader.api.CorpusReader\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, root, fileids)\n",
      "     |      :type root: PathPointer or str\n",
      "     |      :param root: A path pointer identifying the root directory for\n",
      "     |          this corpus.  If a string is specified, then it will be\n",
      "     |          converted to a ``PathPointer`` automatically.\n",
      "     |      :param fileids: A list of the files that make up this corpus.\n",
      "     |          This list can either be specified explicitly, as a list of\n",
      "     |          strings; or implicitly, as a regular expression over file\n",
      "     |          paths.  The absolute path for each file will be constructed\n",
      "     |          by joining the reader's root to each file name.\n",
      "     |      :param encoding: The default unicode encoding for the files\n",
      "     |          that make up the corpus.  The value of ``encoding`` can be any\n",
      "     |          of the following:\n",
      "     |          - A string: ``encoding`` is the encoding name for all files.\n",
      "     |          - A dictionary: ``encoding[file_id]`` is the encoding\n",
      "     |            name for the file whose identifier is ``file_id``.  If\n",
      "     |            ``file_id`` is not in ``encoding``, then the file\n",
      "     |            contents will be processed using non-unicode byte strings.\n",
      "     |          - A list: ``encoding`` should be a list of ``(regexp, encoding)``\n",
      "     |            tuples.  The encoding for a file whose identifier is ``file_id``\n",
      "     |            will be the ``encoding`` value for the first tuple whose\n",
      "     |            ``regexp`` matches the ``file_id``.  If no tuple's ``regexp``\n",
      "     |            matches the ``file_id``, the file contents will be processed\n",
      "     |            using non-unicode byte strings.\n",
      "     |          - None: the file contents of all files will be\n",
      "     |            processed using non-unicode byte strings.\n",
      "     |      :param tagset: The name of the tagset used by this corpus, to be used\n",
      "     |            for normalizing or converting the POS tags returned by the\n",
      "     |            tagged_...() methods.\n",
      "     |  \n",
      "     |  annotations(self, luNamePattern=None, exemplars=True, full_text=True)\n",
      "     |      Frame annotation sets matching the specified criteria.\n",
      "     |  \n",
      "     |  buildindexes(self)\n",
      "     |      Build the internal indexes to make look-ups faster.\n",
      "     |  \n",
      "     |  doc(self, fn_docid)\n",
      "     |      Returns the annotated document whose id number is\n",
      "     |      ``fn_docid``. This id number can be obtained by calling the\n",
      "     |      Documents() function.\n",
      "     |      \n",
      "     |      The dict that is returned from this function will contain the\n",
      "     |      following keys:\n",
      "     |      \n",
      "     |      - '_type'      : 'fulltextannotation'\n",
      "     |      - 'sentence'   : a list of sentences in the document\n",
      "     |         - Each item in the list is a dict containing the following keys:\n",
      "     |            - 'ID'    : the ID number of the sentence\n",
      "     |            - '_type' : 'sentence'\n",
      "     |            - 'text'  : the text of the sentence\n",
      "     |            - 'paragNo' : the paragraph number\n",
      "     |            - 'sentNo'  : the sentence number\n",
      "     |            - 'docID'   : the document ID number\n",
      "     |            - 'corpID'  : the corpus ID number\n",
      "     |            - 'aPos'    : the annotation position\n",
      "     |            - 'annotationSet' : a list of annotation layers for the sentence\n",
      "     |               - Each item in the list is a dict containing the following keys:\n",
      "     |                  - 'ID'       : the ID number of the annotation set\n",
      "     |                  - '_type'    : 'annotationset'\n",
      "     |                  - 'status'   : either 'MANUAL' or 'UNANN'\n",
      "     |                  - 'luName'   : (only if status is 'MANUAL')\n",
      "     |                  - 'luID'     : (only if status is 'MANUAL')\n",
      "     |                  - 'frameID'  : (only if status is 'MANUAL')\n",
      "     |                  - 'frameName': (only if status is 'MANUAL')\n",
      "     |                  - 'layer' : a list of labels for the layer\n",
      "     |                     - Each item in the layer is a dict containing the\n",
      "     |                       following keys:\n",
      "     |                        - '_type': 'layer'\n",
      "     |                        - 'rank'\n",
      "     |                        - 'name'\n",
      "     |                        - 'label' : a list of labels in the layer\n",
      "     |                           - Each item is a dict containing the following keys:\n",
      "     |                              - 'start'\n",
      "     |                              - 'end'\n",
      "     |                              - 'name'\n",
      "     |                              - 'feID' (optional)\n",
      "     |      \n",
      "     |      :param fn_docid: The Framenet id number of the document\n",
      "     |      :type fn_docid: int\n",
      "     |      :return: Information about the annotated document\n",
      "     |      :rtype: dict\n",
      "     |  \n",
      "     |  docs(self, name=None)\n",
      "     |      Return a list of the annotated full-text documents in FrameNet,\n",
      "     |      optionally filtered by a regex to be matched against the document name.\n",
      "     |  \n",
      "     |  docs_metadata(self, name=None)\n",
      "     |      Return an index of the annotated documents in Framenet.\n",
      "     |      \n",
      "     |      Details for a specific annotated document can be obtained using this\n",
      "     |      class's doc() function and pass it the value of the 'ID' field.\n",
      "     |      \n",
      "     |      >>> from nltk.corpus import framenet as fn\n",
      "     |      >>> len(fn.docs()) in (78, 107) # FN 1.5 and 1.7, resp.\n",
      "     |      True\n",
      "     |      >>> set([x.corpname for x in fn.docs_metadata()])>=set(['ANC', 'KBEval',                     'LUCorpus-v0.3', 'Miscellaneous', 'NTI', 'PropBank'])\n",
      "     |      True\n",
      "     |      \n",
      "     |      :param name: A regular expression pattern used to search the\n",
      "     |          file name of each annotated document. The document's\n",
      "     |          file name contains the name of the corpus that the\n",
      "     |          document is from, followed by two underscores \"__\"\n",
      "     |          followed by the document name. So, for example, the\n",
      "     |          file name \"LUCorpus-v0.3__20000410_nyt-NEW.xml\" is\n",
      "     |          from the corpus named \"LUCorpus-v0.3\" and the\n",
      "     |          document name is \"20000410_nyt-NEW.xml\".\n",
      "     |      :type name: str\n",
      "     |      :return: A list of selected (or all) annotated documents\n",
      "     |      :rtype: list of dicts, where each dict object contains the following\n",
      "     |              keys:\n",
      "     |      \n",
      "     |              - 'name'\n",
      "     |              - 'ID'\n",
      "     |              - 'corpid'\n",
      "     |              - 'corpname'\n",
      "     |              - 'description'\n",
      "     |              - 'filename'\n",
      "     |  \n",
      "     |  exemplars(self, luNamePattern=None, frame=None, fe=None, fe2=None)\n",
      "     |      Lexicographic exemplar sentences, optionally filtered by LU name and/or 1-2 FEs that\n",
      "     |      are realized overtly. 'frame' may be a name pattern, frame ID, or frame instance.\n",
      "     |      'fe' may be a name pattern or FE instance; if specified, 'fe2' may also\n",
      "     |      be specified to retrieve sentences with both overt FEs (in either order).\n",
      "     |  \n",
      "     |  fe_relations(self)\n",
      "     |      Obtain a list of frame element relations.\n",
      "     |      \n",
      "     |      >>> from nltk.corpus import framenet as fn\n",
      "     |      >>> ferels = fn.fe_relations()\n",
      "     |      >>> isinstance(ferels, list)\n",
      "     |      True\n",
      "     |      >>> len(ferels) in (10020, 12393)   # FN 1.5 and 1.7, resp.\n",
      "     |      True\n",
      "     |      >>> PrettyDict(ferels[0], breakLines=True)\n",
      "     |      {'ID': 14642,\n",
      "     |      '_type': 'ferelation',\n",
      "     |      'frameRelation': <Parent=Abounding_with -- Inheritance -> Child=Lively_place>,\n",
      "     |      'subFE': <fe ID=11370 name=Degree>,\n",
      "     |      'subFEName': 'Degree',\n",
      "     |      'subFrame': <frame ID=1904 name=Lively_place>,\n",
      "     |      'subID': 11370,\n",
      "     |      'supID': 2271,\n",
      "     |      'superFE': <fe ID=2271 name=Degree>,\n",
      "     |      'superFEName': 'Degree',\n",
      "     |      'superFrame': <frame ID=262 name=Abounding_with>,\n",
      "     |      'type': <framerelationtype ID=1 name=Inheritance>}\n",
      "     |      \n",
      "     |      :return: A list of all of the frame element relations in framenet\n",
      "     |      :rtype: list(dict)\n",
      "     |  \n",
      "     |  fes(self, name=None, frame=None)\n",
      "     |      Lists frame element objects. If 'name' is provided, this is treated as\n",
      "     |      a case-insensitive regular expression to filter by frame name.\n",
      "     |      (Case-insensitivity is because casing of frame element names is not always\n",
      "     |      consistent across frames.) Specify 'frame' to filter by a frame name pattern,\n",
      "     |      ID, or object.\n",
      "     |      \n",
      "     |      >>> from nltk.corpus import framenet as fn\n",
      "     |      >>> fn.fes('Noise_maker')\n",
      "     |      [<fe ID=6043 name=Noise_maker>]\n",
      "     |      >>> sorted([(fe.frame.name,fe.name) for fe in fn.fes('sound')])\n",
      "     |      [('Cause_to_make_noise', 'Sound_maker'), ('Make_noise', 'Sound'),\n",
      "     |       ('Make_noise', 'Sound_source'), ('Sound_movement', 'Location_of_sound_source'),\n",
      "     |       ('Sound_movement', 'Sound'), ('Sound_movement', 'Sound_source'),\n",
      "     |       ('Sounds', 'Component_sound'), ('Sounds', 'Location_of_sound_source'),\n",
      "     |       ('Sounds', 'Sound_source'), ('Vocalizations', 'Location_of_sound_source'),\n",
      "     |       ('Vocalizations', 'Sound_source')]\n",
      "     |      >>> sorted([(fe.frame.name,fe.name) for fe in fn.fes('sound',r'(?i)make_noise')])\n",
      "     |      [('Cause_to_make_noise', 'Sound_maker'),\n",
      "     |       ('Make_noise', 'Sound'),\n",
      "     |       ('Make_noise', 'Sound_source')]\n",
      "     |      >>> sorted(set(fe.name for fe in fn.fes('^sound')))\n",
      "     |      ['Sound', 'Sound_maker', 'Sound_source']\n",
      "     |      >>> len(fn.fes('^sound$'))\n",
      "     |      2\n",
      "     |      \n",
      "     |      :param name: A regular expression pattern used to match against\n",
      "     |          frame element names. If 'name' is None, then a list of all\n",
      "     |          frame elements will be returned.\n",
      "     |      :type name: str\n",
      "     |      :return: A list of matching frame elements\n",
      "     |      :rtype: list(AttrDict)\n",
      "     |  \n",
      "     |  frame(self, fn_fid_or_fname, ignorekeys=[])\n",
      "     |      Get the details for the specified Frame using the frame's name\n",
      "     |      or id number.\n",
      "     |      \n",
      "     |      Usage examples:\n",
      "     |      \n",
      "     |      >>> from nltk.corpus import framenet as fn\n",
      "     |      >>> f = fn.frame(256)\n",
      "     |      >>> f.name\n",
      "     |      'Medical_specialties'\n",
      "     |      >>> f = fn.frame('Medical_specialties')\n",
      "     |      >>> f.ID\n",
      "     |      256\n",
      "     |      >>> # ensure non-ASCII character in definition doesn't trigger an encoding error:\n",
      "     |      >>> fn.frame('Imposing_obligation')\n",
      "     |      frame (1494): Imposing_obligation...\n",
      "     |      \n",
      "     |      The dict that is returned from this function will contain the\n",
      "     |      following information about the Frame:\n",
      "     |      \n",
      "     |      - 'name'       : the name of the Frame (e.g. 'Birth', 'Apply_heat', etc.)\n",
      "     |      - 'definition' : textual definition of the Frame\n",
      "     |      - 'ID'         : the internal ID number of the Frame\n",
      "     |      - 'semTypes'   : a list of semantic types for this frame\n",
      "     |         - Each item in the list is a dict containing the following keys:\n",
      "     |            - 'name' : can be used with the semtype() function\n",
      "     |            - 'ID'   : can be used with the semtype() function\n",
      "     |      \n",
      "     |      - 'lexUnit'    : a dict containing all of the LUs for this frame.\n",
      "     |                       The keys in this dict are the names of the LUs and\n",
      "     |                       the value for each key is itself a dict containing\n",
      "     |                       info about the LU (see the lu() function for more info.)\n",
      "     |      \n",
      "     |      - 'FE' : a dict containing the Frame Elements that are part of this frame\n",
      "     |               The keys in this dict are the names of the FEs (e.g. 'Body_system')\n",
      "     |               and the values are dicts containing the following keys\n",
      "     |            - 'definition' : The definition of the FE\n",
      "     |            - 'name'       : The name of the FE e.g. 'Body_system'\n",
      "     |            - 'ID'         : The id number\n",
      "     |            - '_type'      : 'fe'\n",
      "     |            - 'abbrev'     : Abbreviation e.g. 'bod'\n",
      "     |            - 'coreType'   : one of \"Core\", \"Peripheral\", or \"Extra-Thematic\"\n",
      "     |            - 'semType'    : if not None, a dict with the following two keys:\n",
      "     |               - 'name' : name of the semantic type. can be used with\n",
      "     |                          the semtype() function\n",
      "     |               - 'ID'   : id number of the semantic type. can be used with\n",
      "     |                          the semtype() function\n",
      "     |            - 'requiresFE' : if not None, a dict with the following two keys:\n",
      "     |               - 'name' : the name of another FE in this frame\n",
      "     |               - 'ID'   : the id of the other FE in this frame\n",
      "     |            - 'excludesFE' : if not None, a dict with the following two keys:\n",
      "     |               - 'name' : the name of another FE in this frame\n",
      "     |               - 'ID'   : the id of the other FE in this frame\n",
      "     |      \n",
      "     |      - 'frameRelation'      : a list of objects describing frame relations\n",
      "     |      - 'FEcoreSets'  : a list of Frame Element core sets for this frame\n",
      "     |         - Each item in the list is a list of FE objects\n",
      "     |      \n",
      "     |      :param fn_fid_or_fname: The Framenet name or id number of the frame\n",
      "     |      :type fn_fid_or_fname: int or str\n",
      "     |      :param ignorekeys: The keys to ignore. These keys will not be\n",
      "     |          included in the output. (optional)\n",
      "     |      :type ignorekeys: list(str)\n",
      "     |      :return: Information about a frame\n",
      "     |      :rtype: dict\n",
      "     |  \n",
      "     |  frame_by_id(self, fn_fid, ignorekeys=[])\n",
      "     |      Get the details for the specified Frame using the frame's id\n",
      "     |      number.\n",
      "     |      \n",
      "     |      Usage examples:\n",
      "     |      \n",
      "     |      >>> from nltk.corpus import framenet as fn\n",
      "     |      >>> f = fn.frame_by_id(256)\n",
      "     |      >>> f.ID\n",
      "     |      256\n",
      "     |      >>> f.name\n",
      "     |      'Medical_specialties'\n",
      "     |      >>> f.definition\n",
      "     |      \"This frame includes words that name ...\"\n",
      "     |      \n",
      "     |      :param fn_fid: The Framenet id number of the frame\n",
      "     |      :type fn_fid: int\n",
      "     |      :param ignorekeys: The keys to ignore. These keys will not be\n",
      "     |          included in the output. (optional)\n",
      "     |      :type ignorekeys: list(str)\n",
      "     |      :return: Information about a frame\n",
      "     |      :rtype: dict\n",
      "     |      \n",
      "     |      Also see the ``frame()`` function for details about what is\n",
      "     |      contained in the dict that is returned.\n",
      "     |  \n",
      "     |  frame_by_name(self, fn_fname, ignorekeys=[], check_cache=True)\n",
      "     |      Get the details for the specified Frame using the frame's name.\n",
      "     |      \n",
      "     |      Usage examples:\n",
      "     |      \n",
      "     |      >>> from nltk.corpus import framenet as fn\n",
      "     |      >>> f = fn.frame_by_name('Medical_specialties')\n",
      "     |      >>> f.ID\n",
      "     |      256\n",
      "     |      >>> f.name\n",
      "     |      'Medical_specialties'\n",
      "     |      >>> f.definition\n",
      "     |      \"This frame includes words that name ...\"\n",
      "     |      \n",
      "     |      :param fn_fname: The name of the frame\n",
      "     |      :type fn_fname: str\n",
      "     |      :param ignorekeys: The keys to ignore. These keys will not be\n",
      "     |          included in the output. (optional)\n",
      "     |      :type ignorekeys: list(str)\n",
      "     |      :return: Information about a frame\n",
      "     |      :rtype: dict\n",
      "     |      \n",
      "     |      Also see the ``frame()`` function for details about what is\n",
      "     |      contained in the dict that is returned.\n",
      "     |  \n",
      "     |  frame_ids_and_names(self, name=None)\n",
      "     |      Uses the frame index, which is much faster than looking up each frame definition\n",
      "     |      if only the names and IDs are needed.\n",
      "     |  \n",
      "     |  frame_relation_types(self)\n",
      "     |      Obtain a list of frame relation types.\n",
      "     |      \n",
      "     |      >>> from nltk.corpus import framenet as fn\n",
      "     |      >>> frts = sorted(fn.frame_relation_types(), key=itemgetter('ID'))\n",
      "     |      >>> isinstance(frts, list)\n",
      "     |      True\n",
      "     |      >>> len(frts) in (9, 10)    # FN 1.5 and 1.7, resp.\n",
      "     |      True\n",
      "     |      >>> PrettyDict(frts[0], breakLines=True)\n",
      "     |      {'ID': 1,\n",
      "     |       '_type': 'framerelationtype',\n",
      "     |       'frameRelations': [<Parent=Event -- Inheritance -> Child=Change_of_consistency>, <Parent=Event -- Inheritance -> Child=Rotting>, ...],\n",
      "     |       'name': 'Inheritance',\n",
      "     |       'subFrameName': 'Child',\n",
      "     |       'superFrameName': 'Parent'}\n",
      "     |      \n",
      "     |      :return: A list of all of the frame relation types in framenet\n",
      "     |      :rtype: list(dict)\n",
      "     |  \n",
      "     |  frame_relations(self, frame=None, frame2=None, type=None)\n",
      "     |      :param frame: (optional) frame object, name, or ID; only relations involving\n",
      "     |      this frame will be returned\n",
      "     |      :param frame2: (optional; 'frame' must be a different frame) only show relations\n",
      "     |      between the two specified frames, in either direction\n",
      "     |      :param type: (optional) frame relation type (name or object); show only relations\n",
      "     |      of this type\n",
      "     |      :type frame: int or str or AttrDict\n",
      "     |      :return: A list of all of the frame relations in framenet\n",
      "     |      :rtype: list(dict)\n",
      "     |      \n",
      "     |      >>> from nltk.corpus import framenet as fn\n",
      "     |      >>> frels = fn.frame_relations()\n",
      "     |      >>> isinstance(frels, list)\n",
      "     |      True\n",
      "     |      >>> len(frels) in (1676, 2070)  # FN 1.5 and 1.7, resp.\n",
      "     |      True\n",
      "     |      >>> PrettyList(fn.frame_relations('Cooking_creation'), maxReprSize=0, breakLines=True)\n",
      "     |      [<Parent=Intentionally_create -- Inheritance -> Child=Cooking_creation>,\n",
      "     |       <Parent=Apply_heat -- Using -> Child=Cooking_creation>,\n",
      "     |       <MainEntry=Apply_heat -- See_also -> ReferringEntry=Cooking_creation>]\n",
      "     |      >>> PrettyList(fn.frame_relations(274), breakLines=True)\n",
      "     |      [<Parent=Avoiding -- Inheritance -> Child=Dodging>,\n",
      "     |       <Parent=Avoiding -- Inheritance -> Child=Evading>, ...]\n",
      "     |      >>> PrettyList(fn.frame_relations(fn.frame('Cooking_creation')), breakLines=True)\n",
      "     |      [<Parent=Intentionally_create -- Inheritance -> Child=Cooking_creation>,\n",
      "     |       <Parent=Apply_heat -- Using -> Child=Cooking_creation>, ...]\n",
      "     |      >>> PrettyList(fn.frame_relations('Cooking_creation', type='Inheritance'))\n",
      "     |      [<Parent=Intentionally_create -- Inheritance -> Child=Cooking_creation>]\n",
      "     |      >>> PrettyList(fn.frame_relations('Cooking_creation', 'Apply_heat'), breakLines=True)\n",
      "     |      [<Parent=Apply_heat -- Using -> Child=Cooking_creation>,\n",
      "     |      <MainEntry=Apply_heat -- See_also -> ReferringEntry=Cooking_creation>]\n",
      "     |  \n",
      "     |  frames(self, name=None)\n",
      "     |      Obtain details for a specific frame.\n",
      "     |      \n",
      "     |      >>> from nltk.corpus import framenet as fn\n",
      "     |      >>> len(fn.frames()) in (1019, 1221)    # FN 1.5 and 1.7, resp.\n",
      "     |      True\n",
      "     |      >>> x = PrettyList(fn.frames(r'(?i)crim'), maxReprSize=0, breakLines=True)\n",
      "     |      >>> x.sort(key=itemgetter('ID'))\n",
      "     |      >>> x\n",
      "     |      [<frame ID=200 name=Criminal_process>,\n",
      "     |       <frame ID=500 name=Criminal_investigation>,\n",
      "     |       <frame ID=692 name=Crime_scenario>,\n",
      "     |       <frame ID=700 name=Committing_crime>]\n",
      "     |      \n",
      "     |      A brief intro to Frames (excerpted from \"FrameNet II: Extended\n",
      "     |      Theory and Practice\" by Ruppenhofer et. al., 2010):\n",
      "     |      \n",
      "     |      A Frame is a script-like conceptual structure that describes a\n",
      "     |      particular type of situation, object, or event along with the\n",
      "     |      participants and props that are needed for that Frame. For\n",
      "     |      example, the \"Apply_heat\" frame describes a common situation\n",
      "     |      involving a Cook, some Food, and a Heating_Instrument, and is\n",
      "     |      evoked by words such as bake, blanch, boil, broil, brown,\n",
      "     |      simmer, steam, etc.\n",
      "     |      \n",
      "     |      We call the roles of a Frame \"frame elements\" (FEs) and the\n",
      "     |      frame-evoking words are called \"lexical units\" (LUs).\n",
      "     |      \n",
      "     |      FrameNet includes relations between Frames. Several types of\n",
      "     |      relations are defined, of which the most important are:\n",
      "     |      \n",
      "     |         - Inheritance: An IS-A relation. The child frame is a subtype\n",
      "     |           of the parent frame, and each FE in the parent is bound to\n",
      "     |           a corresponding FE in the child. An example is the\n",
      "     |           \"Revenge\" frame which inherits from the\n",
      "     |           \"Rewards_and_punishments\" frame.\n",
      "     |      \n",
      "     |         - Using: The child frame presupposes the parent frame as\n",
      "     |           background, e.g the \"Speed\" frame \"uses\" (or presupposes)\n",
      "     |           the \"Motion\" frame; however, not all parent FEs need to be\n",
      "     |           bound to child FEs.\n",
      "     |      \n",
      "     |         - Subframe: The child frame is a subevent of a complex event\n",
      "     |           represented by the parent, e.g. the \"Criminal_process\" frame\n",
      "     |           has subframes of \"Arrest\", \"Arraignment\", \"Trial\", and\n",
      "     |           \"Sentencing\".\n",
      "     |      \n",
      "     |         - Perspective_on: The child frame provides a particular\n",
      "     |           perspective on an un-perspectivized parent frame. A pair of\n",
      "     |           examples consists of the \"Hiring\" and \"Get_a_job\" frames,\n",
      "     |           which perspectivize the \"Employment_start\" frame from the\n",
      "     |           Employer's and the Employee's point of view, respectively.\n",
      "     |      \n",
      "     |      :param name: A regular expression pattern used to match against\n",
      "     |          Frame names. If 'name' is None, then a list of all\n",
      "     |          Framenet Frames will be returned.\n",
      "     |      :type name: str\n",
      "     |      :return: A list of matching Frames (or all Frames).\n",
      "     |      :rtype: list(AttrDict)\n",
      "     |  \n",
      "     |  frames_by_lemma(self, pat)\n",
      "     |      Returns a list of all frames that contain LUs in which the\n",
      "     |      ``name`` attribute of the LU matchs the given regular expression\n",
      "     |      ``pat``. Note that LU names are composed of \"lemma.POS\", where\n",
      "     |      the \"lemma\" part can be made up of either a single lexeme\n",
      "     |      (e.g. 'run') or multiple lexemes (e.g. 'a little').\n",
      "     |      \n",
      "     |      Note: if you are going to be doing a lot of this type of\n",
      "     |      searching, you'd want to build an index that maps from lemmas to\n",
      "     |      frames because each time frames_by_lemma() is called, it has to\n",
      "     |      search through ALL of the frame XML files in the db.\n",
      "     |      \n",
      "     |      >>> from nltk.corpus import framenet as fn\n",
      "     |      >>> from nltk.corpus.reader.framenet import PrettyList\n",
      "     |      >>> PrettyList(sorted(fn.frames_by_lemma(r'(?i)a little'), key=itemgetter('ID'))) # doctest: +ELLIPSIS\n",
      "     |      [<frame ID=189 name=Quanti...>, <frame ID=2001 name=Degree>]\n",
      "     |      \n",
      "     |      :return: A list of frame objects.\n",
      "     |      :rtype: list(AttrDict)\n",
      "     |  \n",
      "     |  ft_sents(self, docNamePattern=None)\n",
      "     |      Full-text annotation sentences, optionally filtered by document name.\n",
      "     |  \n",
      "     |  help(self, attrname=None)\n",
      "     |      Display help information summarizing the main methods.\n",
      "     |  \n",
      "     |  lu(self, fn_luid, ignorekeys=[], luName=None, frameID=None, frameName=None)\n",
      "     |      Access a lexical unit by its ID. luName, frameID, and frameName are used\n",
      "     |      only in the event that the LU does not have a file in the database\n",
      "     |      (which is the case for LUs with \"Problem\" status); in this case,\n",
      "     |      a placeholder LU is created which just contains its name, ID, and frame.\n",
      "     |      \n",
      "     |      \n",
      "     |      Usage examples:\n",
      "     |      \n",
      "     |      >>> from nltk.corpus import framenet as fn\n",
      "     |      >>> fn.lu(256).name\n",
      "     |      'foresee.v'\n",
      "     |      >>> fn.lu(256).definition\n",
      "     |      'COD: be aware of beforehand; predict.'\n",
      "     |      >>> fn.lu(256).frame.name\n",
      "     |      'Expectation'\n",
      "     |      >>> pprint(list(map(PrettyDict, fn.lu(256).lexemes)))\n",
      "     |      [{'POS': 'V', 'breakBefore': 'false', 'headword': 'false', 'name': 'foresee', 'order': 1}]\n",
      "     |      \n",
      "     |      >>> fn.lu(227).exemplars[23]\n",
      "     |      exemplar sentence (352962):\n",
      "     |      [sentNo] 0\n",
      "     |      [aPos] 59699508\n",
      "     |      <BLANKLINE>\n",
      "     |      [LU] (227) guess.v in Coming_to_believe\n",
      "     |      <BLANKLINE>\n",
      "     |      [frame] (23) Coming_to_believe\n",
      "     |      <BLANKLINE>\n",
      "     |      [annotationSet] 2 annotation sets\n",
      "     |      <BLANKLINE>\n",
      "     |      [POS] 18 tags\n",
      "     |      <BLANKLINE>\n",
      "     |      [POS_tagset] BNC\n",
      "     |      <BLANKLINE>\n",
      "     |      [GF] 3 relations\n",
      "     |      <BLANKLINE>\n",
      "     |      [PT] 3 phrases\n",
      "     |      <BLANKLINE>\n",
      "     |      [Other] 1 entry\n",
      "     |      <BLANKLINE>\n",
      "     |      [text] + [Target] + [FE]\n",
      "     |      <BLANKLINE>\n",
      "     |      When he was inside the house , Culley noticed the characteristic\n",
      "     |                                                    ------------------\n",
      "     |                                                    Content\n",
      "     |      <BLANKLINE>\n",
      "     |      he would n't have guessed at .\n",
      "     |      --                ******* --\n",
      "     |      Co                        C1 [Evidence:INI]\n",
      "     |       (Co=Cognizer, C1=Content)\n",
      "     |      <BLANKLINE>\n",
      "     |      <BLANKLINE>\n",
      "     |      \n",
      "     |      The dict that is returned from this function will contain most of the\n",
      "     |      following information about the LU. Note that some LUs do not contain\n",
      "     |      all of these pieces of information - particularly 'totalAnnotated' and\n",
      "     |      'incorporatedFE' may be missing in some LUs:\n",
      "     |      \n",
      "     |      - 'name'       : the name of the LU (e.g. 'merger.n')\n",
      "     |      - 'definition' : textual definition of the LU\n",
      "     |      - 'ID'         : the internal ID number of the LU\n",
      "     |      - '_type'      : 'lu'\n",
      "     |      - 'status'     : e.g. 'Created'\n",
      "     |      - 'frame'      : Frame that this LU belongs to\n",
      "     |      - 'POS'        : the part of speech of this LU (e.g. 'N')\n",
      "     |      - 'totalAnnotated' : total number of examples annotated with this LU\n",
      "     |      - 'incorporatedFE' : FE that incorporates this LU (e.g. 'Ailment')\n",
      "     |      - 'sentenceCount'  : a dict with the following two keys:\n",
      "     |               - 'annotated': number of sentences annotated with this LU\n",
      "     |               - 'total'    : total number of sentences with this LU\n",
      "     |      \n",
      "     |      - 'lexemes'  : a list of dicts describing the lemma of this LU.\n",
      "     |         Each dict in the list contains these keys:\n",
      "     |         - 'POS'     : part of speech e.g. 'N'\n",
      "     |         - 'name'    : either single-lexeme e.g. 'merger' or\n",
      "     |                       multi-lexeme e.g. 'a little'\n",
      "     |         - 'order': the order of the lexeme in the lemma (starting from 1)\n",
      "     |         - 'headword': a boolean ('true' or 'false')\n",
      "     |         - 'breakBefore': Can this lexeme be separated from the previous lexeme?\n",
      "     |              Consider: \"take over.v\" as in:\n",
      "     |                       Germany took over the Netherlands in 2 days.\n",
      "     |                       Germany took the Netherlands over in 2 days.\n",
      "     |              In this case, 'breakBefore' would be \"true\" for the lexeme\n",
      "     |              \"over\". Contrast this with \"take after.v\" as in:\n",
      "     |                       Mary takes after her grandmother.\n",
      "     |                      *Mary takes her grandmother after.\n",
      "     |              In this case, 'breakBefore' would be \"false\" for the lexeme \"after\"\n",
      "     |      \n",
      "     |      - 'lemmaID'    : Can be used to connect lemmas in different LUs\n",
      "     |      - 'semTypes'   : a list of semantic type objects for this LU\n",
      "     |      - 'subCorpus'  : a list of subcorpora\n",
      "     |         - Each item in the list is a dict containing the following keys:\n",
      "     |            - 'name' :\n",
      "     |            - 'sentence' : a list of sentences in the subcorpus\n",
      "     |               - each item in the list is a dict with the following keys:\n",
      "     |                  - 'ID':\n",
      "     |                  - 'sentNo':\n",
      "     |                  - 'text': the text of the sentence\n",
      "     |                  - 'aPos':\n",
      "     |                  - 'annotationSet': a list of annotation sets\n",
      "     |                     - each item in the list is a dict with the following keys:\n",
      "     |                        - 'ID':\n",
      "     |                        - 'status':\n",
      "     |                        - 'layer': a list of layers\n",
      "     |                           - each layer is a dict containing the following keys:\n",
      "     |                              - 'name': layer name (e.g. 'BNC')\n",
      "     |                              - 'rank':\n",
      "     |                              - 'label': a list of labels for the layer\n",
      "     |                                 - each label is a dict containing the following keys:\n",
      "     |                                    - 'start': start pos of label in sentence 'text' (0-based)\n",
      "     |                                    - 'end': end pos of label in sentence 'text' (0-based)\n",
      "     |                                    - 'name': name of label (e.g. 'NN1')\n",
      "     |      \n",
      "     |      Under the hood, this implementation looks up the lexical unit information\n",
      "     |      in the *frame* definition file. That file does not contain\n",
      "     |      corpus annotations, so the LU files will be accessed on demand if those are\n",
      "     |      needed. In principle, valence patterns could be loaded here too,\n",
      "     |      though these are not currently supported.\n",
      "     |      \n",
      "     |      :param fn_luid: The id number of the lexical unit\n",
      "     |      :type fn_luid: int\n",
      "     |      :param ignorekeys: The keys to ignore. These keys will not be\n",
      "     |          included in the output. (optional)\n",
      "     |      :type ignorekeys: list(str)\n",
      "     |      :return: All information about the lexical unit\n",
      "     |      :rtype: dict\n",
      "     |  \n",
      "     |  lu_basic(self, fn_luid)\n",
      "     |      Returns basic information about the LU whose id is\n",
      "     |      ``fn_luid``. This is basically just a wrapper around the\n",
      "     |      ``lu()`` function with \"subCorpus\" info excluded.\n",
      "     |      \n",
      "     |      >>> from nltk.corpus import framenet as fn\n",
      "     |      >>> lu = PrettyDict(fn.lu_basic(256), breakLines=True)\n",
      "     |      >>> # ellipses account for differences between FN 1.5 and 1.7\n",
      "     |      >>> lu # doctest: +ELLIPSIS\n",
      "     |      {'ID': 256,\n",
      "     |       'POS': 'V',\n",
      "     |       'URL': u'https://framenet2.icsi.berkeley.edu/fnReports/data/lu/lu256.xml',\n",
      "     |       '_type': 'lu',\n",
      "     |       'cBy': ...,\n",
      "     |       'cDate': '02/08/2001 01:27:50 PST Thu',\n",
      "     |       'definition': 'COD: be aware of beforehand; predict.',\n",
      "     |       'definitionMarkup': 'COD: be aware of beforehand; predict.',\n",
      "     |       'frame': <frame ID=26 name=Expectation>,\n",
      "     |       'lemmaID': 15082,\n",
      "     |       'lexemes': [{'POS': 'V', 'breakBefore': 'false', 'headword': 'false', 'name': 'foresee', 'order': 1}],\n",
      "     |       'name': 'foresee.v',\n",
      "     |       'semTypes': [],\n",
      "     |       'sentenceCount': {'annotated': ..., 'total': ...},\n",
      "     |       'status': 'FN1_Sent'}\n",
      "     |      \n",
      "     |      :param fn_luid: The id number of the desired LU\n",
      "     |      :type fn_luid: int\n",
      "     |      :return: Basic information about the lexical unit\n",
      "     |      :rtype: dict\n",
      "     |  \n",
      "     |  lu_ids_and_names(self, name=None)\n",
      "     |      Uses the LU index, which is much faster than looking up each LU definition\n",
      "     |      if only the names and IDs are needed.\n",
      "     |  \n",
      "     |  lus(self, name=None, frame=None)\n",
      "     |      Obtain details for lexical units.\n",
      "     |      Optionally restrict by lexical unit name pattern, and/or to a certain frame\n",
      "     |      or frames whose name matches a pattern.\n",
      "     |      \n",
      "     |      >>> from nltk.corpus import framenet as fn\n",
      "     |      >>> len(fn.lus()) in (11829, 13572) # FN 1.5 and 1.7, resp.\n",
      "     |      True\n",
      "     |      >>> PrettyList(sorted(fn.lus(r'(?i)a little'), key=itemgetter('ID')), maxReprSize=0, breakLines=True)\n",
      "     |      [<lu ID=14733 name=a little.n>,\n",
      "     |       <lu ID=14743 name=a little.adv>,\n",
      "     |       <lu ID=14744 name=a little bit.adv>]\n",
      "     |      >>> PrettyList(sorted(fn.lus(r'interest', r'(?i)stimulus'), key=itemgetter('ID')))\n",
      "     |      [<lu ID=14894 name=interested.a>, <lu ID=14920 name=interesting.a>]\n",
      "     |      \n",
      "     |      A brief intro to Lexical Units (excerpted from \"FrameNet II:\n",
      "     |      Extended Theory and Practice\" by Ruppenhofer et. al., 2010):\n",
      "     |      \n",
      "     |      A lexical unit (LU) is a pairing of a word with a meaning. For\n",
      "     |      example, the \"Apply_heat\" Frame describes a common situation\n",
      "     |      involving a Cook, some Food, and a Heating Instrument, and is\n",
      "     |      _evoked_ by words such as bake, blanch, boil, broil, brown,\n",
      "     |      simmer, steam, etc. These frame-evoking words are the LUs in the\n",
      "     |      Apply_heat frame. Each sense of a polysemous word is a different\n",
      "     |      LU.\n",
      "     |      \n",
      "     |      We have used the word \"word\" in talking about LUs. The reality\n",
      "     |      is actually rather complex. When we say that the word \"bake\" is\n",
      "     |      polysemous, we mean that the lemma \"bake.v\" (which has the\n",
      "     |      word-forms \"bake\", \"bakes\", \"baked\", and \"baking\") is linked to\n",
      "     |      three different frames:\n",
      "     |      \n",
      "     |         - Apply_heat: \"Michelle baked the potatoes for 45 minutes.\"\n",
      "     |      \n",
      "     |         - Cooking_creation: \"Michelle baked her mother a cake for her birthday.\"\n",
      "     |      \n",
      "     |         - Absorb_heat: \"The potatoes have to bake for more than 30 minutes.\"\n",
      "     |      \n",
      "     |      These constitute three different LUs, with different\n",
      "     |      definitions.\n",
      "     |      \n",
      "     |      Multiword expressions such as \"given name\" and hyphenated words\n",
      "     |      like \"shut-eye\" can also be LUs. Idiomatic phrases such as\n",
      "     |      \"middle of nowhere\" and \"give the slip (to)\" are also defined as\n",
      "     |      LUs in the appropriate frames (\"Isolated_places\" and \"Evading\",\n",
      "     |      respectively), and their internal structure is not analyzed.\n",
      "     |      \n",
      "     |      Framenet provides multiple annotated examples of each sense of a\n",
      "     |      word (i.e. each LU).  Moreover, the set of examples\n",
      "     |      (approximately 20 per LU) illustrates all of the combinatorial\n",
      "     |      possibilities of the lexical unit.\n",
      "     |      \n",
      "     |      Each LU is linked to a Frame, and hence to the other words which\n",
      "     |      evoke that Frame. This makes the FrameNet database similar to a\n",
      "     |      thesaurus, grouping together semantically similar words.\n",
      "     |      \n",
      "     |      In the simplest case, frame-evoking words are verbs such as\n",
      "     |      \"fried\" in:\n",
      "     |      \n",
      "     |         \"Matilde fried the catfish in a heavy iron skillet.\"\n",
      "     |      \n",
      "     |      Sometimes event nouns may evoke a Frame. For example,\n",
      "     |      \"reduction\" evokes \"Cause_change_of_scalar_position\" in:\n",
      "     |      \n",
      "     |         \"...the reduction of debt levels to $665 million from $2.6 billion.\"\n",
      "     |      \n",
      "     |      Adjectives may also evoke a Frame. For example, \"asleep\" may\n",
      "     |      evoke the \"Sleep\" frame as in:\n",
      "     |      \n",
      "     |         \"They were asleep for hours.\"\n",
      "     |      \n",
      "     |      Many common nouns, such as artifacts like \"hat\" or \"tower\",\n",
      "     |      typically serve as dependents rather than clearly evoking their\n",
      "     |      own frames.\n",
      "     |      \n",
      "     |      :param name: A regular expression pattern used to search the LU\n",
      "     |          names. Note that LU names take the form of a dotted\n",
      "     |          string (e.g. \"run.v\" or \"a little.adv\") in which a\n",
      "     |          lemma preceeds the \".\" and a POS follows the\n",
      "     |          dot. The lemma may be composed of a single lexeme\n",
      "     |          (e.g. \"run\") or of multiple lexemes (e.g. \"a\n",
      "     |          little\"). If 'name' is not given, then all LUs will\n",
      "     |          be returned.\n",
      "     |      \n",
      "     |          The valid POSes are:\n",
      "     |      \n",
      "     |                 v    - verb\n",
      "     |                 n    - noun\n",
      "     |                 a    - adjective\n",
      "     |                 adv  - adverb\n",
      "     |                 prep - preposition\n",
      "     |                 num  - numbers\n",
      "     |                 intj - interjection\n",
      "     |                 art  - article\n",
      "     |                 c    - conjunction\n",
      "     |                 scon - subordinating conjunction\n",
      "     |      \n",
      "     |      :type name: str\n",
      "     |      :type frame: str or int or frame\n",
      "     |      :return: A list of selected (or all) lexical units\n",
      "     |      :rtype: list of LU objects (dicts). See the lu() function for info\n",
      "     |        about the specifics of LU objects.\n",
      "     |  \n",
      "     |  propagate_semtypes(self)\n",
      "     |      Apply inference rules to distribute semtypes over relations between FEs.\n",
      "     |      For FrameNet 1.5, this results in 1011 semtypes being propagated.\n",
      "     |      (Not done by default because it requires loading all frame files,\n",
      "     |      which takes several seconds. If this needed to be fast, it could be rewritten\n",
      "     |      to traverse the neighboring relations on demand for each FE semtype.)\n",
      "     |      \n",
      "     |      >>> from nltk.corpus import framenet as fn\n",
      "     |      >>> x = sum(1 for f in fn.frames() for fe in f.FE.values() if fe.semType)\n",
      "     |      >>> fn.propagate_semtypes()\n",
      "     |      >>> y = sum(1 for f in fn.frames() for fe in f.FE.values() if fe.semType)\n",
      "     |      >>> y-x > 1000\n",
      "     |      True\n",
      "     |  \n",
      "     |  readme(self)\n",
      "     |      Return the contents of the corpus README.txt (or README) file.\n",
      "     |  \n",
      "     |  semtype(self, key)\n",
      "     |      >>> from nltk.corpus import framenet as fn\n",
      "     |      >>> fn.semtype(233).name\n",
      "     |      'Temperature'\n",
      "     |      >>> fn.semtype(233).abbrev\n",
      "     |      'Temp'\n",
      "     |      >>> fn.semtype('Temperature').ID\n",
      "     |      233\n",
      "     |      \n",
      "     |      :param key: The name, abbreviation, or id number of the semantic type\n",
      "     |      :type key: string or int\n",
      "     |      :return: Information about a semantic type\n",
      "     |      :rtype: dict\n",
      "     |  \n",
      "     |  semtype_inherits(self, st, superST)\n",
      "     |  \n",
      "     |  semtypes(self)\n",
      "     |      Obtain a list of semantic types.\n",
      "     |      \n",
      "     |      >>> from nltk.corpus import framenet as fn\n",
      "     |      >>> stypes = fn.semtypes()\n",
      "     |      >>> len(stypes) in (73, 109) # FN 1.5 and 1.7, resp.\n",
      "     |      True\n",
      "     |      >>> sorted(stypes[0].keys())\n",
      "     |      ['ID', '_type', 'abbrev', 'definition', 'definitionMarkup', 'name', 'rootType', 'subTypes', 'superType']\n",
      "     |      \n",
      "     |      :return: A list of all of the semantic types in framenet\n",
      "     |      :rtype: list(dict)\n",
      "     |  \n",
      "     |  sents(self, exemplars=True, full_text=True)\n",
      "     |      Annotated sentences matching the specified criteria.\n",
      "     |  \n",
      "     |  warnings(self, v)\n",
      "     |      Enable or disable warnings of data integrity issues as they are encountered.\n",
      "     |      If v is truthy, warnings will be enabled.\n",
      "     |      \n",
      "     |      (This is a function rather than just an attribute/property to ensure that if\n",
      "     |      enabling warnings is the first action taken, the corpus reader is instantiated first.)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from nltk.corpus.reader.xmldocs.XMLCorpusReader:\n",
      "     |  \n",
      "     |  raw(self, fileids=None)\n",
      "     |  \n",
      "     |  words(self, fileid=None)\n",
      "     |      Returns all of the words and punctuation symbols in the specified file\n",
      "     |      that were in text nodes -- ie, tags are ignored. Like the xml() method,\n",
      "     |      fileid can only specify one file.\n",
      "     |      \n",
      "     |      :return: the given file's text nodes as a list of words and punctuation symbols\n",
      "     |      :rtype: list(str)\n",
      "     |  \n",
      "     |  xml(self, fileid=None)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __unicode__ = __str__(self, /)\n",
      "     |  \n",
      "     |  abspath(self, fileid)\n",
      "     |      Return the absolute path for the given file.\n",
      "     |      \n",
      "     |      :type fileid: str\n",
      "     |      :param fileid: The file identifier for the file whose path\n",
      "     |          should be returned.\n",
      "     |      :rtype: PathPointer\n",
      "     |  \n",
      "     |  abspaths(self, fileids=None, include_encoding=False, include_fileid=False)\n",
      "     |      Return a list of the absolute paths for all fileids in this corpus;\n",
      "     |      or for the given list of fileids, if specified.\n",
      "     |      \n",
      "     |      :type fileids: None or str or list\n",
      "     |      :param fileids: Specifies the set of fileids for which paths should\n",
      "     |          be returned.  Can be None, for all fileids; a list of\n",
      "     |          file identifiers, for a specified set of fileids; or a single\n",
      "     |          file identifier, for a single file.  Note that the return\n",
      "     |          value is always a list of paths, even if ``fileids`` is a\n",
      "     |          single file identifier.\n",
      "     |      \n",
      "     |      :param include_encoding: If true, then return a list of\n",
      "     |          ``(path_pointer, encoding)`` tuples.\n",
      "     |      \n",
      "     |      :rtype: list(PathPointer)\n",
      "     |  \n",
      "     |  citation(self)\n",
      "     |      Return the contents of the corpus citation.bib file, if it exists.\n",
      "     |  \n",
      "     |  encoding(self, file)\n",
      "     |      Return the unicode encoding for the given corpus file, if known.\n",
      "     |      If the encoding is unknown, or if the given file should be\n",
      "     |      processed using byte strings (str), then return None.\n",
      "     |  \n",
      "     |  ensure_loaded(self)\n",
      "     |      Load this corpus (if it has not already been loaded).  This is\n",
      "     |      used by LazyCorpusLoader as a simple method that can be used to\n",
      "     |      make sure a corpus is loaded -- e.g., in case a user wants to\n",
      "     |      do help(some_corpus).\n",
      "     |  \n",
      "     |  fileids(self)\n",
      "     |      Return a list of file identifiers for the fileids that make up\n",
      "     |      this corpus.\n",
      "     |  \n",
      "     |  license(self)\n",
      "     |      Return the contents of the corpus LICENSE file, if it exists.\n",
      "     |  \n",
      "     |  open(self, file)\n",
      "     |      Return an open stream that can be used to read the given file.\n",
      "     |      If the file's encoding is not None, then the stream will\n",
      "     |      automatically decode the file's contents into unicode.\n",
      "     |      \n",
      "     |      :param file: The file identifier of the file to read.\n",
      "     |  \n",
      "     |  unicode_repr = __repr__(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  root\n",
      "     |      The directory where this corpus is stored.\n",
      "     |      \n",
      "     |      :type: PathPointer\n",
      "    \n",
      "    class IEERCorpusReader(nltk.corpus.reader.api.CorpusReader)\n",
      "     |  IEERCorpusReader(root, fileids, encoding='utf8', tagset=None)\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      IEERCorpusReader\n",
      "     |      nltk.corpus.reader.api.CorpusReader\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  docs(self, fileids=None)\n",
      "     |  \n",
      "     |  parsed_docs(self, fileids=None)\n",
      "     |  \n",
      "     |  raw(self, fileids=None)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __init__(self, root, fileids, encoding='utf8', tagset=None)\n",
      "     |      :type root: PathPointer or str\n",
      "     |      :param root: A path pointer identifying the root directory for\n",
      "     |          this corpus.  If a string is specified, then it will be\n",
      "     |          converted to a ``PathPointer`` automatically.\n",
      "     |      :param fileids: A list of the files that make up this corpus.\n",
      "     |          This list can either be specified explicitly, as a list of\n",
      "     |          strings; or implicitly, as a regular expression over file\n",
      "     |          paths.  The absolute path for each file will be constructed\n",
      "     |          by joining the reader's root to each file name.\n",
      "     |      :param encoding: The default unicode encoding for the files\n",
      "     |          that make up the corpus.  The value of ``encoding`` can be any\n",
      "     |          of the following:\n",
      "     |          - A string: ``encoding`` is the encoding name for all files.\n",
      "     |          - A dictionary: ``encoding[file_id]`` is the encoding\n",
      "     |            name for the file whose identifier is ``file_id``.  If\n",
      "     |            ``file_id`` is not in ``encoding``, then the file\n",
      "     |            contents will be processed using non-unicode byte strings.\n",
      "     |          - A list: ``encoding`` should be a list of ``(regexp, encoding)``\n",
      "     |            tuples.  The encoding for a file whose identifier is ``file_id``\n",
      "     |            will be the ``encoding`` value for the first tuple whose\n",
      "     |            ``regexp`` matches the ``file_id``.  If no tuple's ``regexp``\n",
      "     |            matches the ``file_id``, the file contents will be processed\n",
      "     |            using non-unicode byte strings.\n",
      "     |          - None: the file contents of all files will be\n",
      "     |            processed using non-unicode byte strings.\n",
      "     |      :param tagset: The name of the tagset used by this corpus, to be used\n",
      "     |            for normalizing or converting the POS tags returned by the\n",
      "     |            tagged_...() methods.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __unicode__ = __str__(self, /)\n",
      "     |  \n",
      "     |  abspath(self, fileid)\n",
      "     |      Return the absolute path for the given file.\n",
      "     |      \n",
      "     |      :type fileid: str\n",
      "     |      :param fileid: The file identifier for the file whose path\n",
      "     |          should be returned.\n",
      "     |      :rtype: PathPointer\n",
      "     |  \n",
      "     |  abspaths(self, fileids=None, include_encoding=False, include_fileid=False)\n",
      "     |      Return a list of the absolute paths for all fileids in this corpus;\n",
      "     |      or for the given list of fileids, if specified.\n",
      "     |      \n",
      "     |      :type fileids: None or str or list\n",
      "     |      :param fileids: Specifies the set of fileids for which paths should\n",
      "     |          be returned.  Can be None, for all fileids; a list of\n",
      "     |          file identifiers, for a specified set of fileids; or a single\n",
      "     |          file identifier, for a single file.  Note that the return\n",
      "     |          value is always a list of paths, even if ``fileids`` is a\n",
      "     |          single file identifier.\n",
      "     |      \n",
      "     |      :param include_encoding: If true, then return a list of\n",
      "     |          ``(path_pointer, encoding)`` tuples.\n",
      "     |      \n",
      "     |      :rtype: list(PathPointer)\n",
      "     |  \n",
      "     |  citation(self)\n",
      "     |      Return the contents of the corpus citation.bib file, if it exists.\n",
      "     |  \n",
      "     |  encoding(self, file)\n",
      "     |      Return the unicode encoding for the given corpus file, if known.\n",
      "     |      If the encoding is unknown, or if the given file should be\n",
      "     |      processed using byte strings (str), then return None.\n",
      "     |  \n",
      "     |  ensure_loaded(self)\n",
      "     |      Load this corpus (if it has not already been loaded).  This is\n",
      "     |      used by LazyCorpusLoader as a simple method that can be used to\n",
      "     |      make sure a corpus is loaded -- e.g., in case a user wants to\n",
      "     |      do help(some_corpus).\n",
      "     |  \n",
      "     |  fileids(self)\n",
      "     |      Return a list of file identifiers for the fileids that make up\n",
      "     |      this corpus.\n",
      "     |  \n",
      "     |  license(self)\n",
      "     |      Return the contents of the corpus LICENSE file, if it exists.\n",
      "     |  \n",
      "     |  open(self, file)\n",
      "     |      Return an open stream that can be used to read the given file.\n",
      "     |      If the file's encoding is not None, then the stream will\n",
      "     |      automatically decode the file's contents into unicode.\n",
      "     |      \n",
      "     |      :param file: The file identifier of the file to read.\n",
      "     |  \n",
      "     |  readme(self)\n",
      "     |      Return the contents of the corpus README file, if it exists.\n",
      "     |  \n",
      "     |  unicode_repr = __repr__(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  root\n",
      "     |      The directory where this corpus is stored.\n",
      "     |      \n",
      "     |      :type: PathPointer\n",
      "    \n",
      "    class IPIPANCorpusReader(nltk.corpus.reader.api.CorpusReader)\n",
      "     |  IPIPANCorpusReader(root, fileids)\n",
      "     |  \n",
      "     |  Corpus reader designed to work with corpus created by IPI PAN.\n",
      "     |  See http://korpus.pl/en/ for more details about IPI PAN corpus.\n",
      "     |  \n",
      "     |  The corpus includes information about text domain, channel and categories.\n",
      "     |  You can access possible values using ``domains()``, ``channels()`` and\n",
      "     |  ``categories()``. You can use also this metadata to filter files, e.g.:\n",
      "     |  ``fileids(channel='prasa')``, ``fileids(categories='publicystyczny')``.\n",
      "     |  \n",
      "     |  The reader supports methods: words, sents, paras and their tagged versions.\n",
      "     |  You can get part of speech instead of full tag by giving \"simplify_tags=True\"\n",
      "     |  parameter, e.g.: ``tagged_sents(simplify_tags=True)``.\n",
      "     |  \n",
      "     |  Also you can get all tags disambiguated tags specifying parameter\n",
      "     |  \"one_tag=False\", e.g.: ``tagged_paras(one_tag=False)``.\n",
      "     |  \n",
      "     |  You can get all tags that were assigned by a morphological analyzer specifying\n",
      "     |  parameter \"disamb_only=False\", e.g. ``tagged_words(disamb_only=False)``.\n",
      "     |  \n",
      "     |  The IPIPAN Corpus contains tags indicating if there is a space between two\n",
      "     |  tokens. To add special \"no space\" markers, you should specify parameter\n",
      "     |  \"append_no_space=True\", e.g. ``tagged_words(append_no_space=True)``.\n",
      "     |  As a result in place where there should be no space between two tokens new\n",
      "     |  pair ('', 'no-space') will be inserted (for tagged data) and just '' for\n",
      "     |  methods without tags.\n",
      "     |  \n",
      "     |  The corpus reader can also try to append spaces between words. To enable this\n",
      "     |  option, specify parameter \"append_space=True\", e.g. ``words(append_space=True)``.\n",
      "     |  As a result either ' ' or (' ', 'space') will be inserted between tokens.\n",
      "     |  \n",
      "     |  By default, xml entities like &quot; and &amp; are replaced by corresponding\n",
      "     |  characters. You can turn off this feature, specifying parameter\n",
      "     |  \"replace_xmlentities=False\", e.g. ``words(replace_xmlentities=False)``.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      IPIPANCorpusReader\n",
      "     |      nltk.corpus.reader.api.CorpusReader\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, root, fileids)\n",
      "     |      :type root: PathPointer or str\n",
      "     |      :param root: A path pointer identifying the root directory for\n",
      "     |          this corpus.  If a string is specified, then it will be\n",
      "     |          converted to a ``PathPointer`` automatically.\n",
      "     |      :param fileids: A list of the files that make up this corpus.\n",
      "     |          This list can either be specified explicitly, as a list of\n",
      "     |          strings; or implicitly, as a regular expression over file\n",
      "     |          paths.  The absolute path for each file will be constructed\n",
      "     |          by joining the reader's root to each file name.\n",
      "     |      :param encoding: The default unicode encoding for the files\n",
      "     |          that make up the corpus.  The value of ``encoding`` can be any\n",
      "     |          of the following:\n",
      "     |          - A string: ``encoding`` is the encoding name for all files.\n",
      "     |          - A dictionary: ``encoding[file_id]`` is the encoding\n",
      "     |            name for the file whose identifier is ``file_id``.  If\n",
      "     |            ``file_id`` is not in ``encoding``, then the file\n",
      "     |            contents will be processed using non-unicode byte strings.\n",
      "     |          - A list: ``encoding`` should be a list of ``(regexp, encoding)``\n",
      "     |            tuples.  The encoding for a file whose identifier is ``file_id``\n",
      "     |            will be the ``encoding`` value for the first tuple whose\n",
      "     |            ``regexp`` matches the ``file_id``.  If no tuple's ``regexp``\n",
      "     |            matches the ``file_id``, the file contents will be processed\n",
      "     |            using non-unicode byte strings.\n",
      "     |          - None: the file contents of all files will be\n",
      "     |            processed using non-unicode byte strings.\n",
      "     |      :param tagset: The name of the tagset used by this corpus, to be used\n",
      "     |            for normalizing or converting the POS tags returned by the\n",
      "     |            tagged_...() methods.\n",
      "     |  \n",
      "     |  categories(self, fileids=None)\n",
      "     |  \n",
      "     |  channels(self, fileids=None)\n",
      "     |  \n",
      "     |  domains(self, fileids=None)\n",
      "     |  \n",
      "     |  fileids(self, channels=None, domains=None, categories=None)\n",
      "     |      Return a list of file identifiers for the fileids that make up\n",
      "     |      this corpus.\n",
      "     |  \n",
      "     |  paras(self, fileids=None, **kwargs)\n",
      "     |  \n",
      "     |  raw(self, fileids=None)\n",
      "     |  \n",
      "     |  sents(self, fileids=None, **kwargs)\n",
      "     |  \n",
      "     |  tagged_paras(self, fileids=None, **kwargs)\n",
      "     |  \n",
      "     |  tagged_sents(self, fileids=None, **kwargs)\n",
      "     |  \n",
      "     |  tagged_words(self, fileids=None, **kwargs)\n",
      "     |  \n",
      "     |  words(self, fileids=None, **kwargs)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __unicode__ = __str__(self, /)\n",
      "     |  \n",
      "     |  abspath(self, fileid)\n",
      "     |      Return the absolute path for the given file.\n",
      "     |      \n",
      "     |      :type fileid: str\n",
      "     |      :param fileid: The file identifier for the file whose path\n",
      "     |          should be returned.\n",
      "     |      :rtype: PathPointer\n",
      "     |  \n",
      "     |  abspaths(self, fileids=None, include_encoding=False, include_fileid=False)\n",
      "     |      Return a list of the absolute paths for all fileids in this corpus;\n",
      "     |      or for the given list of fileids, if specified.\n",
      "     |      \n",
      "     |      :type fileids: None or str or list\n",
      "     |      :param fileids: Specifies the set of fileids for which paths should\n",
      "     |          be returned.  Can be None, for all fileids; a list of\n",
      "     |          file identifiers, for a specified set of fileids; or a single\n",
      "     |          file identifier, for a single file.  Note that the return\n",
      "     |          value is always a list of paths, even if ``fileids`` is a\n",
      "     |          single file identifier.\n",
      "     |      \n",
      "     |      :param include_encoding: If true, then return a list of\n",
      "     |          ``(path_pointer, encoding)`` tuples.\n",
      "     |      \n",
      "     |      :rtype: list(PathPointer)\n",
      "     |  \n",
      "     |  citation(self)\n",
      "     |      Return the contents of the corpus citation.bib file, if it exists.\n",
      "     |  \n",
      "     |  encoding(self, file)\n",
      "     |      Return the unicode encoding for the given corpus file, if known.\n",
      "     |      If the encoding is unknown, or if the given file should be\n",
      "     |      processed using byte strings (str), then return None.\n",
      "     |  \n",
      "     |  ensure_loaded(self)\n",
      "     |      Load this corpus (if it has not already been loaded).  This is\n",
      "     |      used by LazyCorpusLoader as a simple method that can be used to\n",
      "     |      make sure a corpus is loaded -- e.g., in case a user wants to\n",
      "     |      do help(some_corpus).\n",
      "     |  \n",
      "     |  license(self)\n",
      "     |      Return the contents of the corpus LICENSE file, if it exists.\n",
      "     |  \n",
      "     |  open(self, file)\n",
      "     |      Return an open stream that can be used to read the given file.\n",
      "     |      If the file's encoding is not None, then the stream will\n",
      "     |      automatically decode the file's contents into unicode.\n",
      "     |      \n",
      "     |      :param file: The file identifier of the file to read.\n",
      "     |  \n",
      "     |  readme(self)\n",
      "     |      Return the contents of the corpus README file, if it exists.\n",
      "     |  \n",
      "     |  unicode_repr = __repr__(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  root\n",
      "     |      The directory where this corpus is stored.\n",
      "     |      \n",
      "     |      :type: PathPointer\n",
      "    \n",
      "    class IndianCorpusReader(nltk.corpus.reader.api.CorpusReader)\n",
      "     |  IndianCorpusReader(root, fileids, encoding='utf8', tagset=None)\n",
      "     |  \n",
      "     |  List of words, one per line.  Blank lines are ignored.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      IndianCorpusReader\n",
      "     |      nltk.corpus.reader.api.CorpusReader\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  raw(self, fileids=None)\n",
      "     |  \n",
      "     |  sents(self, fileids=None)\n",
      "     |  \n",
      "     |  tagged_sents(self, fileids=None, tagset=None)\n",
      "     |  \n",
      "     |  tagged_words(self, fileids=None, tagset=None)\n",
      "     |  \n",
      "     |  words(self, fileids=None)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __init__(self, root, fileids, encoding='utf8', tagset=None)\n",
      "     |      :type root: PathPointer or str\n",
      "     |      :param root: A path pointer identifying the root directory for\n",
      "     |          this corpus.  If a string is specified, then it will be\n",
      "     |          converted to a ``PathPointer`` automatically.\n",
      "     |      :param fileids: A list of the files that make up this corpus.\n",
      "     |          This list can either be specified explicitly, as a list of\n",
      "     |          strings; or implicitly, as a regular expression over file\n",
      "     |          paths.  The absolute path for each file will be constructed\n",
      "     |          by joining the reader's root to each file name.\n",
      "     |      :param encoding: The default unicode encoding for the files\n",
      "     |          that make up the corpus.  The value of ``encoding`` can be any\n",
      "     |          of the following:\n",
      "     |          - A string: ``encoding`` is the encoding name for all files.\n",
      "     |          - A dictionary: ``encoding[file_id]`` is the encoding\n",
      "     |            name for the file whose identifier is ``file_id``.  If\n",
      "     |            ``file_id`` is not in ``encoding``, then the file\n",
      "     |            contents will be processed using non-unicode byte strings.\n",
      "     |          - A list: ``encoding`` should be a list of ``(regexp, encoding)``\n",
      "     |            tuples.  The encoding for a file whose identifier is ``file_id``\n",
      "     |            will be the ``encoding`` value for the first tuple whose\n",
      "     |            ``regexp`` matches the ``file_id``.  If no tuple's ``regexp``\n",
      "     |            matches the ``file_id``, the file contents will be processed\n",
      "     |            using non-unicode byte strings.\n",
      "     |          - None: the file contents of all files will be\n",
      "     |            processed using non-unicode byte strings.\n",
      "     |      :param tagset: The name of the tagset used by this corpus, to be used\n",
      "     |            for normalizing or converting the POS tags returned by the\n",
      "     |            tagged_...() methods.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __unicode__ = __str__(self, /)\n",
      "     |  \n",
      "     |  abspath(self, fileid)\n",
      "     |      Return the absolute path for the given file.\n",
      "     |      \n",
      "     |      :type fileid: str\n",
      "     |      :param fileid: The file identifier for the file whose path\n",
      "     |          should be returned.\n",
      "     |      :rtype: PathPointer\n",
      "     |  \n",
      "     |  abspaths(self, fileids=None, include_encoding=False, include_fileid=False)\n",
      "     |      Return a list of the absolute paths for all fileids in this corpus;\n",
      "     |      or for the given list of fileids, if specified.\n",
      "     |      \n",
      "     |      :type fileids: None or str or list\n",
      "     |      :param fileids: Specifies the set of fileids for which paths should\n",
      "     |          be returned.  Can be None, for all fileids; a list of\n",
      "     |          file identifiers, for a specified set of fileids; or a single\n",
      "     |          file identifier, for a single file.  Note that the return\n",
      "     |          value is always a list of paths, even if ``fileids`` is a\n",
      "     |          single file identifier.\n",
      "     |      \n",
      "     |      :param include_encoding: If true, then return a list of\n",
      "     |          ``(path_pointer, encoding)`` tuples.\n",
      "     |      \n",
      "     |      :rtype: list(PathPointer)\n",
      "     |  \n",
      "     |  citation(self)\n",
      "     |      Return the contents of the corpus citation.bib file, if it exists.\n",
      "     |  \n",
      "     |  encoding(self, file)\n",
      "     |      Return the unicode encoding for the given corpus file, if known.\n",
      "     |      If the encoding is unknown, or if the given file should be\n",
      "     |      processed using byte strings (str), then return None.\n",
      "     |  \n",
      "     |  ensure_loaded(self)\n",
      "     |      Load this corpus (if it has not already been loaded).  This is\n",
      "     |      used by LazyCorpusLoader as a simple method that can be used to\n",
      "     |      make sure a corpus is loaded -- e.g., in case a user wants to\n",
      "     |      do help(some_corpus).\n",
      "     |  \n",
      "     |  fileids(self)\n",
      "     |      Return a list of file identifiers for the fileids that make up\n",
      "     |      this corpus.\n",
      "     |  \n",
      "     |  license(self)\n",
      "     |      Return the contents of the corpus LICENSE file, if it exists.\n",
      "     |  \n",
      "     |  open(self, file)\n",
      "     |      Return an open stream that can be used to read the given file.\n",
      "     |      If the file's encoding is not None, then the stream will\n",
      "     |      automatically decode the file's contents into unicode.\n",
      "     |      \n",
      "     |      :param file: The file identifier of the file to read.\n",
      "     |  \n",
      "     |  readme(self)\n",
      "     |      Return the contents of the corpus README file, if it exists.\n",
      "     |  \n",
      "     |  unicode_repr = __repr__(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  root\n",
      "     |      The directory where this corpus is stored.\n",
      "     |      \n",
      "     |      :type: PathPointer\n",
      "    \n",
      "    class KNBCorpusReader(nltk.corpus.reader.api.SyntaxCorpusReader)\n",
      "     |  KNBCorpusReader(root, fileids, encoding='utf8', morphs2str=<function <lambda> at 0x1259f4e60>)\n",
      "     |  \n",
      "     |  This class implements:\n",
      "     |    - ``__init__``, which specifies the location of the corpus\n",
      "     |      and a method for detecting the sentence blocks in corpus files.\n",
      "     |    - ``_read_block``, which reads a block from the input stream.\n",
      "     |    - ``_word``, which takes a block and returns a list of list of words.\n",
      "     |    - ``_tag``, which takes a block and returns a list of list of tagged\n",
      "     |      words.\n",
      "     |    - ``_parse``, which takes a block and returns a list of parsed\n",
      "     |      sentences.\n",
      "     |  \n",
      "     |  The structure of tagged words:\n",
      "     |    tagged_word = (word(str), tags(tuple))\n",
      "     |    tags = (surface, reading, lemma, pos1, posid1, pos2, posid2, pos3, posid3, others ...)\n",
      "     |  \n",
      "     |  Usage example\n",
      "     |  -------------\n",
      "     |  \n",
      "     |  >>> from nltk.corpus.util import LazyCorpusLoader\n",
      "     |  >>> knbc = LazyCorpusLoader(\n",
      "     |  ...     'knbc/corpus1',\n",
      "     |  ...     KNBCorpusReader,\n",
      "     |  ...     r'.*/KN.*',\n",
      "     |  ...     encoding='euc-jp',\n",
      "     |  ... )\n",
      "     |  \n",
      "     |  >>> len(knbc.sents()[0])\n",
      "     |  9\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      KNBCorpusReader\n",
      "     |      nltk.corpus.reader.api.SyntaxCorpusReader\n",
      "     |      nltk.corpus.reader.api.CorpusReader\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, root, fileids, encoding='utf8', morphs2str=<function <lambda> at 0x1259f4e60>)\n",
      "     |      Initialize KNBCorpusReader\n",
      "     |      morphs2str is a function to convert morphlist to str for tree representation\n",
      "     |      for _parse()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from nltk.corpus.reader.api.SyntaxCorpusReader:\n",
      "     |  \n",
      "     |  parsed_sents(self, fileids=None)\n",
      "     |  \n",
      "     |  raw(self, fileids=None)\n",
      "     |  \n",
      "     |  sents(self, fileids=None)\n",
      "     |  \n",
      "     |  tagged_sents(self, fileids=None, tagset=None)\n",
      "     |  \n",
      "     |  tagged_words(self, fileids=None, tagset=None)\n",
      "     |  \n",
      "     |  words(self, fileids=None)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __unicode__ = __str__(self, /)\n",
      "     |  \n",
      "     |  abspath(self, fileid)\n",
      "     |      Return the absolute path for the given file.\n",
      "     |      \n",
      "     |      :type fileid: str\n",
      "     |      :param fileid: The file identifier for the file whose path\n",
      "     |          should be returned.\n",
      "     |      :rtype: PathPointer\n",
      "     |  \n",
      "     |  abspaths(self, fileids=None, include_encoding=False, include_fileid=False)\n",
      "     |      Return a list of the absolute paths for all fileids in this corpus;\n",
      "     |      or for the given list of fileids, if specified.\n",
      "     |      \n",
      "     |      :type fileids: None or str or list\n",
      "     |      :param fileids: Specifies the set of fileids for which paths should\n",
      "     |          be returned.  Can be None, for all fileids; a list of\n",
      "     |          file identifiers, for a specified set of fileids; or a single\n",
      "     |          file identifier, for a single file.  Note that the return\n",
      "     |          value is always a list of paths, even if ``fileids`` is a\n",
      "     |          single file identifier.\n",
      "     |      \n",
      "     |      :param include_encoding: If true, then return a list of\n",
      "     |          ``(path_pointer, encoding)`` tuples.\n",
      "     |      \n",
      "     |      :rtype: list(PathPointer)\n",
      "     |  \n",
      "     |  citation(self)\n",
      "     |      Return the contents of the corpus citation.bib file, if it exists.\n",
      "     |  \n",
      "     |  encoding(self, file)\n",
      "     |      Return the unicode encoding for the given corpus file, if known.\n",
      "     |      If the encoding is unknown, or if the given file should be\n",
      "     |      processed using byte strings (str), then return None.\n",
      "     |  \n",
      "     |  ensure_loaded(self)\n",
      "     |      Load this corpus (if it has not already been loaded).  This is\n",
      "     |      used by LazyCorpusLoader as a simple method that can be used to\n",
      "     |      make sure a corpus is loaded -- e.g., in case a user wants to\n",
      "     |      do help(some_corpus).\n",
      "     |  \n",
      "     |  fileids(self)\n",
      "     |      Return a list of file identifiers for the fileids that make up\n",
      "     |      this corpus.\n",
      "     |  \n",
      "     |  license(self)\n",
      "     |      Return the contents of the corpus LICENSE file, if it exists.\n",
      "     |  \n",
      "     |  open(self, file)\n",
      "     |      Return an open stream that can be used to read the given file.\n",
      "     |      If the file's encoding is not None, then the stream will\n",
      "     |      automatically decode the file's contents into unicode.\n",
      "     |      \n",
      "     |      :param file: The file identifier of the file to read.\n",
      "     |  \n",
      "     |  readme(self)\n",
      "     |      Return the contents of the corpus README file, if it exists.\n",
      "     |  \n",
      "     |  unicode_repr = __repr__(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  root\n",
      "     |      The directory where this corpus is stored.\n",
      "     |      \n",
      "     |      :type: PathPointer\n",
      "    \n",
      "    class LinThesaurusCorpusReader(nltk.corpus.reader.api.CorpusReader)\n",
      "     |  LinThesaurusCorpusReader(root, badscore=0.0)\n",
      "     |  \n",
      "     |  Wrapper for the LISP-formatted thesauruses distributed by Dekang Lin.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      LinThesaurusCorpusReader\n",
      "     |      nltk.corpus.reader.api.CorpusReader\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __contains__(self, ngram)\n",
      "     |      Determines whether or not the given ngram is in the thesaurus.\n",
      "     |      \n",
      "     |      :param ngram: ngram to lookup\n",
      "     |      :type ngram: C{string}\n",
      "     |      :return: whether the given ngram is in the thesaurus.\n",
      "     |  \n",
      "     |  __init__(self, root, badscore=0.0)\n",
      "     |      Initialize the thesaurus.\n",
      "     |      \n",
      "     |      :param root: root directory containing thesaurus LISP files\n",
      "     |      :type root: C{string}\n",
      "     |      :param badscore: the score to give to words which do not appear in each other's sets of synonyms\n",
      "     |      :type badscore: C{float}\n",
      "     |  \n",
      "     |  scored_synonyms(self, ngram, fileid=None)\n",
      "     |      Returns a list of scored synonyms (tuples of synonyms and scores) for the current ngram\n",
      "     |      \n",
      "     |      :param ngram: ngram to lookup\n",
      "     |      :type ngram: C{string}\n",
      "     |      :param fileid: thesaurus fileid to search in. If None, search all fileids.\n",
      "     |      :type fileid: C{string}\n",
      "     |      :return: If fileid is specified, list of tuples of scores and synonyms; otherwise,\n",
      "     |               list of tuples of fileids and lists, where inner lists consist of tuples of\n",
      "     |               scores and synonyms.\n",
      "     |  \n",
      "     |  similarity(self, ngram1, ngram2, fileid=None)\n",
      "     |      Returns the similarity score for two ngrams.\n",
      "     |      \n",
      "     |      :param ngram1: first ngram to compare\n",
      "     |      :type ngram1: C{string}\n",
      "     |      :param ngram2: second ngram to compare\n",
      "     |      :type ngram2: C{string}\n",
      "     |      :param fileid: thesaurus fileid to search in. If None, search all fileids.\n",
      "     |      :type fileid: C{string}\n",
      "     |      :return: If fileid is specified, just the score for the two ngrams; otherwise,\n",
      "     |               list of tuples of fileids and scores.\n",
      "     |  \n",
      "     |  synonyms(self, ngram, fileid=None)\n",
      "     |      Returns a list of synonyms for the current ngram.\n",
      "     |      \n",
      "     |      :param ngram: ngram to lookup\n",
      "     |      :type ngram: C{string}\n",
      "     |      :param fileid: thesaurus fileid to search in. If None, search all fileids.\n",
      "     |      :type fileid: C{string}\n",
      "     |      :return: If fileid is specified, list of synonyms; otherwise, list of tuples of fileids and\n",
      "     |               lists, where inner lists contain synonyms.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __unicode__ = __str__(self, /)\n",
      "     |  \n",
      "     |  abspath(self, fileid)\n",
      "     |      Return the absolute path for the given file.\n",
      "     |      \n",
      "     |      :type fileid: str\n",
      "     |      :param fileid: The file identifier for the file whose path\n",
      "     |          should be returned.\n",
      "     |      :rtype: PathPointer\n",
      "     |  \n",
      "     |  abspaths(self, fileids=None, include_encoding=False, include_fileid=False)\n",
      "     |      Return a list of the absolute paths for all fileids in this corpus;\n",
      "     |      or for the given list of fileids, if specified.\n",
      "     |      \n",
      "     |      :type fileids: None or str or list\n",
      "     |      :param fileids: Specifies the set of fileids for which paths should\n",
      "     |          be returned.  Can be None, for all fileids; a list of\n",
      "     |          file identifiers, for a specified set of fileids; or a single\n",
      "     |          file identifier, for a single file.  Note that the return\n",
      "     |          value is always a list of paths, even if ``fileids`` is a\n",
      "     |          single file identifier.\n",
      "     |      \n",
      "     |      :param include_encoding: If true, then return a list of\n",
      "     |          ``(path_pointer, encoding)`` tuples.\n",
      "     |      \n",
      "     |      :rtype: list(PathPointer)\n",
      "     |  \n",
      "     |  citation(self)\n",
      "     |      Return the contents of the corpus citation.bib file, if it exists.\n",
      "     |  \n",
      "     |  encoding(self, file)\n",
      "     |      Return the unicode encoding for the given corpus file, if known.\n",
      "     |      If the encoding is unknown, or if the given file should be\n",
      "     |      processed using byte strings (str), then return None.\n",
      "     |  \n",
      "     |  ensure_loaded(self)\n",
      "     |      Load this corpus (if it has not already been loaded).  This is\n",
      "     |      used by LazyCorpusLoader as a simple method that can be used to\n",
      "     |      make sure a corpus is loaded -- e.g., in case a user wants to\n",
      "     |      do help(some_corpus).\n",
      "     |  \n",
      "     |  fileids(self)\n",
      "     |      Return a list of file identifiers for the fileids that make up\n",
      "     |      this corpus.\n",
      "     |  \n",
      "     |  license(self)\n",
      "     |      Return the contents of the corpus LICENSE file, if it exists.\n",
      "     |  \n",
      "     |  open(self, file)\n",
      "     |      Return an open stream that can be used to read the given file.\n",
      "     |      If the file's encoding is not None, then the stream will\n",
      "     |      automatically decode the file's contents into unicode.\n",
      "     |      \n",
      "     |      :param file: The file identifier of the file to read.\n",
      "     |  \n",
      "     |  readme(self)\n",
      "     |      Return the contents of the corpus README file, if it exists.\n",
      "     |  \n",
      "     |  unicode_repr = __repr__(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  root\n",
      "     |      The directory where this corpus is stored.\n",
      "     |      \n",
      "     |      :type: PathPointer\n",
      "    \n",
      "    class MTECorpusReader(nltk.corpus.reader.tagged.TaggedCorpusReader)\n",
      "     |  MTECorpusReader(root=None, fileids=None, encoding='utf8')\n",
      "     |  \n",
      "     |  Reader for corpora following the TEI-p5 xml scheme, such as MULTEXT-East.\n",
      "     |  MULTEXT-East contains part-of-speech-tagged words with a quite precise tagging\n",
      "     |  scheme. These tags can be converted to the Universal tagset\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      MTECorpusReader\n",
      "     |      nltk.corpus.reader.tagged.TaggedCorpusReader\n",
      "     |      nltk.corpus.reader.api.CorpusReader\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, root=None, fileids=None, encoding='utf8')\n",
      "     |      Construct a new MTECorpusreader for a set of documents\n",
      "     |      located at the given root directory.  Example usage:\n",
      "     |      \n",
      "     |          >>> root = '/...path to corpus.../'\n",
      "     |          >>> reader = MTECorpusReader(root, 'oana-*.xml', 'utf8') # doctest: +SKIP\n",
      "     |      \n",
      "     |      :param root: The root directory for this corpus. (default points to location in multext config file)\n",
      "     |      :param fileids: A list or regexp specifying the fileids in this corpus. (default is oana-en.xml)\n",
      "     |      :param enconding: The encoding of the given files (default is utf8)\n",
      "     |  \n",
      "     |  lemma_paras(self, fileids=None)\n",
      "     |          :param fileids: A list specifying the fileids that should be used.\n",
      "     |      :return: the given file(s) as a list of paragraphs, each encoded as a\n",
      "     |               list of sentences, which are in turn encoded as a list of\n",
      "     |               tuples of the word and the corresponding lemma (word, lemma)\n",
      "     |      :rtype: list(List(List(tuple(str, str))))\n",
      "     |  \n",
      "     |  lemma_sents(self, fileids=None)\n",
      "     |          :param fileids: A list specifying the fileids that should be used.\n",
      "     |      :return: the given file(s) as a list of sentences or utterances, each\n",
      "     |               encoded as a list of tuples of the word and the corresponding\n",
      "     |               lemma (word, lemma)\n",
      "     |      :rtype: list(list(tuple(str, str)))\n",
      "     |  \n",
      "     |  lemma_words(self, fileids=None)\n",
      "     |          :param fileids: A list specifying the fileids that should be used.\n",
      "     |      :return: the given file(s) as a list of words, the corresponding lemmas\n",
      "     |               and punctuation symbols, encoded as tuples (word, lemma)\n",
      "     |      :rtype: list(tuple(str,str))\n",
      "     |  \n",
      "     |  paras(self, fileids=None)\n",
      "     |          :param fileids: A list specifying the fileids that should be used.\n",
      "     |      :return: the given file(s) as a list of paragraphs, each encoded as a list\n",
      "     |               of sentences, which are in turn encoded as lists of word string\n",
      "     |      :rtype: list(list(list(str)))\n",
      "     |  \n",
      "     |  raw(self, fileids=None)\n",
      "     |          :param fileids: A list specifying the fileids that should be used.\n",
      "     |      :return: the given file(s) as a single string.\n",
      "     |      :rtype: str\n",
      "     |  \n",
      "     |  readme(self)\n",
      "     |      Prints some information about this corpus.\n",
      "     |      :return: the content of the attached README file\n",
      "     |      :rtype: str\n",
      "     |  \n",
      "     |  sents(self, fileids=None)\n",
      "     |          :param fileids: A list specifying the fileids that should be used.\n",
      "     |      :return: the given file(s) as a list of sentences or utterances,\n",
      "     |               each encoded as a list of word strings\n",
      "     |      :rtype: list(list(str))\n",
      "     |  \n",
      "     |  tagged_paras(self, fileids=None, tagset='msd', tags='')\n",
      "     |          :param fileids: A list specifying the fileids that should be used.\n",
      "     |      :param tagset: The tagset that should be used in the returned object,\n",
      "     |                     either \"universal\" or \"msd\", \"msd\" is the default\n",
      "     |      :param tags: An MSD Tag that is used to filter all parts of the used corpus\n",
      "     |                   that are not more precise or at least equal to the given tag\n",
      "     |      :return: the given file(s) as a list of paragraphs, each encoded as a\n",
      "     |               list of sentences, which are in turn encoded as a list\n",
      "     |               of (word,tag) tuples\n",
      "     |      :rtype: list(list(list(tuple(str, str))))\n",
      "     |  \n",
      "     |  tagged_sents(self, fileids=None, tagset='msd', tags='')\n",
      "     |          :param fileids: A list specifying the fileids that should be used.\n",
      "     |      :param tagset: The tagset that should be used in the returned object,\n",
      "     |                     either \"universal\" or \"msd\", \"msd\" is the default\n",
      "     |      :param tags: An MSD Tag that is used to filter all parts of the used corpus\n",
      "     |                   that are not more precise or at least equal to the given tag\n",
      "     |      :return: the given file(s) as a list of sentences or utterances, each\n",
      "     |               each encoded as a list of (word,tag) tuples\n",
      "     |      :rtype: list(list(tuple(str, str)))\n",
      "     |  \n",
      "     |  tagged_words(self, fileids=None, tagset='msd', tags='')\n",
      "     |          :param fileids: A list specifying the fileids that should be used.\n",
      "     |      :param tagset: The tagset that should be used in the returned object,\n",
      "     |                     either \"universal\" or \"msd\", \"msd\" is the default\n",
      "     |      :param tags: An MSD Tag that is used to filter all parts of the used corpus\n",
      "     |                   that are not more precise or at least equal to the given tag\n",
      "     |      :return: the given file(s) as a list of tagged words and punctuation symbols\n",
      "     |               encoded as tuples (word, tag)\n",
      "     |      :rtype: list(tuple(str, str))\n",
      "     |  \n",
      "     |  words(self, fileids=None)\n",
      "     |          :param fileids: A list specifying the fileids that should be used.\n",
      "     |      :return: the given file(s) as a list of words and punctuation symbols.\n",
      "     |      :rtype: list(str)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __unicode__ = __str__(self, /)\n",
      "     |  \n",
      "     |  abspath(self, fileid)\n",
      "     |      Return the absolute path for the given file.\n",
      "     |      \n",
      "     |      :type fileid: str\n",
      "     |      :param fileid: The file identifier for the file whose path\n",
      "     |          should be returned.\n",
      "     |      :rtype: PathPointer\n",
      "     |  \n",
      "     |  abspaths(self, fileids=None, include_encoding=False, include_fileid=False)\n",
      "     |      Return a list of the absolute paths for all fileids in this corpus;\n",
      "     |      or for the given list of fileids, if specified.\n",
      "     |      \n",
      "     |      :type fileids: None or str or list\n",
      "     |      :param fileids: Specifies the set of fileids for which paths should\n",
      "     |          be returned.  Can be None, for all fileids; a list of\n",
      "     |          file identifiers, for a specified set of fileids; or a single\n",
      "     |          file identifier, for a single file.  Note that the return\n",
      "     |          value is always a list of paths, even if ``fileids`` is a\n",
      "     |          single file identifier.\n",
      "     |      \n",
      "     |      :param include_encoding: If true, then return a list of\n",
      "     |          ``(path_pointer, encoding)`` tuples.\n",
      "     |      \n",
      "     |      :rtype: list(PathPointer)\n",
      "     |  \n",
      "     |  citation(self)\n",
      "     |      Return the contents of the corpus citation.bib file, if it exists.\n",
      "     |  \n",
      "     |  encoding(self, file)\n",
      "     |      Return the unicode encoding for the given corpus file, if known.\n",
      "     |      If the encoding is unknown, or if the given file should be\n",
      "     |      processed using byte strings (str), then return None.\n",
      "     |  \n",
      "     |  ensure_loaded(self)\n",
      "     |      Load this corpus (if it has not already been loaded).  This is\n",
      "     |      used by LazyCorpusLoader as a simple method that can be used to\n",
      "     |      make sure a corpus is loaded -- e.g., in case a user wants to\n",
      "     |      do help(some_corpus).\n",
      "     |  \n",
      "     |  fileids(self)\n",
      "     |      Return a list of file identifiers for the fileids that make up\n",
      "     |      this corpus.\n",
      "     |  \n",
      "     |  license(self)\n",
      "     |      Return the contents of the corpus LICENSE file, if it exists.\n",
      "     |  \n",
      "     |  open(self, file)\n",
      "     |      Return an open stream that can be used to read the given file.\n",
      "     |      If the file's encoding is not None, then the stream will\n",
      "     |      automatically decode the file's contents into unicode.\n",
      "     |      \n",
      "     |      :param file: The file identifier of the file to read.\n",
      "     |  \n",
      "     |  unicode_repr = __repr__(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  root\n",
      "     |      The directory where this corpus is stored.\n",
      "     |      \n",
      "     |      :type: PathPointer\n",
      "    \n",
      "    class MWAPPDBCorpusReader(WordListCorpusReader)\n",
      "     |  MWAPPDBCorpusReader(root, fileids, encoding='utf8', tagset=None)\n",
      "     |  \n",
      "     |  This class is used to read the list of word pairs from the subset of lexical\n",
      "     |  pairs of The Paraphrase Database (PPDB) XXXL used in the Monolingual Word\n",
      "     |  Alignment (MWA) algorithm described in Sultan et al. (2014a, 2014b, 2015):\n",
      "     |   - http://acl2014.org/acl2014/Q14/pdf/Q14-1017\n",
      "     |   - http://www.aclweb.org/anthology/S14-2039\n",
      "     |   - http://www.aclweb.org/anthology/S15-2027\n",
      "     |  \n",
      "     |  The original source of the full PPDB corpus can be found on\n",
      "     |  http://www.cis.upenn.edu/~ccb/ppdb/\n",
      "     |  \n",
      "     |  :return: a list of tuples of similar lexical terms.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      MWAPPDBCorpusReader\n",
      "     |      WordListCorpusReader\n",
      "     |      nltk.corpus.reader.api.CorpusReader\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  entries(self, fileids='ppdb-1.0-xxxl-lexical.extended.synonyms.uniquepairs')\n",
      "     |      :return: a tuple of synonym word pairs.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  mwa_ppdb_xxxl_file = 'ppdb-1.0-xxxl-lexical.extended.synonyms.uniquepa...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from WordListCorpusReader:\n",
      "     |  \n",
      "     |  raw(self, fileids=None)\n",
      "     |  \n",
      "     |  words(self, fileids=None, ignore_lines_startswith='\\n')\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __init__(self, root, fileids, encoding='utf8', tagset=None)\n",
      "     |      :type root: PathPointer or str\n",
      "     |      :param root: A path pointer identifying the root directory for\n",
      "     |          this corpus.  If a string is specified, then it will be\n",
      "     |          converted to a ``PathPointer`` automatically.\n",
      "     |      :param fileids: A list of the files that make up this corpus.\n",
      "     |          This list can either be specified explicitly, as a list of\n",
      "     |          strings; or implicitly, as a regular expression over file\n",
      "     |          paths.  The absolute path for each file will be constructed\n",
      "     |          by joining the reader's root to each file name.\n",
      "     |      :param encoding: The default unicode encoding for the files\n",
      "     |          that make up the corpus.  The value of ``encoding`` can be any\n",
      "     |          of the following:\n",
      "     |          - A string: ``encoding`` is the encoding name for all files.\n",
      "     |          - A dictionary: ``encoding[file_id]`` is the encoding\n",
      "     |            name for the file whose identifier is ``file_id``.  If\n",
      "     |            ``file_id`` is not in ``encoding``, then the file\n",
      "     |            contents will be processed using non-unicode byte strings.\n",
      "     |          - A list: ``encoding`` should be a list of ``(regexp, encoding)``\n",
      "     |            tuples.  The encoding for a file whose identifier is ``file_id``\n",
      "     |            will be the ``encoding`` value for the first tuple whose\n",
      "     |            ``regexp`` matches the ``file_id``.  If no tuple's ``regexp``\n",
      "     |            matches the ``file_id``, the file contents will be processed\n",
      "     |            using non-unicode byte strings.\n",
      "     |          - None: the file contents of all files will be\n",
      "     |            processed using non-unicode byte strings.\n",
      "     |      :param tagset: The name of the tagset used by this corpus, to be used\n",
      "     |            for normalizing or converting the POS tags returned by the\n",
      "     |            tagged_...() methods.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __unicode__ = __str__(self, /)\n",
      "     |  \n",
      "     |  abspath(self, fileid)\n",
      "     |      Return the absolute path for the given file.\n",
      "     |      \n",
      "     |      :type fileid: str\n",
      "     |      :param fileid: The file identifier for the file whose path\n",
      "     |          should be returned.\n",
      "     |      :rtype: PathPointer\n",
      "     |  \n",
      "     |  abspaths(self, fileids=None, include_encoding=False, include_fileid=False)\n",
      "     |      Return a list of the absolute paths for all fileids in this corpus;\n",
      "     |      or for the given list of fileids, if specified.\n",
      "     |      \n",
      "     |      :type fileids: None or str or list\n",
      "     |      :param fileids: Specifies the set of fileids for which paths should\n",
      "     |          be returned.  Can be None, for all fileids; a list of\n",
      "     |          file identifiers, for a specified set of fileids; or a single\n",
      "     |          file identifier, for a single file.  Note that the return\n",
      "     |          value is always a list of paths, even if ``fileids`` is a\n",
      "     |          single file identifier.\n",
      "     |      \n",
      "     |      :param include_encoding: If true, then return a list of\n",
      "     |          ``(path_pointer, encoding)`` tuples.\n",
      "     |      \n",
      "     |      :rtype: list(PathPointer)\n",
      "     |  \n",
      "     |  citation(self)\n",
      "     |      Return the contents of the corpus citation.bib file, if it exists.\n",
      "     |  \n",
      "     |  encoding(self, file)\n",
      "     |      Return the unicode encoding for the given corpus file, if known.\n",
      "     |      If the encoding is unknown, or if the given file should be\n",
      "     |      processed using byte strings (str), then return None.\n",
      "     |  \n",
      "     |  ensure_loaded(self)\n",
      "     |      Load this corpus (if it has not already been loaded).  This is\n",
      "     |      used by LazyCorpusLoader as a simple method that can be used to\n",
      "     |      make sure a corpus is loaded -- e.g., in case a user wants to\n",
      "     |      do help(some_corpus).\n",
      "     |  \n",
      "     |  fileids(self)\n",
      "     |      Return a list of file identifiers for the fileids that make up\n",
      "     |      this corpus.\n",
      "     |  \n",
      "     |  license(self)\n",
      "     |      Return the contents of the corpus LICENSE file, if it exists.\n",
      "     |  \n",
      "     |  open(self, file)\n",
      "     |      Return an open stream that can be used to read the given file.\n",
      "     |      If the file's encoding is not None, then the stream will\n",
      "     |      automatically decode the file's contents into unicode.\n",
      "     |      \n",
      "     |      :param file: The file identifier of the file to read.\n",
      "     |  \n",
      "     |  readme(self)\n",
      "     |      Return the contents of the corpus README file, if it exists.\n",
      "     |  \n",
      "     |  unicode_repr = __repr__(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  root\n",
      "     |      The directory where this corpus is stored.\n",
      "     |      \n",
      "     |      :type: PathPointer\n",
      "    \n",
      "    class MacMorphoCorpusReader(TaggedCorpusReader)\n",
      "     |  MacMorphoCorpusReader(root, fileids, encoding='utf8', tagset=None)\n",
      "     |  \n",
      "     |  A corpus reader for the MAC_MORPHO corpus.  Each line contains a\n",
      "     |  single tagged word, using '_' as a separator.  Sentence boundaries\n",
      "     |  are based on the end-sentence tag ('_.').  Paragraph information\n",
      "     |  is not included in the corpus, so each paragraph returned by\n",
      "     |  ``self.paras()`` and ``self.tagged_paras()`` contains a single\n",
      "     |  sentence.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      MacMorphoCorpusReader\n",
      "     |      TaggedCorpusReader\n",
      "     |      nltk.corpus.reader.api.CorpusReader\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, root, fileids, encoding='utf8', tagset=None)\n",
      "     |      Construct a new Tagged Corpus reader for a set of documents\n",
      "     |      located at the given root directory.  Example usage:\n",
      "     |      \n",
      "     |          >>> root = '/...path to corpus.../'\n",
      "     |          >>> reader = TaggedCorpusReader(root, '.*', '.txt') # doctest: +SKIP\n",
      "     |      \n",
      "     |      :param root: The root directory for this corpus.\n",
      "     |      :param fileids: A list or regexp specifying the fileids in this corpus.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from TaggedCorpusReader:\n",
      "     |  \n",
      "     |  paras(self, fileids=None)\n",
      "     |      :return: the given file(s) as a list of\n",
      "     |          paragraphs, each encoded as a list of sentences, which are\n",
      "     |          in turn encoded as lists of word strings.\n",
      "     |      :rtype: list(list(list(str)))\n",
      "     |  \n",
      "     |  raw(self, fileids=None)\n",
      "     |      :return: the given file(s) as a single string.\n",
      "     |      :rtype: str\n",
      "     |  \n",
      "     |  sents(self, fileids=None)\n",
      "     |      :return: the given file(s) as a list of\n",
      "     |          sentences or utterances, each encoded as a list of word\n",
      "     |          strings.\n",
      "     |      :rtype: list(list(str))\n",
      "     |  \n",
      "     |  tagged_paras(self, fileids=None, tagset=None)\n",
      "     |      :return: the given file(s) as a list of\n",
      "     |          paragraphs, each encoded as a list of sentences, which are\n",
      "     |          in turn encoded as lists of ``(word,tag)`` tuples.\n",
      "     |      :rtype: list(list(list(tuple(str,str))))\n",
      "     |  \n",
      "     |  tagged_sents(self, fileids=None, tagset=None)\n",
      "     |      :return: the given file(s) as a list of\n",
      "     |          sentences, each encoded as a list of ``(word,tag)`` tuples.\n",
      "     |      \n",
      "     |      :rtype: list(list(tuple(str,str)))\n",
      "     |  \n",
      "     |  tagged_words(self, fileids=None, tagset=None)\n",
      "     |      :return: the given file(s) as a list of tagged\n",
      "     |          words and punctuation symbols, encoded as tuples\n",
      "     |          ``(word,tag)``.\n",
      "     |      :rtype: list(tuple(str,str))\n",
      "     |  \n",
      "     |  words(self, fileids=None)\n",
      "     |      :return: the given file(s) as a list of words\n",
      "     |          and punctuation symbols.\n",
      "     |      :rtype: list(str)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __unicode__ = __str__(self, /)\n",
      "     |  \n",
      "     |  abspath(self, fileid)\n",
      "     |      Return the absolute path for the given file.\n",
      "     |      \n",
      "     |      :type fileid: str\n",
      "     |      :param fileid: The file identifier for the file whose path\n",
      "     |          should be returned.\n",
      "     |      :rtype: PathPointer\n",
      "     |  \n",
      "     |  abspaths(self, fileids=None, include_encoding=False, include_fileid=False)\n",
      "     |      Return a list of the absolute paths for all fileids in this corpus;\n",
      "     |      or for the given list of fileids, if specified.\n",
      "     |      \n",
      "     |      :type fileids: None or str or list\n",
      "     |      :param fileids: Specifies the set of fileids for which paths should\n",
      "     |          be returned.  Can be None, for all fileids; a list of\n",
      "     |          file identifiers, for a specified set of fileids; or a single\n",
      "     |          file identifier, for a single file.  Note that the return\n",
      "     |          value is always a list of paths, even if ``fileids`` is a\n",
      "     |          single file identifier.\n",
      "     |      \n",
      "     |      :param include_encoding: If true, then return a list of\n",
      "     |          ``(path_pointer, encoding)`` tuples.\n",
      "     |      \n",
      "     |      :rtype: list(PathPointer)\n",
      "     |  \n",
      "     |  citation(self)\n",
      "     |      Return the contents of the corpus citation.bib file, if it exists.\n",
      "     |  \n",
      "     |  encoding(self, file)\n",
      "     |      Return the unicode encoding for the given corpus file, if known.\n",
      "     |      If the encoding is unknown, or if the given file should be\n",
      "     |      processed using byte strings (str), then return None.\n",
      "     |  \n",
      "     |  ensure_loaded(self)\n",
      "     |      Load this corpus (if it has not already been loaded).  This is\n",
      "     |      used by LazyCorpusLoader as a simple method that can be used to\n",
      "     |      make sure a corpus is loaded -- e.g., in case a user wants to\n",
      "     |      do help(some_corpus).\n",
      "     |  \n",
      "     |  fileids(self)\n",
      "     |      Return a list of file identifiers for the fileids that make up\n",
      "     |      this corpus.\n",
      "     |  \n",
      "     |  license(self)\n",
      "     |      Return the contents of the corpus LICENSE file, if it exists.\n",
      "     |  \n",
      "     |  open(self, file)\n",
      "     |      Return an open stream that can be used to read the given file.\n",
      "     |      If the file's encoding is not None, then the stream will\n",
      "     |      automatically decode the file's contents into unicode.\n",
      "     |      \n",
      "     |      :param file: The file identifier of the file to read.\n",
      "     |  \n",
      "     |  readme(self)\n",
      "     |      Return the contents of the corpus README file, if it exists.\n",
      "     |  \n",
      "     |  unicode_repr = __repr__(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  root\n",
      "     |      The directory where this corpus is stored.\n",
      "     |      \n",
      "     |      :type: PathPointer\n",
      "    \n",
      "    class NKJPCorpusReader(nltk.corpus.reader.xmldocs.XMLCorpusReader)\n",
      "     |  NKJPCorpusReader(root, fileids='.*')\n",
      "     |  \n",
      "     |  Corpus reader for corpora whose documents are xml files.\n",
      "     |  \n",
      "     |  Note that the ``XMLCorpusReader`` constructor does not take an\n",
      "     |  ``encoding`` argument, because the unicode encoding is specified by\n",
      "     |  the XML files themselves.  See the XML specs for more info.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      NKJPCorpusReader\n",
      "     |      nltk.corpus.reader.xmldocs.XMLCorpusReader\n",
      "     |      nltk.corpus.reader.api.CorpusReader\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, root, fileids='.*')\n",
      "     |      Corpus reader designed to work with National Corpus of Polish.\n",
      "     |      See http://nkjp.pl/ for more details about NKJP.\n",
      "     |      use example:\n",
      "     |      import nltk\n",
      "     |      import nkjp\n",
      "     |      from nkjp import NKJPCorpusReader\n",
      "     |      x = NKJPCorpusReader(root='/home/USER/nltk_data/corpora/nkjp/', fileids='') # obtain the whole corpus\n",
      "     |      x.header()\n",
      "     |      x.raw()\n",
      "     |      x.words()\n",
      "     |      x.tagged_words(tags=['subst', 'comp'])  #Link to find more tags: nkjp.pl/poliqarp/help/ense2.html\n",
      "     |      x.sents()\n",
      "     |      x = NKJPCorpusReader(root='/home/USER/nltk_data/corpora/nkjp/', fileids='Wilk*') # obtain particular file(s)\n",
      "     |      x.header(fileids=['WilkDom', '/home/USER/nltk_data/corpora/nkjp/WilkWilczy'])\n",
      "     |      x.tagged_words(fileids=['WilkDom', '/home/USER/nltk_data/corpora/nkjp/WilkWilczy'], tags=['subst', 'comp'])\n",
      "     |  \n",
      "     |  add_root(self, fileid)\n",
      "     |      Add root if necessary to specified fileid.\n",
      "     |  \n",
      "     |  fileids(self)\n",
      "     |      Returns a list of file identifiers for the fileids that make up\n",
      "     |      this corpus.\n",
      "     |  \n",
      "     |  get_paths(self)\n",
      "     |  \n",
      "     |  header(self, fileids=None, **kwargs)\n",
      "     |      Returns header(s) of specified fileids.\n",
      "     |  \n",
      "     |  raw(self, fileids=None, **kwargs)\n",
      "     |      Returns words in specified fileids.\n",
      "     |  \n",
      "     |  sents(self, fileids=None, **kwargs)\n",
      "     |      Returns sentences in specified fileids.\n",
      "     |  \n",
      "     |  tagged_words(self, fileids=None, **kwargs)\n",
      "     |      Call with specified tags as a list, e.g. tags=['subst', 'comp'].\n",
      "     |      Returns tagged words in specified fileids.\n",
      "     |  \n",
      "     |  words(self, fileids=None, **kwargs)\n",
      "     |      Returns words in specified fileids.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  HEADER_MODE = 2\n",
      "     |  \n",
      "     |  RAW_MODE = 3\n",
      "     |  \n",
      "     |  SENTS_MODE = 1\n",
      "     |  \n",
      "     |  WORDS_MODE = 0\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from nltk.corpus.reader.xmldocs.XMLCorpusReader:\n",
      "     |  \n",
      "     |  xml(self, fileid=None)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __unicode__ = __str__(self, /)\n",
      "     |  \n",
      "     |  abspath(self, fileid)\n",
      "     |      Return the absolute path for the given file.\n",
      "     |      \n",
      "     |      :type fileid: str\n",
      "     |      :param fileid: The file identifier for the file whose path\n",
      "     |          should be returned.\n",
      "     |      :rtype: PathPointer\n",
      "     |  \n",
      "     |  abspaths(self, fileids=None, include_encoding=False, include_fileid=False)\n",
      "     |      Return a list of the absolute paths for all fileids in this corpus;\n",
      "     |      or for the given list of fileids, if specified.\n",
      "     |      \n",
      "     |      :type fileids: None or str or list\n",
      "     |      :param fileids: Specifies the set of fileids for which paths should\n",
      "     |          be returned.  Can be None, for all fileids; a list of\n",
      "     |          file identifiers, for a specified set of fileids; or a single\n",
      "     |          file identifier, for a single file.  Note that the return\n",
      "     |          value is always a list of paths, even if ``fileids`` is a\n",
      "     |          single file identifier.\n",
      "     |      \n",
      "     |      :param include_encoding: If true, then return a list of\n",
      "     |          ``(path_pointer, encoding)`` tuples.\n",
      "     |      \n",
      "     |      :rtype: list(PathPointer)\n",
      "     |  \n",
      "     |  citation(self)\n",
      "     |      Return the contents of the corpus citation.bib file, if it exists.\n",
      "     |  \n",
      "     |  encoding(self, file)\n",
      "     |      Return the unicode encoding for the given corpus file, if known.\n",
      "     |      If the encoding is unknown, or if the given file should be\n",
      "     |      processed using byte strings (str), then return None.\n",
      "     |  \n",
      "     |  ensure_loaded(self)\n",
      "     |      Load this corpus (if it has not already been loaded).  This is\n",
      "     |      used by LazyCorpusLoader as a simple method that can be used to\n",
      "     |      make sure a corpus is loaded -- e.g., in case a user wants to\n",
      "     |      do help(some_corpus).\n",
      "     |  \n",
      "     |  license(self)\n",
      "     |      Return the contents of the corpus LICENSE file, if it exists.\n",
      "     |  \n",
      "     |  open(self, file)\n",
      "     |      Return an open stream that can be used to read the given file.\n",
      "     |      If the file's encoding is not None, then the stream will\n",
      "     |      automatically decode the file's contents into unicode.\n",
      "     |      \n",
      "     |      :param file: The file identifier of the file to read.\n",
      "     |  \n",
      "     |  readme(self)\n",
      "     |      Return the contents of the corpus README file, if it exists.\n",
      "     |  \n",
      "     |  unicode_repr = __repr__(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  root\n",
      "     |      The directory where this corpus is stored.\n",
      "     |      \n",
      "     |      :type: PathPointer\n",
      "    \n",
      "    class NPSChatCorpusReader(nltk.corpus.reader.xmldocs.XMLCorpusReader)\n",
      "     |  NPSChatCorpusReader(root, fileids, wrap_etree=False, tagset=None)\n",
      "     |  \n",
      "     |  Corpus reader for corpora whose documents are xml files.\n",
      "     |  \n",
      "     |  Note that the ``XMLCorpusReader`` constructor does not take an\n",
      "     |  ``encoding`` argument, because the unicode encoding is specified by\n",
      "     |  the XML files themselves.  See the XML specs for more info.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      NPSChatCorpusReader\n",
      "     |      nltk.corpus.reader.xmldocs.XMLCorpusReader\n",
      "     |      nltk.corpus.reader.api.CorpusReader\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, root, fileids, wrap_etree=False, tagset=None)\n",
      "     |      :type root: PathPointer or str\n",
      "     |      :param root: A path pointer identifying the root directory for\n",
      "     |          this corpus.  If a string is specified, then it will be\n",
      "     |          converted to a ``PathPointer`` automatically.\n",
      "     |      :param fileids: A list of the files that make up this corpus.\n",
      "     |          This list can either be specified explicitly, as a list of\n",
      "     |          strings; or implicitly, as a regular expression over file\n",
      "     |          paths.  The absolute path for each file will be constructed\n",
      "     |          by joining the reader's root to each file name.\n",
      "     |      :param encoding: The default unicode encoding for the files\n",
      "     |          that make up the corpus.  The value of ``encoding`` can be any\n",
      "     |          of the following:\n",
      "     |          - A string: ``encoding`` is the encoding name for all files.\n",
      "     |          - A dictionary: ``encoding[file_id]`` is the encoding\n",
      "     |            name for the file whose identifier is ``file_id``.  If\n",
      "     |            ``file_id`` is not in ``encoding``, then the file\n",
      "     |            contents will be processed using non-unicode byte strings.\n",
      "     |          - A list: ``encoding`` should be a list of ``(regexp, encoding)``\n",
      "     |            tuples.  The encoding for a file whose identifier is ``file_id``\n",
      "     |            will be the ``encoding`` value for the first tuple whose\n",
      "     |            ``regexp`` matches the ``file_id``.  If no tuple's ``regexp``\n",
      "     |            matches the ``file_id``, the file contents will be processed\n",
      "     |            using non-unicode byte strings.\n",
      "     |          - None: the file contents of all files will be\n",
      "     |            processed using non-unicode byte strings.\n",
      "     |      :param tagset: The name of the tagset used by this corpus, to be used\n",
      "     |            for normalizing or converting the POS tags returned by the\n",
      "     |            tagged_...() methods.\n",
      "     |  \n",
      "     |  posts(self, fileids=None)\n",
      "     |  \n",
      "     |  tagged_posts(self, fileids=None, tagset=None)\n",
      "     |  \n",
      "     |  tagged_words(self, fileids=None, tagset=None)\n",
      "     |  \n",
      "     |  words(self, fileids=None)\n",
      "     |      Returns all of the words and punctuation symbols in the specified file\n",
      "     |      that were in text nodes -- ie, tags are ignored. Like the xml() method,\n",
      "     |      fileid can only specify one file.\n",
      "     |      \n",
      "     |      :return: the given file's text nodes as a list of words and punctuation symbols\n",
      "     |      :rtype: list(str)\n",
      "     |  \n",
      "     |  xml_posts(self, fileids=None)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from nltk.corpus.reader.xmldocs.XMLCorpusReader:\n",
      "     |  \n",
      "     |  raw(self, fileids=None)\n",
      "     |  \n",
      "     |  xml(self, fileid=None)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __unicode__ = __str__(self, /)\n",
      "     |  \n",
      "     |  abspath(self, fileid)\n",
      "     |      Return the absolute path for the given file.\n",
      "     |      \n",
      "     |      :type fileid: str\n",
      "     |      :param fileid: The file identifier for the file whose path\n",
      "     |          should be returned.\n",
      "     |      :rtype: PathPointer\n",
      "     |  \n",
      "     |  abspaths(self, fileids=None, include_encoding=False, include_fileid=False)\n",
      "     |      Return a list of the absolute paths for all fileids in this corpus;\n",
      "     |      or for the given list of fileids, if specified.\n",
      "     |      \n",
      "     |      :type fileids: None or str or list\n",
      "     |      :param fileids: Specifies the set of fileids for which paths should\n",
      "     |          be returned.  Can be None, for all fileids; a list of\n",
      "     |          file identifiers, for a specified set of fileids; or a single\n",
      "     |          file identifier, for a single file.  Note that the return\n",
      "     |          value is always a list of paths, even if ``fileids`` is a\n",
      "     |          single file identifier.\n",
      "     |      \n",
      "     |      :param include_encoding: If true, then return a list of\n",
      "     |          ``(path_pointer, encoding)`` tuples.\n",
      "     |      \n",
      "     |      :rtype: list(PathPointer)\n",
      "     |  \n",
      "     |  citation(self)\n",
      "     |      Return the contents of the corpus citation.bib file, if it exists.\n",
      "     |  \n",
      "     |  encoding(self, file)\n",
      "     |      Return the unicode encoding for the given corpus file, if known.\n",
      "     |      If the encoding is unknown, or if the given file should be\n",
      "     |      processed using byte strings (str), then return None.\n",
      "     |  \n",
      "     |  ensure_loaded(self)\n",
      "     |      Load this corpus (if it has not already been loaded).  This is\n",
      "     |      used by LazyCorpusLoader as a simple method that can be used to\n",
      "     |      make sure a corpus is loaded -- e.g., in case a user wants to\n",
      "     |      do help(some_corpus).\n",
      "     |  \n",
      "     |  fileids(self)\n",
      "     |      Return a list of file identifiers for the fileids that make up\n",
      "     |      this corpus.\n",
      "     |  \n",
      "     |  license(self)\n",
      "     |      Return the contents of the corpus LICENSE file, if it exists.\n",
      "     |  \n",
      "     |  open(self, file)\n",
      "     |      Return an open stream that can be used to read the given file.\n",
      "     |      If the file's encoding is not None, then the stream will\n",
      "     |      automatically decode the file's contents into unicode.\n",
      "     |      \n",
      "     |      :param file: The file identifier of the file to read.\n",
      "     |  \n",
      "     |  readme(self)\n",
      "     |      Return the contents of the corpus README file, if it exists.\n",
      "     |  \n",
      "     |  unicode_repr = __repr__(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  root\n",
      "     |      The directory where this corpus is stored.\n",
      "     |      \n",
      "     |      :type: PathPointer\n",
      "    \n",
      "    class NombankCorpusReader(nltk.corpus.reader.api.CorpusReader)\n",
      "     |  NombankCorpusReader(root, nomfile, framefiles='', nounsfile=None, parse_fileid_xform=None, parse_corpus=None, encoding='utf8')\n",
      "     |  \n",
      "     |  Corpus reader for the nombank corpus, which augments the Penn\n",
      "     |  Treebank with information about the predicate argument structure\n",
      "     |  of every noun instance.  The corpus consists of two parts: the\n",
      "     |  predicate-argument annotations themselves, and a set of \"frameset\n",
      "     |  files\" which define the argument labels used by the annotations,\n",
      "     |  on a per-noun basis.  Each \"frameset file\" contains one or more\n",
      "     |  predicates, such as ``'turn'`` or ``'turn_on'``, each of which is\n",
      "     |  divided into coarse-grained word senses called \"rolesets\".  For\n",
      "     |  each \"roleset\", the frameset file provides descriptions of the\n",
      "     |  argument roles, along with examples.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      NombankCorpusReader\n",
      "     |      nltk.corpus.reader.api.CorpusReader\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, root, nomfile, framefiles='', nounsfile=None, parse_fileid_xform=None, parse_corpus=None, encoding='utf8')\n",
      "     |      :param root: The root directory for this corpus.\n",
      "     |      :param nomfile: The name of the file containing the predicate-\n",
      "     |          argument annotations (relative to ``root``).\n",
      "     |      :param framefiles: A list or regexp specifying the frameset\n",
      "     |          fileids for this corpus.\n",
      "     |      :param parse_fileid_xform: A transform that should be applied\n",
      "     |          to the fileids in this corpus.  This should be a function\n",
      "     |          of one argument (a fileid) that returns a string (the new\n",
      "     |          fileid).\n",
      "     |      :param parse_corpus: The corpus containing the parse trees\n",
      "     |          corresponding to this corpus.  These parse trees are\n",
      "     |          necessary to resolve the tree pointers used by nombank.\n",
      "     |  \n",
      "     |  instances(self, baseform=None)\n",
      "     |      :return: a corpus view that acts as a list of\n",
      "     |      ``NombankInstance`` objects, one for each noun in the corpus.\n",
      "     |  \n",
      "     |  lines(self)\n",
      "     |      :return: a corpus view that acts as a list of strings, one for\n",
      "     |      each line in the predicate-argument annotation file.\n",
      "     |  \n",
      "     |  nouns(self)\n",
      "     |      :return: a corpus view that acts as a list of all noun lemmas\n",
      "     |      in this corpus (from the nombank.1.0.words file).\n",
      "     |  \n",
      "     |  raw(self, fileids=None)\n",
      "     |      :return: the text contents of the given fileids, as a single string.\n",
      "     |  \n",
      "     |  roleset(self, roleset_id)\n",
      "     |      :return: the xml description for the given roleset.\n",
      "     |  \n",
      "     |  rolesets(self, baseform=None)\n",
      "     |      :return: list of xml descriptions for rolesets.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __unicode__ = __str__(self, /)\n",
      "     |  \n",
      "     |  abspath(self, fileid)\n",
      "     |      Return the absolute path for the given file.\n",
      "     |      \n",
      "     |      :type fileid: str\n",
      "     |      :param fileid: The file identifier for the file whose path\n",
      "     |          should be returned.\n",
      "     |      :rtype: PathPointer\n",
      "     |  \n",
      "     |  abspaths(self, fileids=None, include_encoding=False, include_fileid=False)\n",
      "     |      Return a list of the absolute paths for all fileids in this corpus;\n",
      "     |      or for the given list of fileids, if specified.\n",
      "     |      \n",
      "     |      :type fileids: None or str or list\n",
      "     |      :param fileids: Specifies the set of fileids for which paths should\n",
      "     |          be returned.  Can be None, for all fileids; a list of\n",
      "     |          file identifiers, for a specified set of fileids; or a single\n",
      "     |          file identifier, for a single file.  Note that the return\n",
      "     |          value is always a list of paths, even if ``fileids`` is a\n",
      "     |          single file identifier.\n",
      "     |      \n",
      "     |      :param include_encoding: If true, then return a list of\n",
      "     |          ``(path_pointer, encoding)`` tuples.\n",
      "     |      \n",
      "     |      :rtype: list(PathPointer)\n",
      "     |  \n",
      "     |  citation(self)\n",
      "     |      Return the contents of the corpus citation.bib file, if it exists.\n",
      "     |  \n",
      "     |  encoding(self, file)\n",
      "     |      Return the unicode encoding for the given corpus file, if known.\n",
      "     |      If the encoding is unknown, or if the given file should be\n",
      "     |      processed using byte strings (str), then return None.\n",
      "     |  \n",
      "     |  ensure_loaded(self)\n",
      "     |      Load this corpus (if it has not already been loaded).  This is\n",
      "     |      used by LazyCorpusLoader as a simple method that can be used to\n",
      "     |      make sure a corpus is loaded -- e.g., in case a user wants to\n",
      "     |      do help(some_corpus).\n",
      "     |  \n",
      "     |  fileids(self)\n",
      "     |      Return a list of file identifiers for the fileids that make up\n",
      "     |      this corpus.\n",
      "     |  \n",
      "     |  license(self)\n",
      "     |      Return the contents of the corpus LICENSE file, if it exists.\n",
      "     |  \n",
      "     |  open(self, file)\n",
      "     |      Return an open stream that can be used to read the given file.\n",
      "     |      If the file's encoding is not None, then the stream will\n",
      "     |      automatically decode the file's contents into unicode.\n",
      "     |      \n",
      "     |      :param file: The file identifier of the file to read.\n",
      "     |  \n",
      "     |  readme(self)\n",
      "     |      Return the contents of the corpus README file, if it exists.\n",
      "     |  \n",
      "     |  unicode_repr = __repr__(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  root\n",
      "     |      The directory where this corpus is stored.\n",
      "     |      \n",
      "     |      :type: PathPointer\n",
      "    \n",
      "    class NonbreakingPrefixesCorpusReader(WordListCorpusReader)\n",
      "     |  NonbreakingPrefixesCorpusReader(root, fileids, encoding='utf8', tagset=None)\n",
      "     |  \n",
      "     |  This is a class to read the nonbreaking prefixes textfiles from the\n",
      "     |  Moses Machine Translation toolkit. These lists are used in the Python port\n",
      "     |  of the Moses' word tokenizer.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      NonbreakingPrefixesCorpusReader\n",
      "     |      WordListCorpusReader\n",
      "     |      nltk.corpus.reader.api.CorpusReader\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  words(self, lang=None, fileids=None, ignore_lines_startswith='#')\n",
      "     |      This module returns a list of nonbreaking prefixes for the specified\n",
      "     |      language(s).\n",
      "     |      \n",
      "     |      >>> from nltk.corpus import nonbreaking_prefixes as nbp\n",
      "     |      >>> nbp.words('en')[:10] == [u'A', u'B', u'C', u'D', u'E', u'F', u'G', u'H', u'I', u'J']\n",
      "     |      True\n",
      "     |      >>> nbp.words('ta')[:5] == [u'அ', u'ஆ', u'இ', u'ஈ', u'உ']\n",
      "     |      True\n",
      "     |      \n",
      "     |      :return: a list words for the specified language(s).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  available_langs = {'ca': 'ca', 'catalan': 'ca', 'cs': 'cs', 'czech': '...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from WordListCorpusReader:\n",
      "     |  \n",
      "     |  raw(self, fileids=None)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __init__(self, root, fileids, encoding='utf8', tagset=None)\n",
      "     |      :type root: PathPointer or str\n",
      "     |      :param root: A path pointer identifying the root directory for\n",
      "     |          this corpus.  If a string is specified, then it will be\n",
      "     |          converted to a ``PathPointer`` automatically.\n",
      "     |      :param fileids: A list of the files that make up this corpus.\n",
      "     |          This list can either be specified explicitly, as a list of\n",
      "     |          strings; or implicitly, as a regular expression over file\n",
      "     |          paths.  The absolute path for each file will be constructed\n",
      "     |          by joining the reader's root to each file name.\n",
      "     |      :param encoding: The default unicode encoding for the files\n",
      "     |          that make up the corpus.  The value of ``encoding`` can be any\n",
      "     |          of the following:\n",
      "     |          - A string: ``encoding`` is the encoding name for all files.\n",
      "     |          - A dictionary: ``encoding[file_id]`` is the encoding\n",
      "     |            name for the file whose identifier is ``file_id``.  If\n",
      "     |            ``file_id`` is not in ``encoding``, then the file\n",
      "     |            contents will be processed using non-unicode byte strings.\n",
      "     |          - A list: ``encoding`` should be a list of ``(regexp, encoding)``\n",
      "     |            tuples.  The encoding for a file whose identifier is ``file_id``\n",
      "     |            will be the ``encoding`` value for the first tuple whose\n",
      "     |            ``regexp`` matches the ``file_id``.  If no tuple's ``regexp``\n",
      "     |            matches the ``file_id``, the file contents will be processed\n",
      "     |            using non-unicode byte strings.\n",
      "     |          - None: the file contents of all files will be\n",
      "     |            processed using non-unicode byte strings.\n",
      "     |      :param tagset: The name of the tagset used by this corpus, to be used\n",
      "     |            for normalizing or converting the POS tags returned by the\n",
      "     |            tagged_...() methods.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __unicode__ = __str__(self, /)\n",
      "     |  \n",
      "     |  abspath(self, fileid)\n",
      "     |      Return the absolute path for the given file.\n",
      "     |      \n",
      "     |      :type fileid: str\n",
      "     |      :param fileid: The file identifier for the file whose path\n",
      "     |          should be returned.\n",
      "     |      :rtype: PathPointer\n",
      "     |  \n",
      "     |  abspaths(self, fileids=None, include_encoding=False, include_fileid=False)\n",
      "     |      Return a list of the absolute paths for all fileids in this corpus;\n",
      "     |      or for the given list of fileids, if specified.\n",
      "     |      \n",
      "     |      :type fileids: None or str or list\n",
      "     |      :param fileids: Specifies the set of fileids for which paths should\n",
      "     |          be returned.  Can be None, for all fileids; a list of\n",
      "     |          file identifiers, for a specified set of fileids; or a single\n",
      "     |          file identifier, for a single file.  Note that the return\n",
      "     |          value is always a list of paths, even if ``fileids`` is a\n",
      "     |          single file identifier.\n",
      "     |      \n",
      "     |      :param include_encoding: If true, then return a list of\n",
      "     |          ``(path_pointer, encoding)`` tuples.\n",
      "     |      \n",
      "     |      :rtype: list(PathPointer)\n",
      "     |  \n",
      "     |  citation(self)\n",
      "     |      Return the contents of the corpus citation.bib file, if it exists.\n",
      "     |  \n",
      "     |  encoding(self, file)\n",
      "     |      Return the unicode encoding for the given corpus file, if known.\n",
      "     |      If the encoding is unknown, or if the given file should be\n",
      "     |      processed using byte strings (str), then return None.\n",
      "     |  \n",
      "     |  ensure_loaded(self)\n",
      "     |      Load this corpus (if it has not already been loaded).  This is\n",
      "     |      used by LazyCorpusLoader as a simple method that can be used to\n",
      "     |      make sure a corpus is loaded -- e.g., in case a user wants to\n",
      "     |      do help(some_corpus).\n",
      "     |  \n",
      "     |  fileids(self)\n",
      "     |      Return a list of file identifiers for the fileids that make up\n",
      "     |      this corpus.\n",
      "     |  \n",
      "     |  license(self)\n",
      "     |      Return the contents of the corpus LICENSE file, if it exists.\n",
      "     |  \n",
      "     |  open(self, file)\n",
      "     |      Return an open stream that can be used to read the given file.\n",
      "     |      If the file's encoding is not None, then the stream will\n",
      "     |      automatically decode the file's contents into unicode.\n",
      "     |      \n",
      "     |      :param file: The file identifier of the file to read.\n",
      "     |  \n",
      "     |  readme(self)\n",
      "     |      Return the contents of the corpus README file, if it exists.\n",
      "     |  \n",
      "     |  unicode_repr = __repr__(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  root\n",
      "     |      The directory where this corpus is stored.\n",
      "     |      \n",
      "     |      :type: PathPointer\n",
      "    \n",
      "    class OpinionLexiconCorpusReader(nltk.corpus.reader.wordlist.WordListCorpusReader)\n",
      "     |  OpinionLexiconCorpusReader(root, fileids, encoding='utf8', tagset=None)\n",
      "     |  \n",
      "     |  Reader for Liu and Hu opinion lexicon.  Blank lines and readme are ignored.\n",
      "     |  \n",
      "     |      >>> from nltk.corpus import opinion_lexicon\n",
      "     |      >>> opinion_lexicon.words()\n",
      "     |      ['2-faced', '2-faces', 'abnormal', 'abolish', ...]\n",
      "     |  \n",
      "     |  The OpinionLexiconCorpusReader provides shortcuts to retrieve positive/negative\n",
      "     |  words:\n",
      "     |  \n",
      "     |      >>> opinion_lexicon.negative()\n",
      "     |      ['2-faced', '2-faces', 'abnormal', 'abolish', ...]\n",
      "     |  \n",
      "     |  Note that words from `words()` method are sorted by file id, not alphabetically:\n",
      "     |  \n",
      "     |      >>> opinion_lexicon.words()[0:10]\n",
      "     |      ['2-faced', '2-faces', 'abnormal', 'abolish', 'abominable', 'abominably',\n",
      "     |      'abominate', 'abomination', 'abort', 'aborted']\n",
      "     |      >>> sorted(opinion_lexicon.words())[0:10]\n",
      "     |      ['2-faced', '2-faces', 'a+', 'abnormal', 'abolish', 'abominable', 'abominably',\n",
      "     |      'abominate', 'abomination', 'abort']\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      OpinionLexiconCorpusReader\n",
      "     |      nltk.corpus.reader.wordlist.WordListCorpusReader\n",
      "     |      nltk.corpus.reader.api.CorpusReader\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  negative(self)\n",
      "     |      Return all negative words in alphabetical order.\n",
      "     |      \n",
      "     |      :return: a list of negative words.\n",
      "     |      :rtype: list(str)\n",
      "     |  \n",
      "     |  positive(self)\n",
      "     |      Return all positive words in alphabetical order.\n",
      "     |      \n",
      "     |      :return: a list of positive words.\n",
      "     |      :rtype: list(str)\n",
      "     |  \n",
      "     |  words(self, fileids=None)\n",
      "     |      Return all words in the opinion lexicon. Note that these words are not\n",
      "     |      sorted in alphabetical order.\n",
      "     |      \n",
      "     |      :param fileids: a list or regexp specifying the ids of the files whose\n",
      "     |          words have to be returned.\n",
      "     |      :return: the given file(s) as a list of words and punctuation symbols.\n",
      "     |      :rtype: list(str)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  CorpusView = <class 'nltk.corpus.reader.opinion_lexicon.IgnoreReadmeCo...\n",
      "     |      This CorpusView is used to skip the initial readme block of the corpus.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from nltk.corpus.reader.wordlist.WordListCorpusReader:\n",
      "     |  \n",
      "     |  raw(self, fileids=None)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __init__(self, root, fileids, encoding='utf8', tagset=None)\n",
      "     |      :type root: PathPointer or str\n",
      "     |      :param root: A path pointer identifying the root directory for\n",
      "     |          this corpus.  If a string is specified, then it will be\n",
      "     |          converted to a ``PathPointer`` automatically.\n",
      "     |      :param fileids: A list of the files that make up this corpus.\n",
      "     |          This list can either be specified explicitly, as a list of\n",
      "     |          strings; or implicitly, as a regular expression over file\n",
      "     |          paths.  The absolute path for each file will be constructed\n",
      "     |          by joining the reader's root to each file name.\n",
      "     |      :param encoding: The default unicode encoding for the files\n",
      "     |          that make up the corpus.  The value of ``encoding`` can be any\n",
      "     |          of the following:\n",
      "     |          - A string: ``encoding`` is the encoding name for all files.\n",
      "     |          - A dictionary: ``encoding[file_id]`` is the encoding\n",
      "     |            name for the file whose identifier is ``file_id``.  If\n",
      "     |            ``file_id`` is not in ``encoding``, then the file\n",
      "     |            contents will be processed using non-unicode byte strings.\n",
      "     |          - A list: ``encoding`` should be a list of ``(regexp, encoding)``\n",
      "     |            tuples.  The encoding for a file whose identifier is ``file_id``\n",
      "     |            will be the ``encoding`` value for the first tuple whose\n",
      "     |            ``regexp`` matches the ``file_id``.  If no tuple's ``regexp``\n",
      "     |            matches the ``file_id``, the file contents will be processed\n",
      "     |            using non-unicode byte strings.\n",
      "     |          - None: the file contents of all files will be\n",
      "     |            processed using non-unicode byte strings.\n",
      "     |      :param tagset: The name of the tagset used by this corpus, to be used\n",
      "     |            for normalizing or converting the POS tags returned by the\n",
      "     |            tagged_...() methods.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __unicode__ = __str__(self, /)\n",
      "     |  \n",
      "     |  abspath(self, fileid)\n",
      "     |      Return the absolute path for the given file.\n",
      "     |      \n",
      "     |      :type fileid: str\n",
      "     |      :param fileid: The file identifier for the file whose path\n",
      "     |          should be returned.\n",
      "     |      :rtype: PathPointer\n",
      "     |  \n",
      "     |  abspaths(self, fileids=None, include_encoding=False, include_fileid=False)\n",
      "     |      Return a list of the absolute paths for all fileids in this corpus;\n",
      "     |      or for the given list of fileids, if specified.\n",
      "     |      \n",
      "     |      :type fileids: None or str or list\n",
      "     |      :param fileids: Specifies the set of fileids for which paths should\n",
      "     |          be returned.  Can be None, for all fileids; a list of\n",
      "     |          file identifiers, for a specified set of fileids; or a single\n",
      "     |          file identifier, for a single file.  Note that the return\n",
      "     |          value is always a list of paths, even if ``fileids`` is a\n",
      "     |          single file identifier.\n",
      "     |      \n",
      "     |      :param include_encoding: If true, then return a list of\n",
      "     |          ``(path_pointer, encoding)`` tuples.\n",
      "     |      \n",
      "     |      :rtype: list(PathPointer)\n",
      "     |  \n",
      "     |  citation(self)\n",
      "     |      Return the contents of the corpus citation.bib file, if it exists.\n",
      "     |  \n",
      "     |  encoding(self, file)\n",
      "     |      Return the unicode encoding for the given corpus file, if known.\n",
      "     |      If the encoding is unknown, or if the given file should be\n",
      "     |      processed using byte strings (str), then return None.\n",
      "     |  \n",
      "     |  ensure_loaded(self)\n",
      "     |      Load this corpus (if it has not already been loaded).  This is\n",
      "     |      used by LazyCorpusLoader as a simple method that can be used to\n",
      "     |      make sure a corpus is loaded -- e.g., in case a user wants to\n",
      "     |      do help(some_corpus).\n",
      "     |  \n",
      "     |  fileids(self)\n",
      "     |      Return a list of file identifiers for the fileids that make up\n",
      "     |      this corpus.\n",
      "     |  \n",
      "     |  license(self)\n",
      "     |      Return the contents of the corpus LICENSE file, if it exists.\n",
      "     |  \n",
      "     |  open(self, file)\n",
      "     |      Return an open stream that can be used to read the given file.\n",
      "     |      If the file's encoding is not None, then the stream will\n",
      "     |      automatically decode the file's contents into unicode.\n",
      "     |      \n",
      "     |      :param file: The file identifier of the file to read.\n",
      "     |  \n",
      "     |  readme(self)\n",
      "     |      Return the contents of the corpus README file, if it exists.\n",
      "     |  \n",
      "     |  unicode_repr = __repr__(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  root\n",
      "     |      The directory where this corpus is stored.\n",
      "     |      \n",
      "     |      :type: PathPointer\n",
      "    \n",
      "    class PPAttachmentCorpusReader(nltk.corpus.reader.api.CorpusReader)\n",
      "     |  PPAttachmentCorpusReader(root, fileids, encoding='utf8', tagset=None)\n",
      "     |  \n",
      "     |  sentence_id verb noun1 preposition noun2 attachment\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      PPAttachmentCorpusReader\n",
      "     |      nltk.corpus.reader.api.CorpusReader\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  attachments(self, fileids)\n",
      "     |  \n",
      "     |  raw(self, fileids=None)\n",
      "     |  \n",
      "     |  tuples(self, fileids)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __init__(self, root, fileids, encoding='utf8', tagset=None)\n",
      "     |      :type root: PathPointer or str\n",
      "     |      :param root: A path pointer identifying the root directory for\n",
      "     |          this corpus.  If a string is specified, then it will be\n",
      "     |          converted to a ``PathPointer`` automatically.\n",
      "     |      :param fileids: A list of the files that make up this corpus.\n",
      "     |          This list can either be specified explicitly, as a list of\n",
      "     |          strings; or implicitly, as a regular expression over file\n",
      "     |          paths.  The absolute path for each file will be constructed\n",
      "     |          by joining the reader's root to each file name.\n",
      "     |      :param encoding: The default unicode encoding for the files\n",
      "     |          that make up the corpus.  The value of ``encoding`` can be any\n",
      "     |          of the following:\n",
      "     |          - A string: ``encoding`` is the encoding name for all files.\n",
      "     |          - A dictionary: ``encoding[file_id]`` is the encoding\n",
      "     |            name for the file whose identifier is ``file_id``.  If\n",
      "     |            ``file_id`` is not in ``encoding``, then the file\n",
      "     |            contents will be processed using non-unicode byte strings.\n",
      "     |          - A list: ``encoding`` should be a list of ``(regexp, encoding)``\n",
      "     |            tuples.  The encoding for a file whose identifier is ``file_id``\n",
      "     |            will be the ``encoding`` value for the first tuple whose\n",
      "     |            ``regexp`` matches the ``file_id``.  If no tuple's ``regexp``\n",
      "     |            matches the ``file_id``, the file contents will be processed\n",
      "     |            using non-unicode byte strings.\n",
      "     |          - None: the file contents of all files will be\n",
      "     |            processed using non-unicode byte strings.\n",
      "     |      :param tagset: The name of the tagset used by this corpus, to be used\n",
      "     |            for normalizing or converting the POS tags returned by the\n",
      "     |            tagged_...() methods.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __unicode__ = __str__(self, /)\n",
      "     |  \n",
      "     |  abspath(self, fileid)\n",
      "     |      Return the absolute path for the given file.\n",
      "     |      \n",
      "     |      :type fileid: str\n",
      "     |      :param fileid: The file identifier for the file whose path\n",
      "     |          should be returned.\n",
      "     |      :rtype: PathPointer\n",
      "     |  \n",
      "     |  abspaths(self, fileids=None, include_encoding=False, include_fileid=False)\n",
      "     |      Return a list of the absolute paths for all fileids in this corpus;\n",
      "     |      or for the given list of fileids, if specified.\n",
      "     |      \n",
      "     |      :type fileids: None or str or list\n",
      "     |      :param fileids: Specifies the set of fileids for which paths should\n",
      "     |          be returned.  Can be None, for all fileids; a list of\n",
      "     |          file identifiers, for a specified set of fileids; or a single\n",
      "     |          file identifier, for a single file.  Note that the return\n",
      "     |          value is always a list of paths, even if ``fileids`` is a\n",
      "     |          single file identifier.\n",
      "     |      \n",
      "     |      :param include_encoding: If true, then return a list of\n",
      "     |          ``(path_pointer, encoding)`` tuples.\n",
      "     |      \n",
      "     |      :rtype: list(PathPointer)\n",
      "     |  \n",
      "     |  citation(self)\n",
      "     |      Return the contents of the corpus citation.bib file, if it exists.\n",
      "     |  \n",
      "     |  encoding(self, file)\n",
      "     |      Return the unicode encoding for the given corpus file, if known.\n",
      "     |      If the encoding is unknown, or if the given file should be\n",
      "     |      processed using byte strings (str), then return None.\n",
      "     |  \n",
      "     |  ensure_loaded(self)\n",
      "     |      Load this corpus (if it has not already been loaded).  This is\n",
      "     |      used by LazyCorpusLoader as a simple method that can be used to\n",
      "     |      make sure a corpus is loaded -- e.g., in case a user wants to\n",
      "     |      do help(some_corpus).\n",
      "     |  \n",
      "     |  fileids(self)\n",
      "     |      Return a list of file identifiers for the fileids that make up\n",
      "     |      this corpus.\n",
      "     |  \n",
      "     |  license(self)\n",
      "     |      Return the contents of the corpus LICENSE file, if it exists.\n",
      "     |  \n",
      "     |  open(self, file)\n",
      "     |      Return an open stream that can be used to read the given file.\n",
      "     |      If the file's encoding is not None, then the stream will\n",
      "     |      automatically decode the file's contents into unicode.\n",
      "     |      \n",
      "     |      :param file: The file identifier of the file to read.\n",
      "     |  \n",
      "     |  readme(self)\n",
      "     |      Return the contents of the corpus README file, if it exists.\n",
      "     |  \n",
      "     |  unicode_repr = __repr__(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  root\n",
      "     |      The directory where this corpus is stored.\n",
      "     |      \n",
      "     |      :type: PathPointer\n",
      "    \n",
      "    class PanLexLiteCorpusReader(nltk.corpus.reader.api.CorpusReader)\n",
      "     |  PanLexLiteCorpusReader(root)\n",
      "     |  \n",
      "     |  A base class for \"corpus reader\" classes, each of which can be\n",
      "     |  used to read a specific corpus format.  Each individual corpus\n",
      "     |  reader instance is used to read a specific corpus, consisting of\n",
      "     |  one or more files under a common root directory.  Each file is\n",
      "     |  identified by its ``file identifier``, which is the relative path\n",
      "     |  to the file from the root directory.\n",
      "     |  \n",
      "     |  A separate subclass is defined for each corpus format.  These\n",
      "     |  subclasses define one or more methods that provide 'views' on the\n",
      "     |  corpus contents, such as ``words()`` (for a list of words) and\n",
      "     |  ``parsed_sents()`` (for a list of parsed sentences).  Called with\n",
      "     |  no arguments, these methods will return the contents of the entire\n",
      "     |  corpus.  For most corpora, these methods define one or more\n",
      "     |  selection arguments, such as ``fileids`` or ``categories``, which can\n",
      "     |  be used to select which portion of the corpus should be returned.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      PanLexLiteCorpusReader\n",
      "     |      nltk.corpus.reader.api.CorpusReader\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, root)\n",
      "     |      :type root: PathPointer or str\n",
      "     |      :param root: A path pointer identifying the root directory for\n",
      "     |          this corpus.  If a string is specified, then it will be\n",
      "     |          converted to a ``PathPointer`` automatically.\n",
      "     |      :param fileids: A list of the files that make up this corpus.\n",
      "     |          This list can either be specified explicitly, as a list of\n",
      "     |          strings; or implicitly, as a regular expression over file\n",
      "     |          paths.  The absolute path for each file will be constructed\n",
      "     |          by joining the reader's root to each file name.\n",
      "     |      :param encoding: The default unicode encoding for the files\n",
      "     |          that make up the corpus.  The value of ``encoding`` can be any\n",
      "     |          of the following:\n",
      "     |          - A string: ``encoding`` is the encoding name for all files.\n",
      "     |          - A dictionary: ``encoding[file_id]`` is the encoding\n",
      "     |            name for the file whose identifier is ``file_id``.  If\n",
      "     |            ``file_id`` is not in ``encoding``, then the file\n",
      "     |            contents will be processed using non-unicode byte strings.\n",
      "     |          - A list: ``encoding`` should be a list of ``(regexp, encoding)``\n",
      "     |            tuples.  The encoding for a file whose identifier is ``file_id``\n",
      "     |            will be the ``encoding`` value for the first tuple whose\n",
      "     |            ``regexp`` matches the ``file_id``.  If no tuple's ``regexp``\n",
      "     |            matches the ``file_id``, the file contents will be processed\n",
      "     |            using non-unicode byte strings.\n",
      "     |          - None: the file contents of all files will be\n",
      "     |            processed using non-unicode byte strings.\n",
      "     |      :param tagset: The name of the tagset used by this corpus, to be used\n",
      "     |            for normalizing or converting the POS tags returned by the\n",
      "     |            tagged_...() methods.\n",
      "     |  \n",
      "     |  language_varieties(self, lc=None)\n",
      "     |      Return a list of PanLex language varieties.\n",
      "     |      \n",
      "     |      :param lc: ISO 639 alpha-3 code. If specified, filters returned varieties\n",
      "     |          by this code. If unspecified, all varieties are returned.\n",
      "     |      :return: the specified language varieties as a list of tuples. The first\n",
      "     |          element is the language variety's seven-character uniform identifier,\n",
      "     |          and the second element is its default name.\n",
      "     |      :rtype: list(tuple)\n",
      "     |  \n",
      "     |  meanings(self, expr_uid, expr_tt)\n",
      "     |      Return a list of meanings for an expression.\n",
      "     |      \n",
      "     |      :param expr_uid: the expression's language variety, as a seven-character\n",
      "     |          uniform identifier.\n",
      "     |      :param expr_tt: the expression's text.\n",
      "     |      :return: a list of Meaning objects.\n",
      "     |      :rtype: list(Meaning)\n",
      "     |  \n",
      "     |  translations(self, from_uid, from_tt, to_uid)\n",
      "     |      Return a list of translations for an expression into a single language\n",
      "     |          variety.\n",
      "     |      \n",
      "     |      :param from_uid: the source expression's language variety, as a\n",
      "     |          seven-character uniform identifier.\n",
      "     |      :param from_tt: the source expression's text.\n",
      "     |      :param to_uid: the target language variety, as a seven-character\n",
      "     |          uniform identifier.\n",
      "     |      :return a list of translation tuples. The first element is the expression\n",
      "     |          text and the second element is the translation quality.\n",
      "     |      :rtype: list(tuple)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  MEANING_Q = '\\n        SELECT dnx2.mn, dnx2.uq, dnx2.ap, dnx2.... AND ...\n",
      "     |  \n",
      "     |  TRANSLATION_Q = '\\n        SELECT s.tt, sum(s.uq) AS trq FROM (\\n  ......\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __unicode__ = __str__(self, /)\n",
      "     |  \n",
      "     |  abspath(self, fileid)\n",
      "     |      Return the absolute path for the given file.\n",
      "     |      \n",
      "     |      :type fileid: str\n",
      "     |      :param fileid: The file identifier for the file whose path\n",
      "     |          should be returned.\n",
      "     |      :rtype: PathPointer\n",
      "     |  \n",
      "     |  abspaths(self, fileids=None, include_encoding=False, include_fileid=False)\n",
      "     |      Return a list of the absolute paths for all fileids in this corpus;\n",
      "     |      or for the given list of fileids, if specified.\n",
      "     |      \n",
      "     |      :type fileids: None or str or list\n",
      "     |      :param fileids: Specifies the set of fileids for which paths should\n",
      "     |          be returned.  Can be None, for all fileids; a list of\n",
      "     |          file identifiers, for a specified set of fileids; or a single\n",
      "     |          file identifier, for a single file.  Note that the return\n",
      "     |          value is always a list of paths, even if ``fileids`` is a\n",
      "     |          single file identifier.\n",
      "     |      \n",
      "     |      :param include_encoding: If true, then return a list of\n",
      "     |          ``(path_pointer, encoding)`` tuples.\n",
      "     |      \n",
      "     |      :rtype: list(PathPointer)\n",
      "     |  \n",
      "     |  citation(self)\n",
      "     |      Return the contents of the corpus citation.bib file, if it exists.\n",
      "     |  \n",
      "     |  encoding(self, file)\n",
      "     |      Return the unicode encoding for the given corpus file, if known.\n",
      "     |      If the encoding is unknown, or if the given file should be\n",
      "     |      processed using byte strings (str), then return None.\n",
      "     |  \n",
      "     |  ensure_loaded(self)\n",
      "     |      Load this corpus (if it has not already been loaded).  This is\n",
      "     |      used by LazyCorpusLoader as a simple method that can be used to\n",
      "     |      make sure a corpus is loaded -- e.g., in case a user wants to\n",
      "     |      do help(some_corpus).\n",
      "     |  \n",
      "     |  fileids(self)\n",
      "     |      Return a list of file identifiers for the fileids that make up\n",
      "     |      this corpus.\n",
      "     |  \n",
      "     |  license(self)\n",
      "     |      Return the contents of the corpus LICENSE file, if it exists.\n",
      "     |  \n",
      "     |  open(self, file)\n",
      "     |      Return an open stream that can be used to read the given file.\n",
      "     |      If the file's encoding is not None, then the stream will\n",
      "     |      automatically decode the file's contents into unicode.\n",
      "     |      \n",
      "     |      :param file: The file identifier of the file to read.\n",
      "     |  \n",
      "     |  readme(self)\n",
      "     |      Return the contents of the corpus README file, if it exists.\n",
      "     |  \n",
      "     |  unicode_repr = __repr__(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  root\n",
      "     |      The directory where this corpus is stored.\n",
      "     |      \n",
      "     |      :type: PathPointer\n",
      "    \n",
      "    class PanlexSwadeshCorpusReader(nltk.corpus.reader.wordlist.WordListCorpusReader)\n",
      "     |  PanlexSwadeshCorpusReader(*args, **kwargs)\n",
      "     |  \n",
      "     |  This is a class to read the PanLex Swadesh list from\n",
      "     |  \n",
      "     |  David Kamholz, Jonathan Pool, and Susan M. Colowick (2014).\n",
      "     |  PanLex: Building a Resource for Panlingual Lexical Translation.\n",
      "     |  In LREC. http://www.lrec-conf.org/proceedings/lrec2014/pdf/1029_Paper.pdf\n",
      "     |  \n",
      "     |  License: CC0 1.0 Universal\n",
      "     |  https://creativecommons.org/publicdomain/zero/1.0/legalcode\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      PanlexSwadeshCorpusReader\n",
      "     |      nltk.corpus.reader.wordlist.WordListCorpusReader\n",
      "     |      nltk.corpus.reader.api.CorpusReader\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, *args, **kwargs)\n",
      "     |      :type root: PathPointer or str\n",
      "     |      :param root: A path pointer identifying the root directory for\n",
      "     |          this corpus.  If a string is specified, then it will be\n",
      "     |          converted to a ``PathPointer`` automatically.\n",
      "     |      :param fileids: A list of the files that make up this corpus.\n",
      "     |          This list can either be specified explicitly, as a list of\n",
      "     |          strings; or implicitly, as a regular expression over file\n",
      "     |          paths.  The absolute path for each file will be constructed\n",
      "     |          by joining the reader's root to each file name.\n",
      "     |      :param encoding: The default unicode encoding for the files\n",
      "     |          that make up the corpus.  The value of ``encoding`` can be any\n",
      "     |          of the following:\n",
      "     |          - A string: ``encoding`` is the encoding name for all files.\n",
      "     |          - A dictionary: ``encoding[file_id]`` is the encoding\n",
      "     |            name for the file whose identifier is ``file_id``.  If\n",
      "     |            ``file_id`` is not in ``encoding``, then the file\n",
      "     |            contents will be processed using non-unicode byte strings.\n",
      "     |          - A list: ``encoding`` should be a list of ``(regexp, encoding)``\n",
      "     |            tuples.  The encoding for a file whose identifier is ``file_id``\n",
      "     |            will be the ``encoding`` value for the first tuple whose\n",
      "     |            ``regexp`` matches the ``file_id``.  If no tuple's ``regexp``\n",
      "     |            matches the ``file_id``, the file contents will be processed\n",
      "     |            using non-unicode byte strings.\n",
      "     |          - None: the file contents of all files will be\n",
      "     |            processed using non-unicode byte strings.\n",
      "     |      :param tagset: The name of the tagset used by this corpus, to be used\n",
      "     |            for normalizing or converting the POS tags returned by the\n",
      "     |            tagged_...() methods.\n",
      "     |  \n",
      "     |  entries(self, fileids=None)\n",
      "     |      :return: a tuple of words for the specified fileids.\n",
      "     |  \n",
      "     |  get_languages(self)\n",
      "     |  \n",
      "     |  get_macrolanguages(self)\n",
      "     |  \n",
      "     |  language_codes(self)\n",
      "     |  \n",
      "     |  license(self)\n",
      "     |      Return the contents of the corpus LICENSE file, if it exists.\n",
      "     |  \n",
      "     |  readme(self)\n",
      "     |      Return the contents of the corpus README file, if it exists.\n",
      "     |  \n",
      "     |  words_by_iso639(self, iso63_code)\n",
      "     |      :return: a list of list(str)\n",
      "     |  \n",
      "     |  words_by_lang(self, lang_code)\n",
      "     |      :return: a list of list(str)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from nltk.corpus.reader.wordlist.WordListCorpusReader:\n",
      "     |  \n",
      "     |  raw(self, fileids=None)\n",
      "     |  \n",
      "     |  words(self, fileids=None, ignore_lines_startswith='\\n')\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __unicode__ = __str__(self, /)\n",
      "     |  \n",
      "     |  abspath(self, fileid)\n",
      "     |      Return the absolute path for the given file.\n",
      "     |      \n",
      "     |      :type fileid: str\n",
      "     |      :param fileid: The file identifier for the file whose path\n",
      "     |          should be returned.\n",
      "     |      :rtype: PathPointer\n",
      "     |  \n",
      "     |  abspaths(self, fileids=None, include_encoding=False, include_fileid=False)\n",
      "     |      Return a list of the absolute paths for all fileids in this corpus;\n",
      "     |      or for the given list of fileids, if specified.\n",
      "     |      \n",
      "     |      :type fileids: None or str or list\n",
      "     |      :param fileids: Specifies the set of fileids for which paths should\n",
      "     |          be returned.  Can be None, for all fileids; a list of\n",
      "     |          file identifiers, for a specified set of fileids; or a single\n",
      "     |          file identifier, for a single file.  Note that the return\n",
      "     |          value is always a list of paths, even if ``fileids`` is a\n",
      "     |          single file identifier.\n",
      "     |      \n",
      "     |      :param include_encoding: If true, then return a list of\n",
      "     |          ``(path_pointer, encoding)`` tuples.\n",
      "     |      \n",
      "     |      :rtype: list(PathPointer)\n",
      "     |  \n",
      "     |  citation(self)\n",
      "     |      Return the contents of the corpus citation.bib file, if it exists.\n",
      "     |  \n",
      "     |  encoding(self, file)\n",
      "     |      Return the unicode encoding for the given corpus file, if known.\n",
      "     |      If the encoding is unknown, or if the given file should be\n",
      "     |      processed using byte strings (str), then return None.\n",
      "     |  \n",
      "     |  ensure_loaded(self)\n",
      "     |      Load this corpus (if it has not already been loaded).  This is\n",
      "     |      used by LazyCorpusLoader as a simple method that can be used to\n",
      "     |      make sure a corpus is loaded -- e.g., in case a user wants to\n",
      "     |      do help(some_corpus).\n",
      "     |  \n",
      "     |  fileids(self)\n",
      "     |      Return a list of file identifiers for the fileids that make up\n",
      "     |      this corpus.\n",
      "     |  \n",
      "     |  open(self, file)\n",
      "     |      Return an open stream that can be used to read the given file.\n",
      "     |      If the file's encoding is not None, then the stream will\n",
      "     |      automatically decode the file's contents into unicode.\n",
      "     |      \n",
      "     |      :param file: The file identifier of the file to read.\n",
      "     |  \n",
      "     |  unicode_repr = __repr__(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  root\n",
      "     |      The directory where this corpus is stored.\n",
      "     |      \n",
      "     |      :type: PathPointer\n",
      "    \n",
      "    class Pl196xCorpusReader(nltk.corpus.reader.api.CategorizedCorpusReader, nltk.corpus.reader.xmldocs.XMLCorpusReader)\n",
      "     |  Pl196xCorpusReader(*args, **kwargs)\n",
      "     |  \n",
      "     |  A mixin class used to aid in the implementation of corpus readers\n",
      "     |  for categorized corpora.  This class defines the method\n",
      "     |  ``categories()``, which returns a list of the categories for the\n",
      "     |  corpus or for a specified set of fileids; and overrides ``fileids()``\n",
      "     |  to take a ``categories`` argument, restricting the set of fileids to\n",
      "     |  be returned.\n",
      "     |  \n",
      "     |  Subclasses are expected to:\n",
      "     |  \n",
      "     |    - Call ``__init__()`` to set up the mapping.\n",
      "     |  \n",
      "     |    - Override all view methods to accept a ``categories`` parameter,\n",
      "     |      which can be used *instead* of the ``fileids`` parameter, to\n",
      "     |      select which fileids should be included in the returned view.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Pl196xCorpusReader\n",
      "     |      nltk.corpus.reader.api.CategorizedCorpusReader\n",
      "     |      nltk.corpus.reader.xmldocs.XMLCorpusReader\n",
      "     |      nltk.corpus.reader.api.CorpusReader\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, *args, **kwargs)\n",
      "     |      Initialize this mapping based on keyword arguments, as\n",
      "     |      follows:\n",
      "     |      \n",
      "     |        - cat_pattern: A regular expression pattern used to find the\n",
      "     |          category for each file identifier.  The pattern will be\n",
      "     |          applied to each file identifier, and the first matching\n",
      "     |          group will be used as the category label for that file.\n",
      "     |      \n",
      "     |        - cat_map: A dictionary, mapping from file identifiers to\n",
      "     |          category labels.\n",
      "     |      \n",
      "     |        - cat_file: The name of a file that contains the mapping\n",
      "     |          from file identifiers to categories.  The argument\n",
      "     |          ``cat_delimiter`` can be used to specify a delimiter.\n",
      "     |      \n",
      "     |      The corresponding argument will be deleted from ``kwargs``.  If\n",
      "     |      more than one argument is specified, an exception will be\n",
      "     |      raised.\n",
      "     |  \n",
      "     |  decode_tag(self, tag)\n",
      "     |  \n",
      "     |  paras(self, fileids=None, categories=None, textids=None)\n",
      "     |  \n",
      "     |  raw(self, fileids=None, categories=None)\n",
      "     |  \n",
      "     |  sents(self, fileids=None, categories=None, textids=None)\n",
      "     |  \n",
      "     |  tagged_paras(self, fileids=None, categories=None, textids=None)\n",
      "     |  \n",
      "     |  tagged_sents(self, fileids=None, categories=None, textids=None)\n",
      "     |  \n",
      "     |  tagged_words(self, fileids=None, categories=None, textids=None)\n",
      "     |  \n",
      "     |  textids(self, fileids=None, categories=None)\n",
      "     |      In the pl196x corpus each category is stored in single\n",
      "     |      file and thus both methods provide identical functionality. In order\n",
      "     |      to accommodate finer granularity, a non-standard textids() method was\n",
      "     |      implemented. All the main functions can be supplied with a list\n",
      "     |      of required chunks---giving much more control to the user.\n",
      "     |  \n",
      "     |  words(self, fileids=None, categories=None, textids=None)\n",
      "     |      Returns all of the words and punctuation symbols in the specified file\n",
      "     |      that were in text nodes -- ie, tags are ignored. Like the xml() method,\n",
      "     |      fileid can only specify one file.\n",
      "     |      \n",
      "     |      :return: the given file's text nodes as a list of words and punctuation symbols\n",
      "     |      :rtype: list(str)\n",
      "     |  \n",
      "     |  xml(self, fileids=None, categories=None)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  head_len = 2770\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from nltk.corpus.reader.api.CategorizedCorpusReader:\n",
      "     |  \n",
      "     |  categories(self, fileids=None)\n",
      "     |      Return a list of the categories that are defined for this corpus,\n",
      "     |      or for the file(s) if it is given.\n",
      "     |  \n",
      "     |  fileids(self, categories=None)\n",
      "     |      Return a list of file identifiers for the files that make up\n",
      "     |      this corpus, or that make up the given category(s) if specified.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from nltk.corpus.reader.api.CategorizedCorpusReader:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __unicode__ = __str__(self, /)\n",
      "     |  \n",
      "     |  abspath(self, fileid)\n",
      "     |      Return the absolute path for the given file.\n",
      "     |      \n",
      "     |      :type fileid: str\n",
      "     |      :param fileid: The file identifier for the file whose path\n",
      "     |          should be returned.\n",
      "     |      :rtype: PathPointer\n",
      "     |  \n",
      "     |  abspaths(self, fileids=None, include_encoding=False, include_fileid=False)\n",
      "     |      Return a list of the absolute paths for all fileids in this corpus;\n",
      "     |      or for the given list of fileids, if specified.\n",
      "     |      \n",
      "     |      :type fileids: None or str or list\n",
      "     |      :param fileids: Specifies the set of fileids for which paths should\n",
      "     |          be returned.  Can be None, for all fileids; a list of\n",
      "     |          file identifiers, for a specified set of fileids; or a single\n",
      "     |          file identifier, for a single file.  Note that the return\n",
      "     |          value is always a list of paths, even if ``fileids`` is a\n",
      "     |          single file identifier.\n",
      "     |      \n",
      "     |      :param include_encoding: If true, then return a list of\n",
      "     |          ``(path_pointer, encoding)`` tuples.\n",
      "     |      \n",
      "     |      :rtype: list(PathPointer)\n",
      "     |  \n",
      "     |  citation(self)\n",
      "     |      Return the contents of the corpus citation.bib file, if it exists.\n",
      "     |  \n",
      "     |  encoding(self, file)\n",
      "     |      Return the unicode encoding for the given corpus file, if known.\n",
      "     |      If the encoding is unknown, or if the given file should be\n",
      "     |      processed using byte strings (str), then return None.\n",
      "     |  \n",
      "     |  ensure_loaded(self)\n",
      "     |      Load this corpus (if it has not already been loaded).  This is\n",
      "     |      used by LazyCorpusLoader as a simple method that can be used to\n",
      "     |      make sure a corpus is loaded -- e.g., in case a user wants to\n",
      "     |      do help(some_corpus).\n",
      "     |  \n",
      "     |  license(self)\n",
      "     |      Return the contents of the corpus LICENSE file, if it exists.\n",
      "     |  \n",
      "     |  open(self, file)\n",
      "     |      Return an open stream that can be used to read the given file.\n",
      "     |      If the file's encoding is not None, then the stream will\n",
      "     |      automatically decode the file's contents into unicode.\n",
      "     |      \n",
      "     |      :param file: The file identifier of the file to read.\n",
      "     |  \n",
      "     |  readme(self)\n",
      "     |      Return the contents of the corpus README file, if it exists.\n",
      "     |  \n",
      "     |  unicode_repr = __repr__(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  root\n",
      "     |      The directory where this corpus is stored.\n",
      "     |      \n",
      "     |      :type: PathPointer\n",
      "    \n",
      "    class PlaintextCorpusReader(nltk.corpus.reader.api.CorpusReader)\n",
      "     |  PlaintextCorpusReader(root, fileids, word_tokenizer=WordPunctTokenizer(pattern='\\\\w+|[^\\\\w\\\\s]+', gaps=False, discard_empty=True, flags=<RegexFlag.UNICODE|DOTALL|MULTILINE: 56>), sent_tokenizer=<nltk.tokenize.punkt.PunktSentenceTokenizer object at 0x125903c50>, para_block_reader=<function read_blankline_block at 0x125911320>, encoding='utf8')\n",
      "     |  \n",
      "     |  Reader for corpora that consist of plaintext documents.  Paragraphs\n",
      "     |  are assumed to be split using blank lines.  Sentences and words can\n",
      "     |  be tokenized using the default tokenizers, or by custom tokenizers\n",
      "     |  specificed as parameters to the constructor.\n",
      "     |  \n",
      "     |  This corpus reader can be customized (e.g., to skip preface\n",
      "     |  sections of specific document formats) by creating a subclass and\n",
      "     |  overriding the ``CorpusView`` class variable.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      PlaintextCorpusReader\n",
      "     |      nltk.corpus.reader.api.CorpusReader\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, root, fileids, word_tokenizer=WordPunctTokenizer(pattern='\\\\w+|[^\\\\w\\\\s]+', gaps=False, discard_empty=True, flags=<RegexFlag.UNICODE|DOTALL|MULTILINE: 56>), sent_tokenizer=<nltk.tokenize.punkt.PunktSentenceTokenizer object at 0x125903c50>, para_block_reader=<function read_blankline_block at 0x125911320>, encoding='utf8')\n",
      "     |      Construct a new plaintext corpus reader for a set of documents\n",
      "     |      located at the given root directory.  Example usage:\n",
      "     |      \n",
      "     |          >>> root = '/usr/local/share/nltk_data/corpora/webtext/'\n",
      "     |          >>> reader = PlaintextCorpusReader(root, '.*\\.txt') # doctest: +SKIP\n",
      "     |      \n",
      "     |      :param root: The root directory for this corpus.\n",
      "     |      :param fileids: A list or regexp specifying the fileids in this corpus.\n",
      "     |      :param word_tokenizer: Tokenizer for breaking sentences or\n",
      "     |          paragraphs into words.\n",
      "     |      :param sent_tokenizer: Tokenizer for breaking paragraphs\n",
      "     |          into words.\n",
      "     |      :param para_block_reader: The block reader used to divide the\n",
      "     |          corpus into paragraph blocks.\n",
      "     |  \n",
      "     |  paras(self, fileids=None)\n",
      "     |      :return: the given file(s) as a list of\n",
      "     |          paragraphs, each encoded as a list of sentences, which are\n",
      "     |          in turn encoded as lists of word strings.\n",
      "     |      :rtype: list(list(list(str)))\n",
      "     |  \n",
      "     |  raw(self, fileids=None)\n",
      "     |      :return: the given file(s) as a single string.\n",
      "     |      :rtype: str\n",
      "     |  \n",
      "     |  sents(self, fileids=None)\n",
      "     |      :return: the given file(s) as a list of\n",
      "     |          sentences or utterances, each encoded as a list of word\n",
      "     |          strings.\n",
      "     |      :rtype: list(list(str))\n",
      "     |  \n",
      "     |  words(self, fileids=None)\n",
      "     |      :return: the given file(s) as a list of words\n",
      "     |          and punctuation symbols.\n",
      "     |      :rtype: list(str)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  CorpusView = <class 'nltk.corpus.reader.util.StreamBackedCorpusView'>\n",
      "     |      A 'view' of a corpus file, which acts like a sequence of tokens:\n",
      "     |      it can be accessed by index, iterated over, etc.  However, the\n",
      "     |      tokens are only constructed as-needed -- the entire corpus is\n",
      "     |      never stored in memory at once.\n",
      "     |      \n",
      "     |      The constructor to ``StreamBackedCorpusView`` takes two arguments:\n",
      "     |      a corpus fileid (specified as a string or as a ``PathPointer``);\n",
      "     |      and a block reader.  A \"block reader\" is a function that reads\n",
      "     |      zero or more tokens from a stream, and returns them as a list.  A\n",
      "     |      very simple example of a block reader is:\n",
      "     |      \n",
      "     |          >>> def simple_block_reader(stream):\n",
      "     |          ...     return stream.readline().split()\n",
      "     |      \n",
      "     |      This simple block reader reads a single line at a time, and\n",
      "     |      returns a single token (consisting of a string) for each\n",
      "     |      whitespace-separated substring on the line.\n",
      "     |      \n",
      "     |      When deciding how to define the block reader for a given\n",
      "     |      corpus, careful consideration should be given to the size of\n",
      "     |      blocks handled by the block reader.  Smaller block sizes will\n",
      "     |      increase the memory requirements of the corpus view's internal\n",
      "     |      data structures (by 2 integers per block).  On the other hand,\n",
      "     |      larger block sizes may decrease performance for random access to\n",
      "     |      the corpus.  (But note that larger block sizes will *not*\n",
      "     |      decrease performance for iteration.)\n",
      "     |      \n",
      "     |      Internally, ``CorpusView`` maintains a partial mapping from token\n",
      "     |      index to file position, with one entry per block.  When a token\n",
      "     |      with a given index *i* is requested, the ``CorpusView`` constructs\n",
      "     |      it as follows:\n",
      "     |      \n",
      "     |        1. First, it searches the toknum/filepos mapping for the token\n",
      "     |           index closest to (but less than or equal to) *i*.\n",
      "     |      \n",
      "     |        2. Then, starting at the file position corresponding to that\n",
      "     |           index, it reads one block at a time using the block reader\n",
      "     |           until it reaches the requested token.\n",
      "     |      \n",
      "     |      The toknum/filepos mapping is created lazily: it is initially\n",
      "     |      empty, but every time a new block is read, the block's\n",
      "     |      initial token is added to the mapping.  (Thus, the toknum/filepos\n",
      "     |      map has one entry per block.)\n",
      "     |      \n",
      "     |      In order to increase efficiency for random access patterns that\n",
      "     |      have high degrees of locality, the corpus view may cache one or\n",
      "     |      more blocks.\n",
      "     |      \n",
      "     |      :note: Each ``CorpusView`` object internally maintains an open file\n",
      "     |          object for its underlying corpus file.  This file should be\n",
      "     |          automatically closed when the ``CorpusView`` is garbage collected,\n",
      "     |          but if you wish to close it manually, use the ``close()``\n",
      "     |          method.  If you access a ``CorpusView``'s items after it has been\n",
      "     |          closed, the file object will be automatically re-opened.\n",
      "     |      \n",
      "     |      :warning: If the contents of the file are modified during the\n",
      "     |          lifetime of the ``CorpusView``, then the ``CorpusView``'s behavior\n",
      "     |          is undefined.\n",
      "     |      \n",
      "     |      :warning: If a unicode encoding is specified when constructing a\n",
      "     |          ``CorpusView``, then the block reader may only call\n",
      "     |          ``stream.seek()`` with offsets that have been returned by\n",
      "     |          ``stream.tell()``; in particular, calling ``stream.seek()`` with\n",
      "     |          relative offsets, or with offsets based on string lengths, may\n",
      "     |          lead to incorrect behavior.\n",
      "     |      \n",
      "     |      :ivar _block_reader: The function used to read\n",
      "     |          a single block from the underlying file stream.\n",
      "     |      :ivar _toknum: A list containing the token index of each block\n",
      "     |          that has been processed.  In particular, ``_toknum[i]`` is the\n",
      "     |          token index of the first token in block ``i``.  Together\n",
      "     |          with ``_filepos``, this forms a partial mapping between token\n",
      "     |          indices and file positions.\n",
      "     |      :ivar _filepos: A list containing the file position of each block\n",
      "     |          that has been processed.  In particular, ``_toknum[i]`` is the\n",
      "     |          file position of the first character in block ``i``.  Together\n",
      "     |          with ``_toknum``, this forms a partial mapping between token\n",
      "     |          indices and file positions.\n",
      "     |      :ivar _stream: The stream used to access the underlying corpus file.\n",
      "     |      :ivar _len: The total number of tokens in the corpus, if known;\n",
      "     |          or None, if the number of tokens is not yet known.\n",
      "     |      :ivar _eofpos: The character position of the last character in the\n",
      "     |          file.  This is calculated when the corpus view is initialized,\n",
      "     |          and is used to decide when the end of file has been reached.\n",
      "     |      :ivar _cache: A cache of the most recently read block.  It\n",
      "     |         is encoded as a tuple (start_toknum, end_toknum, tokens), where\n",
      "     |         start_toknum is the token index of the first token in the block;\n",
      "     |         end_toknum is the token index of the first token not in the\n",
      "     |         block; and tokens is a list of the tokens in the block.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __unicode__ = __str__(self, /)\n",
      "     |  \n",
      "     |  abspath(self, fileid)\n",
      "     |      Return the absolute path for the given file.\n",
      "     |      \n",
      "     |      :type fileid: str\n",
      "     |      :param fileid: The file identifier for the file whose path\n",
      "     |          should be returned.\n",
      "     |      :rtype: PathPointer\n",
      "     |  \n",
      "     |  abspaths(self, fileids=None, include_encoding=False, include_fileid=False)\n",
      "     |      Return a list of the absolute paths for all fileids in this corpus;\n",
      "     |      or for the given list of fileids, if specified.\n",
      "     |      \n",
      "     |      :type fileids: None or str or list\n",
      "     |      :param fileids: Specifies the set of fileids for which paths should\n",
      "     |          be returned.  Can be None, for all fileids; a list of\n",
      "     |          file identifiers, for a specified set of fileids; or a single\n",
      "     |          file identifier, for a single file.  Note that the return\n",
      "     |          value is always a list of paths, even if ``fileids`` is a\n",
      "     |          single file identifier.\n",
      "     |      \n",
      "     |      :param include_encoding: If true, then return a list of\n",
      "     |          ``(path_pointer, encoding)`` tuples.\n",
      "     |      \n",
      "     |      :rtype: list(PathPointer)\n",
      "     |  \n",
      "     |  citation(self)\n",
      "     |      Return the contents of the corpus citation.bib file, if it exists.\n",
      "     |  \n",
      "     |  encoding(self, file)\n",
      "     |      Return the unicode encoding for the given corpus file, if known.\n",
      "     |      If the encoding is unknown, or if the given file should be\n",
      "     |      processed using byte strings (str), then return None.\n",
      "     |  \n",
      "     |  ensure_loaded(self)\n",
      "     |      Load this corpus (if it has not already been loaded).  This is\n",
      "     |      used by LazyCorpusLoader as a simple method that can be used to\n",
      "     |      make sure a corpus is loaded -- e.g., in case a user wants to\n",
      "     |      do help(some_corpus).\n",
      "     |  \n",
      "     |  fileids(self)\n",
      "     |      Return a list of file identifiers for the fileids that make up\n",
      "     |      this corpus.\n",
      "     |  \n",
      "     |  license(self)\n",
      "     |      Return the contents of the corpus LICENSE file, if it exists.\n",
      "     |  \n",
      "     |  open(self, file)\n",
      "     |      Return an open stream that can be used to read the given file.\n",
      "     |      If the file's encoding is not None, then the stream will\n",
      "     |      automatically decode the file's contents into unicode.\n",
      "     |      \n",
      "     |      :param file: The file identifier of the file to read.\n",
      "     |  \n",
      "     |  readme(self)\n",
      "     |      Return the contents of the corpus README file, if it exists.\n",
      "     |  \n",
      "     |  unicode_repr = __repr__(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  root\n",
      "     |      The directory where this corpus is stored.\n",
      "     |      \n",
      "     |      :type: PathPointer\n",
      "    \n",
      "    class PortugueseCategorizedPlaintextCorpusReader(CategorizedPlaintextCorpusReader)\n",
      "     |  PortugueseCategorizedPlaintextCorpusReader(*args, **kwargs)\n",
      "     |  \n",
      "     |  A reader for plaintext corpora whose documents are divided into\n",
      "     |  categories based on their file identifiers.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      PortugueseCategorizedPlaintextCorpusReader\n",
      "     |      CategorizedPlaintextCorpusReader\n",
      "     |      nltk.corpus.reader.api.CategorizedCorpusReader\n",
      "     |      PlaintextCorpusReader\n",
      "     |      nltk.corpus.reader.api.CorpusReader\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, *args, **kwargs)\n",
      "     |      Initialize the corpus reader.  Categorization arguments\n",
      "     |      (``cat_pattern``, ``cat_map``, and ``cat_file``) are passed to\n",
      "     |      the ``CategorizedCorpusReader`` constructor.  The remaining arguments\n",
      "     |      are passed to the ``PlaintextCorpusReader`` constructor.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from CategorizedPlaintextCorpusReader:\n",
      "     |  \n",
      "     |  paras(self, fileids=None, categories=None)\n",
      "     |      :return: the given file(s) as a list of\n",
      "     |          paragraphs, each encoded as a list of sentences, which are\n",
      "     |          in turn encoded as lists of word strings.\n",
      "     |      :rtype: list(list(list(str)))\n",
      "     |  \n",
      "     |  raw(self, fileids=None, categories=None)\n",
      "     |      :return: the given file(s) as a single string.\n",
      "     |      :rtype: str\n",
      "     |  \n",
      "     |  sents(self, fileids=None, categories=None)\n",
      "     |      :return: the given file(s) as a list of\n",
      "     |          sentences or utterances, each encoded as a list of word\n",
      "     |          strings.\n",
      "     |      :rtype: list(list(str))\n",
      "     |  \n",
      "     |  words(self, fileids=None, categories=None)\n",
      "     |      :return: the given file(s) as a list of words\n",
      "     |          and punctuation symbols.\n",
      "     |      :rtype: list(str)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from nltk.corpus.reader.api.CategorizedCorpusReader:\n",
      "     |  \n",
      "     |  categories(self, fileids=None)\n",
      "     |      Return a list of the categories that are defined for this corpus,\n",
      "     |      or for the file(s) if it is given.\n",
      "     |  \n",
      "     |  fileids(self, categories=None)\n",
      "     |      Return a list of file identifiers for the files that make up\n",
      "     |      this corpus, or that make up the given category(s) if specified.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from nltk.corpus.reader.api.CategorizedCorpusReader:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from PlaintextCorpusReader:\n",
      "     |  \n",
      "     |  CorpusView = <class 'nltk.corpus.reader.util.StreamBackedCorpusView'>\n",
      "     |      A 'view' of a corpus file, which acts like a sequence of tokens:\n",
      "     |      it can be accessed by index, iterated over, etc.  However, the\n",
      "     |      tokens are only constructed as-needed -- the entire corpus is\n",
      "     |      never stored in memory at once.\n",
      "     |      \n",
      "     |      The constructor to ``StreamBackedCorpusView`` takes two arguments:\n",
      "     |      a corpus fileid (specified as a string or as a ``PathPointer``);\n",
      "     |      and a block reader.  A \"block reader\" is a function that reads\n",
      "     |      zero or more tokens from a stream, and returns them as a list.  A\n",
      "     |      very simple example of a block reader is:\n",
      "     |      \n",
      "     |          >>> def simple_block_reader(stream):\n",
      "     |          ...     return stream.readline().split()\n",
      "     |      \n",
      "     |      This simple block reader reads a single line at a time, and\n",
      "     |      returns a single token (consisting of a string) for each\n",
      "     |      whitespace-separated substring on the line.\n",
      "     |      \n",
      "     |      When deciding how to define the block reader for a given\n",
      "     |      corpus, careful consideration should be given to the size of\n",
      "     |      blocks handled by the block reader.  Smaller block sizes will\n",
      "     |      increase the memory requirements of the corpus view's internal\n",
      "     |      data structures (by 2 integers per block).  On the other hand,\n",
      "     |      larger block sizes may decrease performance for random access to\n",
      "     |      the corpus.  (But note that larger block sizes will *not*\n",
      "     |      decrease performance for iteration.)\n",
      "     |      \n",
      "     |      Internally, ``CorpusView`` maintains a partial mapping from token\n",
      "     |      index to file position, with one entry per block.  When a token\n",
      "     |      with a given index *i* is requested, the ``CorpusView`` constructs\n",
      "     |      it as follows:\n",
      "     |      \n",
      "     |        1. First, it searches the toknum/filepos mapping for the token\n",
      "     |           index closest to (but less than or equal to) *i*.\n",
      "     |      \n",
      "     |        2. Then, starting at the file position corresponding to that\n",
      "     |           index, it reads one block at a time using the block reader\n",
      "     |           until it reaches the requested token.\n",
      "     |      \n",
      "     |      The toknum/filepos mapping is created lazily: it is initially\n",
      "     |      empty, but every time a new block is read, the block's\n",
      "     |      initial token is added to the mapping.  (Thus, the toknum/filepos\n",
      "     |      map has one entry per block.)\n",
      "     |      \n",
      "     |      In order to increase efficiency for random access patterns that\n",
      "     |      have high degrees of locality, the corpus view may cache one or\n",
      "     |      more blocks.\n",
      "     |      \n",
      "     |      :note: Each ``CorpusView`` object internally maintains an open file\n",
      "     |          object for its underlying corpus file.  This file should be\n",
      "     |          automatically closed when the ``CorpusView`` is garbage collected,\n",
      "     |          but if you wish to close it manually, use the ``close()``\n",
      "     |          method.  If you access a ``CorpusView``'s items after it has been\n",
      "     |          closed, the file object will be automatically re-opened.\n",
      "     |      \n",
      "     |      :warning: If the contents of the file are modified during the\n",
      "     |          lifetime of the ``CorpusView``, then the ``CorpusView``'s behavior\n",
      "     |          is undefined.\n",
      "     |      \n",
      "     |      :warning: If a unicode encoding is specified when constructing a\n",
      "     |          ``CorpusView``, then the block reader may only call\n",
      "     |          ``stream.seek()`` with offsets that have been returned by\n",
      "     |          ``stream.tell()``; in particular, calling ``stream.seek()`` with\n",
      "     |          relative offsets, or with offsets based on string lengths, may\n",
      "     |          lead to incorrect behavior.\n",
      "     |      \n",
      "     |      :ivar _block_reader: The function used to read\n",
      "     |          a single block from the underlying file stream.\n",
      "     |      :ivar _toknum: A list containing the token index of each block\n",
      "     |          that has been processed.  In particular, ``_toknum[i]`` is the\n",
      "     |          token index of the first token in block ``i``.  Together\n",
      "     |          with ``_filepos``, this forms a partial mapping between token\n",
      "     |          indices and file positions.\n",
      "     |      :ivar _filepos: A list containing the file position of each block\n",
      "     |          that has been processed.  In particular, ``_toknum[i]`` is the\n",
      "     |          file position of the first character in block ``i``.  Together\n",
      "     |          with ``_toknum``, this forms a partial mapping between token\n",
      "     |          indices and file positions.\n",
      "     |      :ivar _stream: The stream used to access the underlying corpus file.\n",
      "     |      :ivar _len: The total number of tokens in the corpus, if known;\n",
      "     |          or None, if the number of tokens is not yet known.\n",
      "     |      :ivar _eofpos: The character position of the last character in the\n",
      "     |          file.  This is calculated when the corpus view is initialized,\n",
      "     |          and is used to decide when the end of file has been reached.\n",
      "     |      :ivar _cache: A cache of the most recently read block.  It\n",
      "     |         is encoded as a tuple (start_toknum, end_toknum, tokens), where\n",
      "     |         start_toknum is the token index of the first token in the block;\n",
      "     |         end_toknum is the token index of the first token not in the\n",
      "     |         block; and tokens is a list of the tokens in the block.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __unicode__ = __str__(self, /)\n",
      "     |  \n",
      "     |  abspath(self, fileid)\n",
      "     |      Return the absolute path for the given file.\n",
      "     |      \n",
      "     |      :type fileid: str\n",
      "     |      :param fileid: The file identifier for the file whose path\n",
      "     |          should be returned.\n",
      "     |      :rtype: PathPointer\n",
      "     |  \n",
      "     |  abspaths(self, fileids=None, include_encoding=False, include_fileid=False)\n",
      "     |      Return a list of the absolute paths for all fileids in this corpus;\n",
      "     |      or for the given list of fileids, if specified.\n",
      "     |      \n",
      "     |      :type fileids: None or str or list\n",
      "     |      :param fileids: Specifies the set of fileids for which paths should\n",
      "     |          be returned.  Can be None, for all fileids; a list of\n",
      "     |          file identifiers, for a specified set of fileids; or a single\n",
      "     |          file identifier, for a single file.  Note that the return\n",
      "     |          value is always a list of paths, even if ``fileids`` is a\n",
      "     |          single file identifier.\n",
      "     |      \n",
      "     |      :param include_encoding: If true, then return a list of\n",
      "     |          ``(path_pointer, encoding)`` tuples.\n",
      "     |      \n",
      "     |      :rtype: list(PathPointer)\n",
      "     |  \n",
      "     |  citation(self)\n",
      "     |      Return the contents of the corpus citation.bib file, if it exists.\n",
      "     |  \n",
      "     |  encoding(self, file)\n",
      "     |      Return the unicode encoding for the given corpus file, if known.\n",
      "     |      If the encoding is unknown, or if the given file should be\n",
      "     |      processed using byte strings (str), then return None.\n",
      "     |  \n",
      "     |  ensure_loaded(self)\n",
      "     |      Load this corpus (if it has not already been loaded).  This is\n",
      "     |      used by LazyCorpusLoader as a simple method that can be used to\n",
      "     |      make sure a corpus is loaded -- e.g., in case a user wants to\n",
      "     |      do help(some_corpus).\n",
      "     |  \n",
      "     |  license(self)\n",
      "     |      Return the contents of the corpus LICENSE file, if it exists.\n",
      "     |  \n",
      "     |  open(self, file)\n",
      "     |      Return an open stream that can be used to read the given file.\n",
      "     |      If the file's encoding is not None, then the stream will\n",
      "     |      automatically decode the file's contents into unicode.\n",
      "     |      \n",
      "     |      :param file: The file identifier of the file to read.\n",
      "     |  \n",
      "     |  readme(self)\n",
      "     |      Return the contents of the corpus README file, if it exists.\n",
      "     |  \n",
      "     |  unicode_repr = __repr__(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  root\n",
      "     |      The directory where this corpus is stored.\n",
      "     |      \n",
      "     |      :type: PathPointer\n",
      "    \n",
      "    class PropbankCorpusReader(nltk.corpus.reader.api.CorpusReader)\n",
      "     |  PropbankCorpusReader(root, propfile, framefiles='', verbsfile=None, parse_fileid_xform=None, parse_corpus=None, encoding='utf8')\n",
      "     |  \n",
      "     |  Corpus reader for the propbank corpus, which augments the Penn\n",
      "     |  Treebank with information about the predicate argument structure\n",
      "     |  of every verb instance.  The corpus consists of two parts: the\n",
      "     |  predicate-argument annotations themselves, and a set of \"frameset\n",
      "     |  files\" which define the argument labels used by the annotations,\n",
      "     |  on a per-verb basis.  Each \"frameset file\" contains one or more\n",
      "     |  predicates, such as ``'turn'`` or ``'turn_on'``, each of which is\n",
      "     |  divided into coarse-grained word senses called \"rolesets\".  For\n",
      "     |  each \"roleset\", the frameset file provides descriptions of the\n",
      "     |  argument roles, along with examples.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      PropbankCorpusReader\n",
      "     |      nltk.corpus.reader.api.CorpusReader\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, root, propfile, framefiles='', verbsfile=None, parse_fileid_xform=None, parse_corpus=None, encoding='utf8')\n",
      "     |      :param root: The root directory for this corpus.\n",
      "     |      :param propfile: The name of the file containing the predicate-\n",
      "     |          argument annotations (relative to ``root``).\n",
      "     |      :param framefiles: A list or regexp specifying the frameset\n",
      "     |          fileids for this corpus.\n",
      "     |      :param parse_fileid_xform: A transform that should be applied\n",
      "     |          to the fileids in this corpus.  This should be a function\n",
      "     |          of one argument (a fileid) that returns a string (the new\n",
      "     |          fileid).\n",
      "     |      :param parse_corpus: The corpus containing the parse trees\n",
      "     |          corresponding to this corpus.  These parse trees are\n",
      "     |          necessary to resolve the tree pointers used by propbank.\n",
      "     |  \n",
      "     |  instances(self, baseform=None)\n",
      "     |      :return: a corpus view that acts as a list of\n",
      "     |      ``PropBankInstance`` objects, one for each noun in the corpus.\n",
      "     |  \n",
      "     |  lines(self)\n",
      "     |      :return: a corpus view that acts as a list of strings, one for\n",
      "     |      each line in the predicate-argument annotation file.\n",
      "     |  \n",
      "     |  raw(self, fileids=None)\n",
      "     |      :return: the text contents of the given fileids, as a single string.\n",
      "     |  \n",
      "     |  roleset(self, roleset_id)\n",
      "     |      :return: the xml description for the given roleset.\n",
      "     |  \n",
      "     |  rolesets(self, baseform=None)\n",
      "     |      :return: list of xml descriptions for rolesets.\n",
      "     |  \n",
      "     |  verbs(self)\n",
      "     |      :return: a corpus view that acts as a list of all verb lemmas\n",
      "     |      in this corpus (from the verbs.txt file).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __unicode__ = __str__(self, /)\n",
      "     |  \n",
      "     |  abspath(self, fileid)\n",
      "     |      Return the absolute path for the given file.\n",
      "     |      \n",
      "     |      :type fileid: str\n",
      "     |      :param fileid: The file identifier for the file whose path\n",
      "     |          should be returned.\n",
      "     |      :rtype: PathPointer\n",
      "     |  \n",
      "     |  abspaths(self, fileids=None, include_encoding=False, include_fileid=False)\n",
      "     |      Return a list of the absolute paths for all fileids in this corpus;\n",
      "     |      or for the given list of fileids, if specified.\n",
      "     |      \n",
      "     |      :type fileids: None or str or list\n",
      "     |      :param fileids: Specifies the set of fileids for which paths should\n",
      "     |          be returned.  Can be None, for all fileids; a list of\n",
      "     |          file identifiers, for a specified set of fileids; or a single\n",
      "     |          file identifier, for a single file.  Note that the return\n",
      "     |          value is always a list of paths, even if ``fileids`` is a\n",
      "     |          single file identifier.\n",
      "     |      \n",
      "     |      :param include_encoding: If true, then return a list of\n",
      "     |          ``(path_pointer, encoding)`` tuples.\n",
      "     |      \n",
      "     |      :rtype: list(PathPointer)\n",
      "     |  \n",
      "     |  citation(self)\n",
      "     |      Return the contents of the corpus citation.bib file, if it exists.\n",
      "     |  \n",
      "     |  encoding(self, file)\n",
      "     |      Return the unicode encoding for the given corpus file, if known.\n",
      "     |      If the encoding is unknown, or if the given file should be\n",
      "     |      processed using byte strings (str), then return None.\n",
      "     |  \n",
      "     |  ensure_loaded(self)\n",
      "     |      Load this corpus (if it has not already been loaded).  This is\n",
      "     |      used by LazyCorpusLoader as a simple method that can be used to\n",
      "     |      make sure a corpus is loaded -- e.g., in case a user wants to\n",
      "     |      do help(some_corpus).\n",
      "     |  \n",
      "     |  fileids(self)\n",
      "     |      Return a list of file identifiers for the fileids that make up\n",
      "     |      this corpus.\n",
      "     |  \n",
      "     |  license(self)\n",
      "     |      Return the contents of the corpus LICENSE file, if it exists.\n",
      "     |  \n",
      "     |  open(self, file)\n",
      "     |      Return an open stream that can be used to read the given file.\n",
      "     |      If the file's encoding is not None, then the stream will\n",
      "     |      automatically decode the file's contents into unicode.\n",
      "     |      \n",
      "     |      :param file: The file identifier of the file to read.\n",
      "     |  \n",
      "     |  readme(self)\n",
      "     |      Return the contents of the corpus README file, if it exists.\n",
      "     |  \n",
      "     |  unicode_repr = __repr__(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  root\n",
      "     |      The directory where this corpus is stored.\n",
      "     |      \n",
      "     |      :type: PathPointer\n",
      "    \n",
      "    class ProsConsCorpusReader(nltk.corpus.reader.api.CategorizedCorpusReader, nltk.corpus.reader.api.CorpusReader)\n",
      "     |  ProsConsCorpusReader(root, fileids, word_tokenizer=WordPunctTokenizer(pattern='\\\\w+|[^\\\\w\\\\s]+', gaps=False, discard_empty=True, flags=<RegexFlag.UNICODE|DOTALL|MULTILINE: 56>), encoding='utf8', **kwargs)\n",
      "     |  \n",
      "     |  Reader for the Pros and Cons sentence dataset.\n",
      "     |  \n",
      "     |      >>> from nltk.corpus import pros_cons\n",
      "     |      >>> pros_cons.sents(categories='Cons')\n",
      "     |      [['East', 'batteries', '!', 'On', '-', 'off', 'switch', 'too', 'easy',\n",
      "     |      'to', 'maneuver', '.'], ['Eats', '...', 'no', ',', 'GULPS', 'batteries'],\n",
      "     |      ...]\n",
      "     |      >>> pros_cons.words('IntegratedPros.txt')\n",
      "     |      ['Easy', 'to', 'use', ',', 'economical', '!', ...]\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      ProsConsCorpusReader\n",
      "     |      nltk.corpus.reader.api.CategorizedCorpusReader\n",
      "     |      nltk.corpus.reader.api.CorpusReader\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, root, fileids, word_tokenizer=WordPunctTokenizer(pattern='\\\\w+|[^\\\\w\\\\s]+', gaps=False, discard_empty=True, flags=<RegexFlag.UNICODE|DOTALL|MULTILINE: 56>), encoding='utf8', **kwargs)\n",
      "     |      :param root: The root directory for the corpus.\n",
      "     |      :param fileids: a list or regexp specifying the fileids in the corpus.\n",
      "     |      :param word_tokenizer: a tokenizer for breaking sentences or paragraphs\n",
      "     |          into words. Default: `WhitespaceTokenizer`\n",
      "     |      :param encoding: the encoding that should be used to read the corpus.\n",
      "     |      :param kwargs: additional parameters passed to CategorizedCorpusReader.\n",
      "     |  \n",
      "     |  sents(self, fileids=None, categories=None)\n",
      "     |      Return all sentences in the corpus or in the specified files/categories.\n",
      "     |      \n",
      "     |      :param fileids: a list or regexp specifying the ids of the files whose\n",
      "     |          sentences have to be returned.\n",
      "     |      :param categories: a list specifying the categories whose sentences\n",
      "     |          have to be returned.\n",
      "     |      :return: the given file(s) as a list of sentences. Each sentence is\n",
      "     |          tokenized using the specified word_tokenizer.\n",
      "     |      :rtype: list(list(str))\n",
      "     |  \n",
      "     |  words(self, fileids=None, categories=None)\n",
      "     |      Return all words and punctuation symbols in the corpus or in the specified\n",
      "     |      files/categories.\n",
      "     |      \n",
      "     |      :param fileids: a list or regexp specifying the ids of the files whose\n",
      "     |          words have to be returned.\n",
      "     |      :param categories: a list specifying the categories whose words have\n",
      "     |          to be returned.\n",
      "     |      :return: the given file(s) as a list of words and punctuation symbols.\n",
      "     |      :rtype: list(str)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  CorpusView = <class 'nltk.corpus.reader.util.StreamBackedCorpusView'>\n",
      "     |      A 'view' of a corpus file, which acts like a sequence of tokens:\n",
      "     |      it can be accessed by index, iterated over, etc.  However, the\n",
      "     |      tokens are only constructed as-needed -- the entire corpus is\n",
      "     |      never stored in memory at once.\n",
      "     |      \n",
      "     |      The constructor to ``StreamBackedCorpusView`` takes two arguments:\n",
      "     |      a corpus fileid (specified as a string or as a ``PathPointer``);\n",
      "     |      and a block reader.  A \"block reader\" is a function that reads\n",
      "     |      zero or more tokens from a stream, and returns them as a list.  A\n",
      "     |      very simple example of a block reader is:\n",
      "     |      \n",
      "     |          >>> def simple_block_reader(stream):\n",
      "     |          ...     return stream.readline().split()\n",
      "     |      \n",
      "     |      This simple block reader reads a single line at a time, and\n",
      "     |      returns a single token (consisting of a string) for each\n",
      "     |      whitespace-separated substring on the line.\n",
      "     |      \n",
      "     |      When deciding how to define the block reader for a given\n",
      "     |      corpus, careful consideration should be given to the size of\n",
      "     |      blocks handled by the block reader.  Smaller block sizes will\n",
      "     |      increase the memory requirements of the corpus view's internal\n",
      "     |      data structures (by 2 integers per block).  On the other hand,\n",
      "     |      larger block sizes may decrease performance for random access to\n",
      "     |      the corpus.  (But note that larger block sizes will *not*\n",
      "     |      decrease performance for iteration.)\n",
      "     |      \n",
      "     |      Internally, ``CorpusView`` maintains a partial mapping from token\n",
      "     |      index to file position, with one entry per block.  When a token\n",
      "     |      with a given index *i* is requested, the ``CorpusView`` constructs\n",
      "     |      it as follows:\n",
      "     |      \n",
      "     |        1. First, it searches the toknum/filepos mapping for the token\n",
      "     |           index closest to (but less than or equal to) *i*.\n",
      "     |      \n",
      "     |        2. Then, starting at the file position corresponding to that\n",
      "     |           index, it reads one block at a time using the block reader\n",
      "     |           until it reaches the requested token.\n",
      "     |      \n",
      "     |      The toknum/filepos mapping is created lazily: it is initially\n",
      "     |      empty, but every time a new block is read, the block's\n",
      "     |      initial token is added to the mapping.  (Thus, the toknum/filepos\n",
      "     |      map has one entry per block.)\n",
      "     |      \n",
      "     |      In order to increase efficiency for random access patterns that\n",
      "     |      have high degrees of locality, the corpus view may cache one or\n",
      "     |      more blocks.\n",
      "     |      \n",
      "     |      :note: Each ``CorpusView`` object internally maintains an open file\n",
      "     |          object for its underlying corpus file.  This file should be\n",
      "     |          automatically closed when the ``CorpusView`` is garbage collected,\n",
      "     |          but if you wish to close it manually, use the ``close()``\n",
      "     |          method.  If you access a ``CorpusView``'s items after it has been\n",
      "     |          closed, the file object will be automatically re-opened.\n",
      "     |      \n",
      "     |      :warning: If the contents of the file are modified during the\n",
      "     |          lifetime of the ``CorpusView``, then the ``CorpusView``'s behavior\n",
      "     |          is undefined.\n",
      "     |      \n",
      "     |      :warning: If a unicode encoding is specified when constructing a\n",
      "     |          ``CorpusView``, then the block reader may only call\n",
      "     |          ``stream.seek()`` with offsets that have been returned by\n",
      "     |          ``stream.tell()``; in particular, calling ``stream.seek()`` with\n",
      "     |          relative offsets, or with offsets based on string lengths, may\n",
      "     |          lead to incorrect behavior.\n",
      "     |      \n",
      "     |      :ivar _block_reader: The function used to read\n",
      "     |          a single block from the underlying file stream.\n",
      "     |      :ivar _toknum: A list containing the token index of each block\n",
      "     |          that has been processed.  In particular, ``_toknum[i]`` is the\n",
      "     |          token index of the first token in block ``i``.  Together\n",
      "     |          with ``_filepos``, this forms a partial mapping between token\n",
      "     |          indices and file positions.\n",
      "     |      :ivar _filepos: A list containing the file position of each block\n",
      "     |          that has been processed.  In particular, ``_toknum[i]`` is the\n",
      "     |          file position of the first character in block ``i``.  Together\n",
      "     |          with ``_toknum``, this forms a partial mapping between token\n",
      "     |          indices and file positions.\n",
      "     |      :ivar _stream: The stream used to access the underlying corpus file.\n",
      "     |      :ivar _len: The total number of tokens in the corpus, if known;\n",
      "     |          or None, if the number of tokens is not yet known.\n",
      "     |      :ivar _eofpos: The character position of the last character in the\n",
      "     |          file.  This is calculated when the corpus view is initialized,\n",
      "     |          and is used to decide when the end of file has been reached.\n",
      "     |      :ivar _cache: A cache of the most recently read block.  It\n",
      "     |         is encoded as a tuple (start_toknum, end_toknum, tokens), where\n",
      "     |         start_toknum is the token index of the first token in the block;\n",
      "     |         end_toknum is the token index of the first token not in the\n",
      "     |         block; and tokens is a list of the tokens in the block.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from nltk.corpus.reader.api.CategorizedCorpusReader:\n",
      "     |  \n",
      "     |  categories(self, fileids=None)\n",
      "     |      Return a list of the categories that are defined for this corpus,\n",
      "     |      or for the file(s) if it is given.\n",
      "     |  \n",
      "     |  fileids(self, categories=None)\n",
      "     |      Return a list of file identifiers for the files that make up\n",
      "     |      this corpus, or that make up the given category(s) if specified.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from nltk.corpus.reader.api.CategorizedCorpusReader:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __unicode__ = __str__(self, /)\n",
      "     |  \n",
      "     |  abspath(self, fileid)\n",
      "     |      Return the absolute path for the given file.\n",
      "     |      \n",
      "     |      :type fileid: str\n",
      "     |      :param fileid: The file identifier for the file whose path\n",
      "     |          should be returned.\n",
      "     |      :rtype: PathPointer\n",
      "     |  \n",
      "     |  abspaths(self, fileids=None, include_encoding=False, include_fileid=False)\n",
      "     |      Return a list of the absolute paths for all fileids in this corpus;\n",
      "     |      or for the given list of fileids, if specified.\n",
      "     |      \n",
      "     |      :type fileids: None or str or list\n",
      "     |      :param fileids: Specifies the set of fileids for which paths should\n",
      "     |          be returned.  Can be None, for all fileids; a list of\n",
      "     |          file identifiers, for a specified set of fileids; or a single\n",
      "     |          file identifier, for a single file.  Note that the return\n",
      "     |          value is always a list of paths, even if ``fileids`` is a\n",
      "     |          single file identifier.\n",
      "     |      \n",
      "     |      :param include_encoding: If true, then return a list of\n",
      "     |          ``(path_pointer, encoding)`` tuples.\n",
      "     |      \n",
      "     |      :rtype: list(PathPointer)\n",
      "     |  \n",
      "     |  citation(self)\n",
      "     |      Return the contents of the corpus citation.bib file, if it exists.\n",
      "     |  \n",
      "     |  encoding(self, file)\n",
      "     |      Return the unicode encoding for the given corpus file, if known.\n",
      "     |      If the encoding is unknown, or if the given file should be\n",
      "     |      processed using byte strings (str), then return None.\n",
      "     |  \n",
      "     |  ensure_loaded(self)\n",
      "     |      Load this corpus (if it has not already been loaded).  This is\n",
      "     |      used by LazyCorpusLoader as a simple method that can be used to\n",
      "     |      make sure a corpus is loaded -- e.g., in case a user wants to\n",
      "     |      do help(some_corpus).\n",
      "     |  \n",
      "     |  license(self)\n",
      "     |      Return the contents of the corpus LICENSE file, if it exists.\n",
      "     |  \n",
      "     |  open(self, file)\n",
      "     |      Return an open stream that can be used to read the given file.\n",
      "     |      If the file's encoding is not None, then the stream will\n",
      "     |      automatically decode the file's contents into unicode.\n",
      "     |      \n",
      "     |      :param file: The file identifier of the file to read.\n",
      "     |  \n",
      "     |  readme(self)\n",
      "     |      Return the contents of the corpus README file, if it exists.\n",
      "     |  \n",
      "     |  unicode_repr = __repr__(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  root\n",
      "     |      The directory where this corpus is stored.\n",
      "     |      \n",
      "     |      :type: PathPointer\n",
      "    \n",
      "    class RTECorpusReader(nltk.corpus.reader.xmldocs.XMLCorpusReader)\n",
      "     |  RTECorpusReader(root, fileids, wrap_etree=False)\n",
      "     |  \n",
      "     |  Corpus reader for corpora in RTE challenges.\n",
      "     |  \n",
      "     |  This is just a wrapper around the XMLCorpusReader. See module docstring above for the expected\n",
      "     |  structure of input documents.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      RTECorpusReader\n",
      "     |      nltk.corpus.reader.xmldocs.XMLCorpusReader\n",
      "     |      nltk.corpus.reader.api.CorpusReader\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  pairs(self, fileids)\n",
      "     |      Build a list of RTEPairs from a RTE corpus.\n",
      "     |      \n",
      "     |      :param fileids: a list of RTE corpus fileids\n",
      "     |      :type: list\n",
      "     |      :rtype: list(RTEPair)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from nltk.corpus.reader.xmldocs.XMLCorpusReader:\n",
      "     |  \n",
      "     |  __init__(self, root, fileids, wrap_etree=False)\n",
      "     |      :type root: PathPointer or str\n",
      "     |      :param root: A path pointer identifying the root directory for\n",
      "     |          this corpus.  If a string is specified, then it will be\n",
      "     |          converted to a ``PathPointer`` automatically.\n",
      "     |      :param fileids: A list of the files that make up this corpus.\n",
      "     |          This list can either be specified explicitly, as a list of\n",
      "     |          strings; or implicitly, as a regular expression over file\n",
      "     |          paths.  The absolute path for each file will be constructed\n",
      "     |          by joining the reader's root to each file name.\n",
      "     |      :param encoding: The default unicode encoding for the files\n",
      "     |          that make up the corpus.  The value of ``encoding`` can be any\n",
      "     |          of the following:\n",
      "     |          - A string: ``encoding`` is the encoding name for all files.\n",
      "     |          - A dictionary: ``encoding[file_id]`` is the encoding\n",
      "     |            name for the file whose identifier is ``file_id``.  If\n",
      "     |            ``file_id`` is not in ``encoding``, then the file\n",
      "     |            contents will be processed using non-unicode byte strings.\n",
      "     |          - A list: ``encoding`` should be a list of ``(regexp, encoding)``\n",
      "     |            tuples.  The encoding for a file whose identifier is ``file_id``\n",
      "     |            will be the ``encoding`` value for the first tuple whose\n",
      "     |            ``regexp`` matches the ``file_id``.  If no tuple's ``regexp``\n",
      "     |            matches the ``file_id``, the file contents will be processed\n",
      "     |            using non-unicode byte strings.\n",
      "     |          - None: the file contents of all files will be\n",
      "     |            processed using non-unicode byte strings.\n",
      "     |      :param tagset: The name of the tagset used by this corpus, to be used\n",
      "     |            for normalizing or converting the POS tags returned by the\n",
      "     |            tagged_...() methods.\n",
      "     |  \n",
      "     |  raw(self, fileids=None)\n",
      "     |  \n",
      "     |  words(self, fileid=None)\n",
      "     |      Returns all of the words and punctuation symbols in the specified file\n",
      "     |      that were in text nodes -- ie, tags are ignored. Like the xml() method,\n",
      "     |      fileid can only specify one file.\n",
      "     |      \n",
      "     |      :return: the given file's text nodes as a list of words and punctuation symbols\n",
      "     |      :rtype: list(str)\n",
      "     |  \n",
      "     |  xml(self, fileid=None)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __unicode__ = __str__(self, /)\n",
      "     |  \n",
      "     |  abspath(self, fileid)\n",
      "     |      Return the absolute path for the given file.\n",
      "     |      \n",
      "     |      :type fileid: str\n",
      "     |      :param fileid: The file identifier for the file whose path\n",
      "     |          should be returned.\n",
      "     |      :rtype: PathPointer\n",
      "     |  \n",
      "     |  abspaths(self, fileids=None, include_encoding=False, include_fileid=False)\n",
      "     |      Return a list of the absolute paths for all fileids in this corpus;\n",
      "     |      or for the given list of fileids, if specified.\n",
      "     |      \n",
      "     |      :type fileids: None or str or list\n",
      "     |      :param fileids: Specifies the set of fileids for which paths should\n",
      "     |          be returned.  Can be None, for all fileids; a list of\n",
      "     |          file identifiers, for a specified set of fileids; or a single\n",
      "     |          file identifier, for a single file.  Note that the return\n",
      "     |          value is always a list of paths, even if ``fileids`` is a\n",
      "     |          single file identifier.\n",
      "     |      \n",
      "     |      :param include_encoding: If true, then return a list of\n",
      "     |          ``(path_pointer, encoding)`` tuples.\n",
      "     |      \n",
      "     |      :rtype: list(PathPointer)\n",
      "     |  \n",
      "     |  citation(self)\n",
      "     |      Return the contents of the corpus citation.bib file, if it exists.\n",
      "     |  \n",
      "     |  encoding(self, file)\n",
      "     |      Return the unicode encoding for the given corpus file, if known.\n",
      "     |      If the encoding is unknown, or if the given file should be\n",
      "     |      processed using byte strings (str), then return None.\n",
      "     |  \n",
      "     |  ensure_loaded(self)\n",
      "     |      Load this corpus (if it has not already been loaded).  This is\n",
      "     |      used by LazyCorpusLoader as a simple method that can be used to\n",
      "     |      make sure a corpus is loaded -- e.g., in case a user wants to\n",
      "     |      do help(some_corpus).\n",
      "     |  \n",
      "     |  fileids(self)\n",
      "     |      Return a list of file identifiers for the fileids that make up\n",
      "     |      this corpus.\n",
      "     |  \n",
      "     |  license(self)\n",
      "     |      Return the contents of the corpus LICENSE file, if it exists.\n",
      "     |  \n",
      "     |  open(self, file)\n",
      "     |      Return an open stream that can be used to read the given file.\n",
      "     |      If the file's encoding is not None, then the stream will\n",
      "     |      automatically decode the file's contents into unicode.\n",
      "     |      \n",
      "     |      :param file: The file identifier of the file to read.\n",
      "     |  \n",
      "     |  readme(self)\n",
      "     |      Return the contents of the corpus README file, if it exists.\n",
      "     |  \n",
      "     |  unicode_repr = __repr__(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  root\n",
      "     |      The directory where this corpus is stored.\n",
      "     |      \n",
      "     |      :type: PathPointer\n",
      "    \n",
      "    class ReviewsCorpusReader(nltk.corpus.reader.api.CorpusReader)\n",
      "     |  ReviewsCorpusReader(root, fileids, word_tokenizer=WordPunctTokenizer(pattern='\\\\w+|[^\\\\w\\\\s]+', gaps=False, discard_empty=True, flags=<RegexFlag.UNICODE|DOTALL|MULTILINE: 56>), encoding='utf8')\n",
      "     |  \n",
      "     |  Reader for the Customer Review Data dataset by Hu, Liu (2004).\n",
      "     |  Note: we are not applying any sentence tokenization at the moment, just word\n",
      "     |  tokenization.\n",
      "     |  \n",
      "     |      >>> from nltk.corpus import product_reviews_1\n",
      "     |      >>> camera_reviews = product_reviews_1.reviews('Canon_G3.txt')\n",
      "     |      >>> review = camera_reviews[0]\n",
      "     |      >>> review.sents()[0]\n",
      "     |      ['i', 'recently', 'purchased', 'the', 'canon', 'powershot', 'g3', 'and', 'am',\n",
      "     |      'extremely', 'satisfied', 'with', 'the', 'purchase', '.']\n",
      "     |      >>> review.features()\n",
      "     |      [('canon powershot g3', '+3'), ('use', '+2'), ('picture', '+2'),\n",
      "     |      ('picture quality', '+1'), ('picture quality', '+1'), ('camera', '+2'),\n",
      "     |      ('use', '+2'), ('feature', '+1'), ('picture quality', '+3'), ('use', '+1'),\n",
      "     |      ('option', '+1')]\n",
      "     |  \n",
      "     |  We can also reach the same information directly from the stream:\n",
      "     |  \n",
      "     |      >>> product_reviews_1.features('Canon_G3.txt')\n",
      "     |      [('canon powershot g3', '+3'), ('use', '+2'), ...]\n",
      "     |  \n",
      "     |  We can compute stats for specific product features:\n",
      "     |  \n",
      "     |      >>> from __future__ import division\n",
      "     |      >>> n_reviews = len([(feat,score) for (feat,score) in product_reviews_1.features('Canon_G3.txt') if feat=='picture'])\n",
      "     |      >>> tot = sum([int(score) for (feat,score) in product_reviews_1.features('Canon_G3.txt') if feat=='picture'])\n",
      "     |      >>> # We use float for backward compatibility with division in Python2.7\n",
      "     |      >>> mean = tot / n_reviews\n",
      "     |      >>> print(n_reviews, tot, mean)\n",
      "     |      15 24 1.6\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      ReviewsCorpusReader\n",
      "     |      nltk.corpus.reader.api.CorpusReader\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, root, fileids, word_tokenizer=WordPunctTokenizer(pattern='\\\\w+|[^\\\\w\\\\s]+', gaps=False, discard_empty=True, flags=<RegexFlag.UNICODE|DOTALL|MULTILINE: 56>), encoding='utf8')\n",
      "     |      :param root: The root directory for the corpus.\n",
      "     |      :param fileids: a list or regexp specifying the fileids in the corpus.\n",
      "     |      :param word_tokenizer: a tokenizer for breaking sentences or paragraphs\n",
      "     |          into words. Default: `WordPunctTokenizer`\n",
      "     |      :param encoding: the encoding that should be used to read the corpus.\n",
      "     |  \n",
      "     |  features(self, fileids=None)\n",
      "     |      Return a list of features. Each feature is a tuple made of the specific\n",
      "     |      item feature and the opinion strength about that feature.\n",
      "     |      \n",
      "     |      :param fileids: a list or regexp specifying the ids of the files whose\n",
      "     |          features have to be returned.\n",
      "     |      :return: all features for the item(s) in the given file(s).\n",
      "     |      :rtype: list(tuple)\n",
      "     |  \n",
      "     |  raw(self, fileids=None)\n",
      "     |      :param fileids: a list or regexp specifying the fileids of the files that\n",
      "     |          have to be returned as a raw string.\n",
      "     |      :return: the given file(s) as a single string.\n",
      "     |      :rtype: str\n",
      "     |  \n",
      "     |  readme(self)\n",
      "     |      Return the contents of the corpus README.txt file.\n",
      "     |  \n",
      "     |  reviews(self, fileids=None)\n",
      "     |      Return all the reviews as a list of Review objects. If `fileids` is\n",
      "     |      specified, return all the reviews from each of the specified files.\n",
      "     |      \n",
      "     |      :param fileids: a list or regexp specifying the ids of the files whose\n",
      "     |          reviews have to be returned.\n",
      "     |      :return: the given file(s) as a list of reviews.\n",
      "     |  \n",
      "     |  sents(self, fileids=None)\n",
      "     |      Return all sentences in the corpus or in the specified files.\n",
      "     |      \n",
      "     |      :param fileids: a list or regexp specifying the ids of the files whose\n",
      "     |          sentences have to be returned.\n",
      "     |      :return: the given file(s) as a list of sentences, each encoded as a\n",
      "     |          list of word strings.\n",
      "     |      :rtype: list(list(str))\n",
      "     |  \n",
      "     |  words(self, fileids=None)\n",
      "     |      Return all words and punctuation symbols in the corpus or in the specified\n",
      "     |      files.\n",
      "     |      \n",
      "     |      :param fileids: a list or regexp specifying the ids of the files whose\n",
      "     |          words have to be returned.\n",
      "     |      :return: the given file(s) as a list of words and punctuation symbols.\n",
      "     |      :rtype: list(str)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  CorpusView = <class 'nltk.corpus.reader.util.StreamBackedCorpusView'>\n",
      "     |      A 'view' of a corpus file, which acts like a sequence of tokens:\n",
      "     |      it can be accessed by index, iterated over, etc.  However, the\n",
      "     |      tokens are only constructed as-needed -- the entire corpus is\n",
      "     |      never stored in memory at once.\n",
      "     |      \n",
      "     |      The constructor to ``StreamBackedCorpusView`` takes two arguments:\n",
      "     |      a corpus fileid (specified as a string or as a ``PathPointer``);\n",
      "     |      and a block reader.  A \"block reader\" is a function that reads\n",
      "     |      zero or more tokens from a stream, and returns them as a list.  A\n",
      "     |      very simple example of a block reader is:\n",
      "     |      \n",
      "     |          >>> def simple_block_reader(stream):\n",
      "     |          ...     return stream.readline().split()\n",
      "     |      \n",
      "     |      This simple block reader reads a single line at a time, and\n",
      "     |      returns a single token (consisting of a string) for each\n",
      "     |      whitespace-separated substring on the line.\n",
      "     |      \n",
      "     |      When deciding how to define the block reader for a given\n",
      "     |      corpus, careful consideration should be given to the size of\n",
      "     |      blocks handled by the block reader.  Smaller block sizes will\n",
      "     |      increase the memory requirements of the corpus view's internal\n",
      "     |      data structures (by 2 integers per block).  On the other hand,\n",
      "     |      larger block sizes may decrease performance for random access to\n",
      "     |      the corpus.  (But note that larger block sizes will *not*\n",
      "     |      decrease performance for iteration.)\n",
      "     |      \n",
      "     |      Internally, ``CorpusView`` maintains a partial mapping from token\n",
      "     |      index to file position, with one entry per block.  When a token\n",
      "     |      with a given index *i* is requested, the ``CorpusView`` constructs\n",
      "     |      it as follows:\n",
      "     |      \n",
      "     |        1. First, it searches the toknum/filepos mapping for the token\n",
      "     |           index closest to (but less than or equal to) *i*.\n",
      "     |      \n",
      "     |        2. Then, starting at the file position corresponding to that\n",
      "     |           index, it reads one block at a time using the block reader\n",
      "     |           until it reaches the requested token.\n",
      "     |      \n",
      "     |      The toknum/filepos mapping is created lazily: it is initially\n",
      "     |      empty, but every time a new block is read, the block's\n",
      "     |      initial token is added to the mapping.  (Thus, the toknum/filepos\n",
      "     |      map has one entry per block.)\n",
      "     |      \n",
      "     |      In order to increase efficiency for random access patterns that\n",
      "     |      have high degrees of locality, the corpus view may cache one or\n",
      "     |      more blocks.\n",
      "     |      \n",
      "     |      :note: Each ``CorpusView`` object internally maintains an open file\n",
      "     |          object for its underlying corpus file.  This file should be\n",
      "     |          automatically closed when the ``CorpusView`` is garbage collected,\n",
      "     |          but if you wish to close it manually, use the ``close()``\n",
      "     |          method.  If you access a ``CorpusView``'s items after it has been\n",
      "     |          closed, the file object will be automatically re-opened.\n",
      "     |      \n",
      "     |      :warning: If the contents of the file are modified during the\n",
      "     |          lifetime of the ``CorpusView``, then the ``CorpusView``'s behavior\n",
      "     |          is undefined.\n",
      "     |      \n",
      "     |      :warning: If a unicode encoding is specified when constructing a\n",
      "     |          ``CorpusView``, then the block reader may only call\n",
      "     |          ``stream.seek()`` with offsets that have been returned by\n",
      "     |          ``stream.tell()``; in particular, calling ``stream.seek()`` with\n",
      "     |          relative offsets, or with offsets based on string lengths, may\n",
      "     |          lead to incorrect behavior.\n",
      "     |      \n",
      "     |      :ivar _block_reader: The function used to read\n",
      "     |          a single block from the underlying file stream.\n",
      "     |      :ivar _toknum: A list containing the token index of each block\n",
      "     |          that has been processed.  In particular, ``_toknum[i]`` is the\n",
      "     |          token index of the first token in block ``i``.  Together\n",
      "     |          with ``_filepos``, this forms a partial mapping between token\n",
      "     |          indices and file positions.\n",
      "     |      :ivar _filepos: A list containing the file position of each block\n",
      "     |          that has been processed.  In particular, ``_toknum[i]`` is the\n",
      "     |          file position of the first character in block ``i``.  Together\n",
      "     |          with ``_toknum``, this forms a partial mapping between token\n",
      "     |          indices and file positions.\n",
      "     |      :ivar _stream: The stream used to access the underlying corpus file.\n",
      "     |      :ivar _len: The total number of tokens in the corpus, if known;\n",
      "     |          or None, if the number of tokens is not yet known.\n",
      "     |      :ivar _eofpos: The character position of the last character in the\n",
      "     |          file.  This is calculated when the corpus view is initialized,\n",
      "     |          and is used to decide when the end of file has been reached.\n",
      "     |      :ivar _cache: A cache of the most recently read block.  It\n",
      "     |         is encoded as a tuple (start_toknum, end_toknum, tokens), where\n",
      "     |         start_toknum is the token index of the first token in the block;\n",
      "     |         end_toknum is the token index of the first token not in the\n",
      "     |         block; and tokens is a list of the tokens in the block.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __unicode__ = __str__(self, /)\n",
      "     |  \n",
      "     |  abspath(self, fileid)\n",
      "     |      Return the absolute path for the given file.\n",
      "     |      \n",
      "     |      :type fileid: str\n",
      "     |      :param fileid: The file identifier for the file whose path\n",
      "     |          should be returned.\n",
      "     |      :rtype: PathPointer\n",
      "     |  \n",
      "     |  abspaths(self, fileids=None, include_encoding=False, include_fileid=False)\n",
      "     |      Return a list of the absolute paths for all fileids in this corpus;\n",
      "     |      or for the given list of fileids, if specified.\n",
      "     |      \n",
      "     |      :type fileids: None or str or list\n",
      "     |      :param fileids: Specifies the set of fileids for which paths should\n",
      "     |          be returned.  Can be None, for all fileids; a list of\n",
      "     |          file identifiers, for a specified set of fileids; or a single\n",
      "     |          file identifier, for a single file.  Note that the return\n",
      "     |          value is always a list of paths, even if ``fileids`` is a\n",
      "     |          single file identifier.\n",
      "     |      \n",
      "     |      :param include_encoding: If true, then return a list of\n",
      "     |          ``(path_pointer, encoding)`` tuples.\n",
      "     |      \n",
      "     |      :rtype: list(PathPointer)\n",
      "     |  \n",
      "     |  citation(self)\n",
      "     |      Return the contents of the corpus citation.bib file, if it exists.\n",
      "     |  \n",
      "     |  encoding(self, file)\n",
      "     |      Return the unicode encoding for the given corpus file, if known.\n",
      "     |      If the encoding is unknown, or if the given file should be\n",
      "     |      processed using byte strings (str), then return None.\n",
      "     |  \n",
      "     |  ensure_loaded(self)\n",
      "     |      Load this corpus (if it has not already been loaded).  This is\n",
      "     |      used by LazyCorpusLoader as a simple method that can be used to\n",
      "     |      make sure a corpus is loaded -- e.g., in case a user wants to\n",
      "     |      do help(some_corpus).\n",
      "     |  \n",
      "     |  fileids(self)\n",
      "     |      Return a list of file identifiers for the fileids that make up\n",
      "     |      this corpus.\n",
      "     |  \n",
      "     |  license(self)\n",
      "     |      Return the contents of the corpus LICENSE file, if it exists.\n",
      "     |  \n",
      "     |  open(self, file)\n",
      "     |      Return an open stream that can be used to read the given file.\n",
      "     |      If the file's encoding is not None, then the stream will\n",
      "     |      automatically decode the file's contents into unicode.\n",
      "     |      \n",
      "     |      :param file: The file identifier of the file to read.\n",
      "     |  \n",
      "     |  unicode_repr = __repr__(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  root\n",
      "     |      The directory where this corpus is stored.\n",
      "     |      \n",
      "     |      :type: PathPointer\n",
      "    \n",
      "    class SemcorCorpusReader(nltk.corpus.reader.xmldocs.XMLCorpusReader)\n",
      "     |  SemcorCorpusReader(root, fileids, wordnet, lazy=True)\n",
      "     |  \n",
      "     |  Corpus reader for the SemCor Corpus.\n",
      "     |  For access to the complete XML data structure, use the ``xml()``\n",
      "     |  method.  For access to simple word lists and tagged word lists, use\n",
      "     |  ``words()``, ``sents()``, ``tagged_words()``, and ``tagged_sents()``.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      SemcorCorpusReader\n",
      "     |      nltk.corpus.reader.xmldocs.XMLCorpusReader\n",
      "     |      nltk.corpus.reader.api.CorpusReader\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, root, fileids, wordnet, lazy=True)\n",
      "     |      :type root: PathPointer or str\n",
      "     |      :param root: A path pointer identifying the root directory for\n",
      "     |          this corpus.  If a string is specified, then it will be\n",
      "     |          converted to a ``PathPointer`` automatically.\n",
      "     |      :param fileids: A list of the files that make up this corpus.\n",
      "     |          This list can either be specified explicitly, as a list of\n",
      "     |          strings; or implicitly, as a regular expression over file\n",
      "     |          paths.  The absolute path for each file will be constructed\n",
      "     |          by joining the reader's root to each file name.\n",
      "     |      :param encoding: The default unicode encoding for the files\n",
      "     |          that make up the corpus.  The value of ``encoding`` can be any\n",
      "     |          of the following:\n",
      "     |          - A string: ``encoding`` is the encoding name for all files.\n",
      "     |          - A dictionary: ``encoding[file_id]`` is the encoding\n",
      "     |            name for the file whose identifier is ``file_id``.  If\n",
      "     |            ``file_id`` is not in ``encoding``, then the file\n",
      "     |            contents will be processed using non-unicode byte strings.\n",
      "     |          - A list: ``encoding`` should be a list of ``(regexp, encoding)``\n",
      "     |            tuples.  The encoding for a file whose identifier is ``file_id``\n",
      "     |            will be the ``encoding`` value for the first tuple whose\n",
      "     |            ``regexp`` matches the ``file_id``.  If no tuple's ``regexp``\n",
      "     |            matches the ``file_id``, the file contents will be processed\n",
      "     |            using non-unicode byte strings.\n",
      "     |          - None: the file contents of all files will be\n",
      "     |            processed using non-unicode byte strings.\n",
      "     |      :param tagset: The name of the tagset used by this corpus, to be used\n",
      "     |            for normalizing or converting the POS tags returned by the\n",
      "     |            tagged_...() methods.\n",
      "     |  \n",
      "     |  chunk_sents(self, fileids=None)\n",
      "     |      :return: the given file(s) as a list of sentences, each encoded\n",
      "     |          as a list of chunks.\n",
      "     |      :rtype: list(list(list(str)))\n",
      "     |  \n",
      "     |  chunks(self, fileids=None)\n",
      "     |      :return: the given file(s) as a list of chunks,\n",
      "     |          each of which is a list of words and punctuation symbols\n",
      "     |          that form a unit.\n",
      "     |      :rtype: list(list(str))\n",
      "     |  \n",
      "     |  sents(self, fileids=None)\n",
      "     |      :return: the given file(s) as a list of sentences, each encoded\n",
      "     |          as a list of word strings.\n",
      "     |      :rtype: list(list(str))\n",
      "     |  \n",
      "     |  tagged_chunks(self, fileids=None, tag='pos')\n",
      "     |      :return: the given file(s) as a list of tagged chunks, represented\n",
      "     |          in tree form.\n",
      "     |      :rtype: list(Tree)\n",
      "     |      \n",
      "     |      :param tag: `'pos'` (part of speech), `'sem'` (semantic), or `'both'`\n",
      "     |          to indicate the kind of tags to include.  Semantic tags consist of\n",
      "     |          WordNet lemma IDs, plus an `'NE'` node if the chunk is a named entity\n",
      "     |          without a specific entry in WordNet.  (Named entities of type 'other'\n",
      "     |          have no lemma.  Other chunks not in WordNet have no semantic tag.\n",
      "     |          Punctuation tokens have `None` for their part of speech tag.)\n",
      "     |  \n",
      "     |  tagged_sents(self, fileids=None, tag='pos')\n",
      "     |      :return: the given file(s) as a list of sentences. Each sentence\n",
      "     |          is represented as a list of tagged chunks (in tree form).\n",
      "     |      :rtype: list(list(Tree))\n",
      "     |      \n",
      "     |      :param tag: `'pos'` (part of speech), `'sem'` (semantic), or `'both'`\n",
      "     |          to indicate the kind of tags to include.  Semantic tags consist of\n",
      "     |          WordNet lemma IDs, plus an `'NE'` node if the chunk is a named entity\n",
      "     |          without a specific entry in WordNet.  (Named entities of type 'other'\n",
      "     |          have no lemma.  Other chunks not in WordNet have no semantic tag.\n",
      "     |          Punctuation tokens have `None` for their part of speech tag.)\n",
      "     |  \n",
      "     |  words(self, fileids=None)\n",
      "     |      :return: the given file(s) as a list of words and punctuation symbols.\n",
      "     |      :rtype: list(str)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from nltk.corpus.reader.xmldocs.XMLCorpusReader:\n",
      "     |  \n",
      "     |  raw(self, fileids=None)\n",
      "     |  \n",
      "     |  xml(self, fileid=None)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __unicode__ = __str__(self, /)\n",
      "     |  \n",
      "     |  abspath(self, fileid)\n",
      "     |      Return the absolute path for the given file.\n",
      "     |      \n",
      "     |      :type fileid: str\n",
      "     |      :param fileid: The file identifier for the file whose path\n",
      "     |          should be returned.\n",
      "     |      :rtype: PathPointer\n",
      "     |  \n",
      "     |  abspaths(self, fileids=None, include_encoding=False, include_fileid=False)\n",
      "     |      Return a list of the absolute paths for all fileids in this corpus;\n",
      "     |      or for the given list of fileids, if specified.\n",
      "     |      \n",
      "     |      :type fileids: None or str or list\n",
      "     |      :param fileids: Specifies the set of fileids for which paths should\n",
      "     |          be returned.  Can be None, for all fileids; a list of\n",
      "     |          file identifiers, for a specified set of fileids; or a single\n",
      "     |          file identifier, for a single file.  Note that the return\n",
      "     |          value is always a list of paths, even if ``fileids`` is a\n",
      "     |          single file identifier.\n",
      "     |      \n",
      "     |      :param include_encoding: If true, then return a list of\n",
      "     |          ``(path_pointer, encoding)`` tuples.\n",
      "     |      \n",
      "     |      :rtype: list(PathPointer)\n",
      "     |  \n",
      "     |  citation(self)\n",
      "     |      Return the contents of the corpus citation.bib file, if it exists.\n",
      "     |  \n",
      "     |  encoding(self, file)\n",
      "     |      Return the unicode encoding for the given corpus file, if known.\n",
      "     |      If the encoding is unknown, or if the given file should be\n",
      "     |      processed using byte strings (str), then return None.\n",
      "     |  \n",
      "     |  ensure_loaded(self)\n",
      "     |      Load this corpus (if it has not already been loaded).  This is\n",
      "     |      used by LazyCorpusLoader as a simple method that can be used to\n",
      "     |      make sure a corpus is loaded -- e.g., in case a user wants to\n",
      "     |      do help(some_corpus).\n",
      "     |  \n",
      "     |  fileids(self)\n",
      "     |      Return a list of file identifiers for the fileids that make up\n",
      "     |      this corpus.\n",
      "     |  \n",
      "     |  license(self)\n",
      "     |      Return the contents of the corpus LICENSE file, if it exists.\n",
      "     |  \n",
      "     |  open(self, file)\n",
      "     |      Return an open stream that can be used to read the given file.\n",
      "     |      If the file's encoding is not None, then the stream will\n",
      "     |      automatically decode the file's contents into unicode.\n",
      "     |      \n",
      "     |      :param file: The file identifier of the file to read.\n",
      "     |  \n",
      "     |  readme(self)\n",
      "     |      Return the contents of the corpus README file, if it exists.\n",
      "     |  \n",
      "     |  unicode_repr = __repr__(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  root\n",
      "     |      The directory where this corpus is stored.\n",
      "     |      \n",
      "     |      :type: PathPointer\n",
      "    \n",
      "    class SensevalCorpusReader(nltk.corpus.reader.api.CorpusReader)\n",
      "     |  SensevalCorpusReader(root, fileids, encoding='utf8', tagset=None)\n",
      "     |  \n",
      "     |  A base class for \"corpus reader\" classes, each of which can be\n",
      "     |  used to read a specific corpus format.  Each individual corpus\n",
      "     |  reader instance is used to read a specific corpus, consisting of\n",
      "     |  one or more files under a common root directory.  Each file is\n",
      "     |  identified by its ``file identifier``, which is the relative path\n",
      "     |  to the file from the root directory.\n",
      "     |  \n",
      "     |  A separate subclass is defined for each corpus format.  These\n",
      "     |  subclasses define one or more methods that provide 'views' on the\n",
      "     |  corpus contents, such as ``words()`` (for a list of words) and\n",
      "     |  ``parsed_sents()`` (for a list of parsed sentences).  Called with\n",
      "     |  no arguments, these methods will return the contents of the entire\n",
      "     |  corpus.  For most corpora, these methods define one or more\n",
      "     |  selection arguments, such as ``fileids`` or ``categories``, which can\n",
      "     |  be used to select which portion of the corpus should be returned.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      SensevalCorpusReader\n",
      "     |      nltk.corpus.reader.api.CorpusReader\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  instances(self, fileids=None)\n",
      "     |  \n",
      "     |  raw(self, fileids=None)\n",
      "     |      :return: the text contents of the given fileids, as a single string.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __init__(self, root, fileids, encoding='utf8', tagset=None)\n",
      "     |      :type root: PathPointer or str\n",
      "     |      :param root: A path pointer identifying the root directory for\n",
      "     |          this corpus.  If a string is specified, then it will be\n",
      "     |          converted to a ``PathPointer`` automatically.\n",
      "     |      :param fileids: A list of the files that make up this corpus.\n",
      "     |          This list can either be specified explicitly, as a list of\n",
      "     |          strings; or implicitly, as a regular expression over file\n",
      "     |          paths.  The absolute path for each file will be constructed\n",
      "     |          by joining the reader's root to each file name.\n",
      "     |      :param encoding: The default unicode encoding for the files\n",
      "     |          that make up the corpus.  The value of ``encoding`` can be any\n",
      "     |          of the following:\n",
      "     |          - A string: ``encoding`` is the encoding name for all files.\n",
      "     |          - A dictionary: ``encoding[file_id]`` is the encoding\n",
      "     |            name for the file whose identifier is ``file_id``.  If\n",
      "     |            ``file_id`` is not in ``encoding``, then the file\n",
      "     |            contents will be processed using non-unicode byte strings.\n",
      "     |          - A list: ``encoding`` should be a list of ``(regexp, encoding)``\n",
      "     |            tuples.  The encoding for a file whose identifier is ``file_id``\n",
      "     |            will be the ``encoding`` value for the first tuple whose\n",
      "     |            ``regexp`` matches the ``file_id``.  If no tuple's ``regexp``\n",
      "     |            matches the ``file_id``, the file contents will be processed\n",
      "     |            using non-unicode byte strings.\n",
      "     |          - None: the file contents of all files will be\n",
      "     |            processed using non-unicode byte strings.\n",
      "     |      :param tagset: The name of the tagset used by this corpus, to be used\n",
      "     |            for normalizing or converting the POS tags returned by the\n",
      "     |            tagged_...() methods.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __unicode__ = __str__(self, /)\n",
      "     |  \n",
      "     |  abspath(self, fileid)\n",
      "     |      Return the absolute path for the given file.\n",
      "     |      \n",
      "     |      :type fileid: str\n",
      "     |      :param fileid: The file identifier for the file whose path\n",
      "     |          should be returned.\n",
      "     |      :rtype: PathPointer\n",
      "     |  \n",
      "     |  abspaths(self, fileids=None, include_encoding=False, include_fileid=False)\n",
      "     |      Return a list of the absolute paths for all fileids in this corpus;\n",
      "     |      or for the given list of fileids, if specified.\n",
      "     |      \n",
      "     |      :type fileids: None or str or list\n",
      "     |      :param fileids: Specifies the set of fileids for which paths should\n",
      "     |          be returned.  Can be None, for all fileids; a list of\n",
      "     |          file identifiers, for a specified set of fileids; or a single\n",
      "     |          file identifier, for a single file.  Note that the return\n",
      "     |          value is always a list of paths, even if ``fileids`` is a\n",
      "     |          single file identifier.\n",
      "     |      \n",
      "     |      :param include_encoding: If true, then return a list of\n",
      "     |          ``(path_pointer, encoding)`` tuples.\n",
      "     |      \n",
      "     |      :rtype: list(PathPointer)\n",
      "     |  \n",
      "     |  citation(self)\n",
      "     |      Return the contents of the corpus citation.bib file, if it exists.\n",
      "     |  \n",
      "     |  encoding(self, file)\n",
      "     |      Return the unicode encoding for the given corpus file, if known.\n",
      "     |      If the encoding is unknown, or if the given file should be\n",
      "     |      processed using byte strings (str), then return None.\n",
      "     |  \n",
      "     |  ensure_loaded(self)\n",
      "     |      Load this corpus (if it has not already been loaded).  This is\n",
      "     |      used by LazyCorpusLoader as a simple method that can be used to\n",
      "     |      make sure a corpus is loaded -- e.g., in case a user wants to\n",
      "     |      do help(some_corpus).\n",
      "     |  \n",
      "     |  fileids(self)\n",
      "     |      Return a list of file identifiers for the fileids that make up\n",
      "     |      this corpus.\n",
      "     |  \n",
      "     |  license(self)\n",
      "     |      Return the contents of the corpus LICENSE file, if it exists.\n",
      "     |  \n",
      "     |  open(self, file)\n",
      "     |      Return an open stream that can be used to read the given file.\n",
      "     |      If the file's encoding is not None, then the stream will\n",
      "     |      automatically decode the file's contents into unicode.\n",
      "     |      \n",
      "     |      :param file: The file identifier of the file to read.\n",
      "     |  \n",
      "     |  readme(self)\n",
      "     |      Return the contents of the corpus README file, if it exists.\n",
      "     |  \n",
      "     |  unicode_repr = __repr__(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  root\n",
      "     |      The directory where this corpus is stored.\n",
      "     |      \n",
      "     |      :type: PathPointer\n",
      "    \n",
      "    class SentiSynset(builtins.object)\n",
      "     |  SentiSynset(pos_score, neg_score, synset)\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, pos_score, neg_score, synset)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __str__(self)\n",
      "     |      Prints just the Pos/Neg scores for now.\n",
      "     |  \n",
      "     |  __unicode__ = __str__(self)\n",
      "     |  \n",
      "     |  neg_score(self)\n",
      "     |  \n",
      "     |  obj_score(self)\n",
      "     |  \n",
      "     |  pos_score(self)\n",
      "     |  \n",
      "     |  unicode_repr = __repr__(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class SentiWordNetCorpusReader(nltk.corpus.reader.api.CorpusReader)\n",
      "     |  SentiWordNetCorpusReader(root, fileids, encoding='utf-8')\n",
      "     |  \n",
      "     |  A base class for \"corpus reader\" classes, each of which can be\n",
      "     |  used to read a specific corpus format.  Each individual corpus\n",
      "     |  reader instance is used to read a specific corpus, consisting of\n",
      "     |  one or more files under a common root directory.  Each file is\n",
      "     |  identified by its ``file identifier``, which is the relative path\n",
      "     |  to the file from the root directory.\n",
      "     |  \n",
      "     |  A separate subclass is defined for each corpus format.  These\n",
      "     |  subclasses define one or more methods that provide 'views' on the\n",
      "     |  corpus contents, such as ``words()`` (for a list of words) and\n",
      "     |  ``parsed_sents()`` (for a list of parsed sentences).  Called with\n",
      "     |  no arguments, these methods will return the contents of the entire\n",
      "     |  corpus.  For most corpora, these methods define one or more\n",
      "     |  selection arguments, such as ``fileids`` or ``categories``, which can\n",
      "     |  be used to select which portion of the corpus should be returned.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      SentiWordNetCorpusReader\n",
      "     |      nltk.corpus.reader.api.CorpusReader\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, root, fileids, encoding='utf-8')\n",
      "     |      Construct a new SentiWordNet Corpus Reader, using data from\n",
      "     |      the specified file.\n",
      "     |  \n",
      "     |  __unicode__ = __str__(self, /)\n",
      "     |  \n",
      "     |  all_senti_synsets(self)\n",
      "     |  \n",
      "     |  senti_synset(self, *vals)\n",
      "     |  \n",
      "     |  senti_synsets(self, string, pos=None)\n",
      "     |  \n",
      "     |  unicode_repr = __repr__(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  abspath(self, fileid)\n",
      "     |      Return the absolute path for the given file.\n",
      "     |      \n",
      "     |      :type fileid: str\n",
      "     |      :param fileid: The file identifier for the file whose path\n",
      "     |          should be returned.\n",
      "     |      :rtype: PathPointer\n",
      "     |  \n",
      "     |  abspaths(self, fileids=None, include_encoding=False, include_fileid=False)\n",
      "     |      Return a list of the absolute paths for all fileids in this corpus;\n",
      "     |      or for the given list of fileids, if specified.\n",
      "     |      \n",
      "     |      :type fileids: None or str or list\n",
      "     |      :param fileids: Specifies the set of fileids for which paths should\n",
      "     |          be returned.  Can be None, for all fileids; a list of\n",
      "     |          file identifiers, for a specified set of fileids; or a single\n",
      "     |          file identifier, for a single file.  Note that the return\n",
      "     |          value is always a list of paths, even if ``fileids`` is a\n",
      "     |          single file identifier.\n",
      "     |      \n",
      "     |      :param include_encoding: If true, then return a list of\n",
      "     |          ``(path_pointer, encoding)`` tuples.\n",
      "     |      \n",
      "     |      :rtype: list(PathPointer)\n",
      "     |  \n",
      "     |  citation(self)\n",
      "     |      Return the contents of the corpus citation.bib file, if it exists.\n",
      "     |  \n",
      "     |  encoding(self, file)\n",
      "     |      Return the unicode encoding for the given corpus file, if known.\n",
      "     |      If the encoding is unknown, or if the given file should be\n",
      "     |      processed using byte strings (str), then return None.\n",
      "     |  \n",
      "     |  ensure_loaded(self)\n",
      "     |      Load this corpus (if it has not already been loaded).  This is\n",
      "     |      used by LazyCorpusLoader as a simple method that can be used to\n",
      "     |      make sure a corpus is loaded -- e.g., in case a user wants to\n",
      "     |      do help(some_corpus).\n",
      "     |  \n",
      "     |  fileids(self)\n",
      "     |      Return a list of file identifiers for the fileids that make up\n",
      "     |      this corpus.\n",
      "     |  \n",
      "     |  license(self)\n",
      "     |      Return the contents of the corpus LICENSE file, if it exists.\n",
      "     |  \n",
      "     |  open(self, file)\n",
      "     |      Return an open stream that can be used to read the given file.\n",
      "     |      If the file's encoding is not None, then the stream will\n",
      "     |      automatically decode the file's contents into unicode.\n",
      "     |      \n",
      "     |      :param file: The file identifier of the file to read.\n",
      "     |  \n",
      "     |  readme(self)\n",
      "     |      Return the contents of the corpus README file, if it exists.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  root\n",
      "     |      The directory where this corpus is stored.\n",
      "     |      \n",
      "     |      :type: PathPointer\n",
      "    \n",
      "    class SinicaTreebankCorpusReader(nltk.corpus.reader.api.SyntaxCorpusReader)\n",
      "     |  SinicaTreebankCorpusReader(root, fileids, encoding='utf8', tagset=None)\n",
      "     |  \n",
      "     |  Reader for the sinica treebank.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      SinicaTreebankCorpusReader\n",
      "     |      nltk.corpus.reader.api.SyntaxCorpusReader\n",
      "     |      nltk.corpus.reader.api.CorpusReader\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods inherited from nltk.corpus.reader.api.SyntaxCorpusReader:\n",
      "     |  \n",
      "     |  parsed_sents(self, fileids=None)\n",
      "     |  \n",
      "     |  raw(self, fileids=None)\n",
      "     |  \n",
      "     |  sents(self, fileids=None)\n",
      "     |  \n",
      "     |  tagged_sents(self, fileids=None, tagset=None)\n",
      "     |  \n",
      "     |  tagged_words(self, fileids=None, tagset=None)\n",
      "     |  \n",
      "     |  words(self, fileids=None)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __init__(self, root, fileids, encoding='utf8', tagset=None)\n",
      "     |      :type root: PathPointer or str\n",
      "     |      :param root: A path pointer identifying the root directory for\n",
      "     |          this corpus.  If a string is specified, then it will be\n",
      "     |          converted to a ``PathPointer`` automatically.\n",
      "     |      :param fileids: A list of the files that make up this corpus.\n",
      "     |          This list can either be specified explicitly, as a list of\n",
      "     |          strings; or implicitly, as a regular expression over file\n",
      "     |          paths.  The absolute path for each file will be constructed\n",
      "     |          by joining the reader's root to each file name.\n",
      "     |      :param encoding: The default unicode encoding for the files\n",
      "     |          that make up the corpus.  The value of ``encoding`` can be any\n",
      "     |          of the following:\n",
      "     |          - A string: ``encoding`` is the encoding name for all files.\n",
      "     |          - A dictionary: ``encoding[file_id]`` is the encoding\n",
      "     |            name for the file whose identifier is ``file_id``.  If\n",
      "     |            ``file_id`` is not in ``encoding``, then the file\n",
      "     |            contents will be processed using non-unicode byte strings.\n",
      "     |          - A list: ``encoding`` should be a list of ``(regexp, encoding)``\n",
      "     |            tuples.  The encoding for a file whose identifier is ``file_id``\n",
      "     |            will be the ``encoding`` value for the first tuple whose\n",
      "     |            ``regexp`` matches the ``file_id``.  If no tuple's ``regexp``\n",
      "     |            matches the ``file_id``, the file contents will be processed\n",
      "     |            using non-unicode byte strings.\n",
      "     |          - None: the file contents of all files will be\n",
      "     |            processed using non-unicode byte strings.\n",
      "     |      :param tagset: The name of the tagset used by this corpus, to be used\n",
      "     |            for normalizing or converting the POS tags returned by the\n",
      "     |            tagged_...() methods.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __unicode__ = __str__(self, /)\n",
      "     |  \n",
      "     |  abspath(self, fileid)\n",
      "     |      Return the absolute path for the given file.\n",
      "     |      \n",
      "     |      :type fileid: str\n",
      "     |      :param fileid: The file identifier for the file whose path\n",
      "     |          should be returned.\n",
      "     |      :rtype: PathPointer\n",
      "     |  \n",
      "     |  abspaths(self, fileids=None, include_encoding=False, include_fileid=False)\n",
      "     |      Return a list of the absolute paths for all fileids in this corpus;\n",
      "     |      or for the given list of fileids, if specified.\n",
      "     |      \n",
      "     |      :type fileids: None or str or list\n",
      "     |      :param fileids: Specifies the set of fileids for which paths should\n",
      "     |          be returned.  Can be None, for all fileids; a list of\n",
      "     |          file identifiers, for a specified set of fileids; or a single\n",
      "     |          file identifier, for a single file.  Note that the return\n",
      "     |          value is always a list of paths, even if ``fileids`` is a\n",
      "     |          single file identifier.\n",
      "     |      \n",
      "     |      :param include_encoding: If true, then return a list of\n",
      "     |          ``(path_pointer, encoding)`` tuples.\n",
      "     |      \n",
      "     |      :rtype: list(PathPointer)\n",
      "     |  \n",
      "     |  citation(self)\n",
      "     |      Return the contents of the corpus citation.bib file, if it exists.\n",
      "     |  \n",
      "     |  encoding(self, file)\n",
      "     |      Return the unicode encoding for the given corpus file, if known.\n",
      "     |      If the encoding is unknown, or if the given file should be\n",
      "     |      processed using byte strings (str), then return None.\n",
      "     |  \n",
      "     |  ensure_loaded(self)\n",
      "     |      Load this corpus (if it has not already been loaded).  This is\n",
      "     |      used by LazyCorpusLoader as a simple method that can be used to\n",
      "     |      make sure a corpus is loaded -- e.g., in case a user wants to\n",
      "     |      do help(some_corpus).\n",
      "     |  \n",
      "     |  fileids(self)\n",
      "     |      Return a list of file identifiers for the fileids that make up\n",
      "     |      this corpus.\n",
      "     |  \n",
      "     |  license(self)\n",
      "     |      Return the contents of the corpus LICENSE file, if it exists.\n",
      "     |  \n",
      "     |  open(self, file)\n",
      "     |      Return an open stream that can be used to read the given file.\n",
      "     |      If the file's encoding is not None, then the stream will\n",
      "     |      automatically decode the file's contents into unicode.\n",
      "     |      \n",
      "     |      :param file: The file identifier of the file to read.\n",
      "     |  \n",
      "     |  readme(self)\n",
      "     |      Return the contents of the corpus README file, if it exists.\n",
      "     |  \n",
      "     |  unicode_repr = __repr__(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  root\n",
      "     |      The directory where this corpus is stored.\n",
      "     |      \n",
      "     |      :type: PathPointer\n",
      "    \n",
      "    class StringCategoryCorpusReader(nltk.corpus.reader.api.CorpusReader)\n",
      "     |  StringCategoryCorpusReader(root, fileids, delimiter=' ', encoding='utf8')\n",
      "     |  \n",
      "     |  A base class for \"corpus reader\" classes, each of which can be\n",
      "     |  used to read a specific corpus format.  Each individual corpus\n",
      "     |  reader instance is used to read a specific corpus, consisting of\n",
      "     |  one or more files under a common root directory.  Each file is\n",
      "     |  identified by its ``file identifier``, which is the relative path\n",
      "     |  to the file from the root directory.\n",
      "     |  \n",
      "     |  A separate subclass is defined for each corpus format.  These\n",
      "     |  subclasses define one or more methods that provide 'views' on the\n",
      "     |  corpus contents, such as ``words()`` (for a list of words) and\n",
      "     |  ``parsed_sents()`` (for a list of parsed sentences).  Called with\n",
      "     |  no arguments, these methods will return the contents of the entire\n",
      "     |  corpus.  For most corpora, these methods define one or more\n",
      "     |  selection arguments, such as ``fileids`` or ``categories``, which can\n",
      "     |  be used to select which portion of the corpus should be returned.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      StringCategoryCorpusReader\n",
      "     |      nltk.corpus.reader.api.CorpusReader\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, root, fileids, delimiter=' ', encoding='utf8')\n",
      "     |      :param root: The root directory for this corpus.\n",
      "     |      :param fileids: A list or regexp specifying the fileids in this corpus.\n",
      "     |      :param delimiter: Field delimiter\n",
      "     |  \n",
      "     |  raw(self, fileids=None)\n",
      "     |      :return: the text contents of the given fileids, as a single string.\n",
      "     |  \n",
      "     |  tuples(self, fileids=None)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __unicode__ = __str__(self, /)\n",
      "     |  \n",
      "     |  abspath(self, fileid)\n",
      "     |      Return the absolute path for the given file.\n",
      "     |      \n",
      "     |      :type fileid: str\n",
      "     |      :param fileid: The file identifier for the file whose path\n",
      "     |          should be returned.\n",
      "     |      :rtype: PathPointer\n",
      "     |  \n",
      "     |  abspaths(self, fileids=None, include_encoding=False, include_fileid=False)\n",
      "     |      Return a list of the absolute paths for all fileids in this corpus;\n",
      "     |      or for the given list of fileids, if specified.\n",
      "     |      \n",
      "     |      :type fileids: None or str or list\n",
      "     |      :param fileids: Specifies the set of fileids for which paths should\n",
      "     |          be returned.  Can be None, for all fileids; a list of\n",
      "     |          file identifiers, for a specified set of fileids; or a single\n",
      "     |          file identifier, for a single file.  Note that the return\n",
      "     |          value is always a list of paths, even if ``fileids`` is a\n",
      "     |          single file identifier.\n",
      "     |      \n",
      "     |      :param include_encoding: If true, then return a list of\n",
      "     |          ``(path_pointer, encoding)`` tuples.\n",
      "     |      \n",
      "     |      :rtype: list(PathPointer)\n",
      "     |  \n",
      "     |  citation(self)\n",
      "     |      Return the contents of the corpus citation.bib file, if it exists.\n",
      "     |  \n",
      "     |  encoding(self, file)\n",
      "     |      Return the unicode encoding for the given corpus file, if known.\n",
      "     |      If the encoding is unknown, or if the given file should be\n",
      "     |      processed using byte strings (str), then return None.\n",
      "     |  \n",
      "     |  ensure_loaded(self)\n",
      "     |      Load this corpus (if it has not already been loaded).  This is\n",
      "     |      used by LazyCorpusLoader as a simple method that can be used to\n",
      "     |      make sure a corpus is loaded -- e.g., in case a user wants to\n",
      "     |      do help(some_corpus).\n",
      "     |  \n",
      "     |  fileids(self)\n",
      "     |      Return a list of file identifiers for the fileids that make up\n",
      "     |      this corpus.\n",
      "     |  \n",
      "     |  license(self)\n",
      "     |      Return the contents of the corpus LICENSE file, if it exists.\n",
      "     |  \n",
      "     |  open(self, file)\n",
      "     |      Return an open stream that can be used to read the given file.\n",
      "     |      If the file's encoding is not None, then the stream will\n",
      "     |      automatically decode the file's contents into unicode.\n",
      "     |      \n",
      "     |      :param file: The file identifier of the file to read.\n",
      "     |  \n",
      "     |  readme(self)\n",
      "     |      Return the contents of the corpus README file, if it exists.\n",
      "     |  \n",
      "     |  unicode_repr = __repr__(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  root\n",
      "     |      The directory where this corpus is stored.\n",
      "     |      \n",
      "     |      :type: PathPointer\n",
      "    \n",
      "    class SwadeshCorpusReader(WordListCorpusReader)\n",
      "     |  SwadeshCorpusReader(root, fileids, encoding='utf8', tagset=None)\n",
      "     |  \n",
      "     |  List of words, one per line.  Blank lines are ignored.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      SwadeshCorpusReader\n",
      "     |      WordListCorpusReader\n",
      "     |      nltk.corpus.reader.api.CorpusReader\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  entries(self, fileids=None)\n",
      "     |      :return: a tuple of words for the specified fileids.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from WordListCorpusReader:\n",
      "     |  \n",
      "     |  raw(self, fileids=None)\n",
      "     |  \n",
      "     |  words(self, fileids=None, ignore_lines_startswith='\\n')\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __init__(self, root, fileids, encoding='utf8', tagset=None)\n",
      "     |      :type root: PathPointer or str\n",
      "     |      :param root: A path pointer identifying the root directory for\n",
      "     |          this corpus.  If a string is specified, then it will be\n",
      "     |          converted to a ``PathPointer`` automatically.\n",
      "     |      :param fileids: A list of the files that make up this corpus.\n",
      "     |          This list can either be specified explicitly, as a list of\n",
      "     |          strings; or implicitly, as a regular expression over file\n",
      "     |          paths.  The absolute path for each file will be constructed\n",
      "     |          by joining the reader's root to each file name.\n",
      "     |      :param encoding: The default unicode encoding for the files\n",
      "     |          that make up the corpus.  The value of ``encoding`` can be any\n",
      "     |          of the following:\n",
      "     |          - A string: ``encoding`` is the encoding name for all files.\n",
      "     |          - A dictionary: ``encoding[file_id]`` is the encoding\n",
      "     |            name for the file whose identifier is ``file_id``.  If\n",
      "     |            ``file_id`` is not in ``encoding``, then the file\n",
      "     |            contents will be processed using non-unicode byte strings.\n",
      "     |          - A list: ``encoding`` should be a list of ``(regexp, encoding)``\n",
      "     |            tuples.  The encoding for a file whose identifier is ``file_id``\n",
      "     |            will be the ``encoding`` value for the first tuple whose\n",
      "     |            ``regexp`` matches the ``file_id``.  If no tuple's ``regexp``\n",
      "     |            matches the ``file_id``, the file contents will be processed\n",
      "     |            using non-unicode byte strings.\n",
      "     |          - None: the file contents of all files will be\n",
      "     |            processed using non-unicode byte strings.\n",
      "     |      :param tagset: The name of the tagset used by this corpus, to be used\n",
      "     |            for normalizing or converting the POS tags returned by the\n",
      "     |            tagged_...() methods.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __unicode__ = __str__(self, /)\n",
      "     |  \n",
      "     |  abspath(self, fileid)\n",
      "     |      Return the absolute path for the given file.\n",
      "     |      \n",
      "     |      :type fileid: str\n",
      "     |      :param fileid: The file identifier for the file whose path\n",
      "     |          should be returned.\n",
      "     |      :rtype: PathPointer\n",
      "     |  \n",
      "     |  abspaths(self, fileids=None, include_encoding=False, include_fileid=False)\n",
      "     |      Return a list of the absolute paths for all fileids in this corpus;\n",
      "     |      or for the given list of fileids, if specified.\n",
      "     |      \n",
      "     |      :type fileids: None or str or list\n",
      "     |      :param fileids: Specifies the set of fileids for which paths should\n",
      "     |          be returned.  Can be None, for all fileids; a list of\n",
      "     |          file identifiers, for a specified set of fileids; or a single\n",
      "     |          file identifier, for a single file.  Note that the return\n",
      "     |          value is always a list of paths, even if ``fileids`` is a\n",
      "     |          single file identifier.\n",
      "     |      \n",
      "     |      :param include_encoding: If true, then return a list of\n",
      "     |          ``(path_pointer, encoding)`` tuples.\n",
      "     |      \n",
      "     |      :rtype: list(PathPointer)\n",
      "     |  \n",
      "     |  citation(self)\n",
      "     |      Return the contents of the corpus citation.bib file, if it exists.\n",
      "     |  \n",
      "     |  encoding(self, file)\n",
      "     |      Return the unicode encoding for the given corpus file, if known.\n",
      "     |      If the encoding is unknown, or if the given file should be\n",
      "     |      processed using byte strings (str), then return None.\n",
      "     |  \n",
      "     |  ensure_loaded(self)\n",
      "     |      Load this corpus (if it has not already been loaded).  This is\n",
      "     |      used by LazyCorpusLoader as a simple method that can be used to\n",
      "     |      make sure a corpus is loaded -- e.g., in case a user wants to\n",
      "     |      do help(some_corpus).\n",
      "     |  \n",
      "     |  fileids(self)\n",
      "     |      Return a list of file identifiers for the fileids that make up\n",
      "     |      this corpus.\n",
      "     |  \n",
      "     |  license(self)\n",
      "     |      Return the contents of the corpus LICENSE file, if it exists.\n",
      "     |  \n",
      "     |  open(self, file)\n",
      "     |      Return an open stream that can be used to read the given file.\n",
      "     |      If the file's encoding is not None, then the stream will\n",
      "     |      automatically decode the file's contents into unicode.\n",
      "     |      \n",
      "     |      :param file: The file identifier of the file to read.\n",
      "     |  \n",
      "     |  readme(self)\n",
      "     |      Return the contents of the corpus README file, if it exists.\n",
      "     |  \n",
      "     |  unicode_repr = __repr__(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  root\n",
      "     |      The directory where this corpus is stored.\n",
      "     |      \n",
      "     |      :type: PathPointer\n",
      "    \n",
      "    class SwitchboardCorpusReader(nltk.corpus.reader.api.CorpusReader)\n",
      "     |  SwitchboardCorpusReader(root, tagset=None)\n",
      "     |  \n",
      "     |  A base class for \"corpus reader\" classes, each of which can be\n",
      "     |  used to read a specific corpus format.  Each individual corpus\n",
      "     |  reader instance is used to read a specific corpus, consisting of\n",
      "     |  one or more files under a common root directory.  Each file is\n",
      "     |  identified by its ``file identifier``, which is the relative path\n",
      "     |  to the file from the root directory.\n",
      "     |  \n",
      "     |  A separate subclass is defined for each corpus format.  These\n",
      "     |  subclasses define one or more methods that provide 'views' on the\n",
      "     |  corpus contents, such as ``words()`` (for a list of words) and\n",
      "     |  ``parsed_sents()`` (for a list of parsed sentences).  Called with\n",
      "     |  no arguments, these methods will return the contents of the entire\n",
      "     |  corpus.  For most corpora, these methods define one or more\n",
      "     |  selection arguments, such as ``fileids`` or ``categories``, which can\n",
      "     |  be used to select which portion of the corpus should be returned.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      SwitchboardCorpusReader\n",
      "     |      nltk.corpus.reader.api.CorpusReader\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, root, tagset=None)\n",
      "     |      :type root: PathPointer or str\n",
      "     |      :param root: A path pointer identifying the root directory for\n",
      "     |          this corpus.  If a string is specified, then it will be\n",
      "     |          converted to a ``PathPointer`` automatically.\n",
      "     |      :param fileids: A list of the files that make up this corpus.\n",
      "     |          This list can either be specified explicitly, as a list of\n",
      "     |          strings; or implicitly, as a regular expression over file\n",
      "     |          paths.  The absolute path for each file will be constructed\n",
      "     |          by joining the reader's root to each file name.\n",
      "     |      :param encoding: The default unicode encoding for the files\n",
      "     |          that make up the corpus.  The value of ``encoding`` can be any\n",
      "     |          of the following:\n",
      "     |          - A string: ``encoding`` is the encoding name for all files.\n",
      "     |          - A dictionary: ``encoding[file_id]`` is the encoding\n",
      "     |            name for the file whose identifier is ``file_id``.  If\n",
      "     |            ``file_id`` is not in ``encoding``, then the file\n",
      "     |            contents will be processed using non-unicode byte strings.\n",
      "     |          - A list: ``encoding`` should be a list of ``(regexp, encoding)``\n",
      "     |            tuples.  The encoding for a file whose identifier is ``file_id``\n",
      "     |            will be the ``encoding`` value for the first tuple whose\n",
      "     |            ``regexp`` matches the ``file_id``.  If no tuple's ``regexp``\n",
      "     |            matches the ``file_id``, the file contents will be processed\n",
      "     |            using non-unicode byte strings.\n",
      "     |          - None: the file contents of all files will be\n",
      "     |            processed using non-unicode byte strings.\n",
      "     |      :param tagset: The name of the tagset used by this corpus, to be used\n",
      "     |            for normalizing or converting the POS tags returned by the\n",
      "     |            tagged_...() methods.\n",
      "     |  \n",
      "     |  discourses(self)\n",
      "     |  \n",
      "     |  tagged_discourses(self, tagset=False)\n",
      "     |  \n",
      "     |  tagged_turns(self, tagset=None)\n",
      "     |  \n",
      "     |  tagged_words(self, tagset=None)\n",
      "     |  \n",
      "     |  turns(self)\n",
      "     |  \n",
      "     |  words(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __unicode__ = __str__(self, /)\n",
      "     |  \n",
      "     |  abspath(self, fileid)\n",
      "     |      Return the absolute path for the given file.\n",
      "     |      \n",
      "     |      :type fileid: str\n",
      "     |      :param fileid: The file identifier for the file whose path\n",
      "     |          should be returned.\n",
      "     |      :rtype: PathPointer\n",
      "     |  \n",
      "     |  abspaths(self, fileids=None, include_encoding=False, include_fileid=False)\n",
      "     |      Return a list of the absolute paths for all fileids in this corpus;\n",
      "     |      or for the given list of fileids, if specified.\n",
      "     |      \n",
      "     |      :type fileids: None or str or list\n",
      "     |      :param fileids: Specifies the set of fileids for which paths should\n",
      "     |          be returned.  Can be None, for all fileids; a list of\n",
      "     |          file identifiers, for a specified set of fileids; or a single\n",
      "     |          file identifier, for a single file.  Note that the return\n",
      "     |          value is always a list of paths, even if ``fileids`` is a\n",
      "     |          single file identifier.\n",
      "     |      \n",
      "     |      :param include_encoding: If true, then return a list of\n",
      "     |          ``(path_pointer, encoding)`` tuples.\n",
      "     |      \n",
      "     |      :rtype: list(PathPointer)\n",
      "     |  \n",
      "     |  citation(self)\n",
      "     |      Return the contents of the corpus citation.bib file, if it exists.\n",
      "     |  \n",
      "     |  encoding(self, file)\n",
      "     |      Return the unicode encoding for the given corpus file, if known.\n",
      "     |      If the encoding is unknown, or if the given file should be\n",
      "     |      processed using byte strings (str), then return None.\n",
      "     |  \n",
      "     |  ensure_loaded(self)\n",
      "     |      Load this corpus (if it has not already been loaded).  This is\n",
      "     |      used by LazyCorpusLoader as a simple method that can be used to\n",
      "     |      make sure a corpus is loaded -- e.g., in case a user wants to\n",
      "     |      do help(some_corpus).\n",
      "     |  \n",
      "     |  fileids(self)\n",
      "     |      Return a list of file identifiers for the fileids that make up\n",
      "     |      this corpus.\n",
      "     |  \n",
      "     |  license(self)\n",
      "     |      Return the contents of the corpus LICENSE file, if it exists.\n",
      "     |  \n",
      "     |  open(self, file)\n",
      "     |      Return an open stream that can be used to read the given file.\n",
      "     |      If the file's encoding is not None, then the stream will\n",
      "     |      automatically decode the file's contents into unicode.\n",
      "     |      \n",
      "     |      :param file: The file identifier of the file to read.\n",
      "     |  \n",
      "     |  readme(self)\n",
      "     |      Return the contents of the corpus README file, if it exists.\n",
      "     |  \n",
      "     |  unicode_repr = __repr__(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  root\n",
      "     |      The directory where this corpus is stored.\n",
      "     |      \n",
      "     |      :type: PathPointer\n",
      "    \n",
      "    class SyntaxCorpusReader(CorpusReader)\n",
      "     |  SyntaxCorpusReader(root, fileids, encoding='utf8', tagset=None)\n",
      "     |  \n",
      "     |  An abstract base class for reading corpora consisting of\n",
      "     |  syntactically parsed text.  Subclasses should define:\n",
      "     |  \n",
      "     |    - ``__init__``, which specifies the location of the corpus\n",
      "     |      and a method for detecting the sentence blocks in corpus files.\n",
      "     |    - ``_read_block``, which reads a block from the input stream.\n",
      "     |    - ``_word``, which takes a block and returns a list of list of words.\n",
      "     |    - ``_tag``, which takes a block and returns a list of list of tagged\n",
      "     |      words.\n",
      "     |    - ``_parse``, which takes a block and returns a list of parsed\n",
      "     |      sentences.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      SyntaxCorpusReader\n",
      "     |      CorpusReader\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  parsed_sents(self, fileids=None)\n",
      "     |  \n",
      "     |  raw(self, fileids=None)\n",
      "     |  \n",
      "     |  sents(self, fileids=None)\n",
      "     |  \n",
      "     |  tagged_sents(self, fileids=None, tagset=None)\n",
      "     |  \n",
      "     |  tagged_words(self, fileids=None, tagset=None)\n",
      "     |  \n",
      "     |  words(self, fileids=None)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from CorpusReader:\n",
      "     |  \n",
      "     |  __init__(self, root, fileids, encoding='utf8', tagset=None)\n",
      "     |      :type root: PathPointer or str\n",
      "     |      :param root: A path pointer identifying the root directory for\n",
      "     |          this corpus.  If a string is specified, then it will be\n",
      "     |          converted to a ``PathPointer`` automatically.\n",
      "     |      :param fileids: A list of the files that make up this corpus.\n",
      "     |          This list can either be specified explicitly, as a list of\n",
      "     |          strings; or implicitly, as a regular expression over file\n",
      "     |          paths.  The absolute path for each file will be constructed\n",
      "     |          by joining the reader's root to each file name.\n",
      "     |      :param encoding: The default unicode encoding for the files\n",
      "     |          that make up the corpus.  The value of ``encoding`` can be any\n",
      "     |          of the following:\n",
      "     |          - A string: ``encoding`` is the encoding name for all files.\n",
      "     |          - A dictionary: ``encoding[file_id]`` is the encoding\n",
      "     |            name for the file whose identifier is ``file_id``.  If\n",
      "     |            ``file_id`` is not in ``encoding``, then the file\n",
      "     |            contents will be processed using non-unicode byte strings.\n",
      "     |          - A list: ``encoding`` should be a list of ``(regexp, encoding)``\n",
      "     |            tuples.  The encoding for a file whose identifier is ``file_id``\n",
      "     |            will be the ``encoding`` value for the first tuple whose\n",
      "     |            ``regexp`` matches the ``file_id``.  If no tuple's ``regexp``\n",
      "     |            matches the ``file_id``, the file contents will be processed\n",
      "     |            using non-unicode byte strings.\n",
      "     |          - None: the file contents of all files will be\n",
      "     |            processed using non-unicode byte strings.\n",
      "     |      :param tagset: The name of the tagset used by this corpus, to be used\n",
      "     |            for normalizing or converting the POS tags returned by the\n",
      "     |            tagged_...() methods.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __unicode__ = __str__(self, /)\n",
      "     |  \n",
      "     |  abspath(self, fileid)\n",
      "     |      Return the absolute path for the given file.\n",
      "     |      \n",
      "     |      :type fileid: str\n",
      "     |      :param fileid: The file identifier for the file whose path\n",
      "     |          should be returned.\n",
      "     |      :rtype: PathPointer\n",
      "     |  \n",
      "     |  abspaths(self, fileids=None, include_encoding=False, include_fileid=False)\n",
      "     |      Return a list of the absolute paths for all fileids in this corpus;\n",
      "     |      or for the given list of fileids, if specified.\n",
      "     |      \n",
      "     |      :type fileids: None or str or list\n",
      "     |      :param fileids: Specifies the set of fileids for which paths should\n",
      "     |          be returned.  Can be None, for all fileids; a list of\n",
      "     |          file identifiers, for a specified set of fileids; or a single\n",
      "     |          file identifier, for a single file.  Note that the return\n",
      "     |          value is always a list of paths, even if ``fileids`` is a\n",
      "     |          single file identifier.\n",
      "     |      \n",
      "     |      :param include_encoding: If true, then return a list of\n",
      "     |          ``(path_pointer, encoding)`` tuples.\n",
      "     |      \n",
      "     |      :rtype: list(PathPointer)\n",
      "     |  \n",
      "     |  citation(self)\n",
      "     |      Return the contents of the corpus citation.bib file, if it exists.\n",
      "     |  \n",
      "     |  encoding(self, file)\n",
      "     |      Return the unicode encoding for the given corpus file, if known.\n",
      "     |      If the encoding is unknown, or if the given file should be\n",
      "     |      processed using byte strings (str), then return None.\n",
      "     |  \n",
      "     |  ensure_loaded(self)\n",
      "     |      Load this corpus (if it has not already been loaded).  This is\n",
      "     |      used by LazyCorpusLoader as a simple method that can be used to\n",
      "     |      make sure a corpus is loaded -- e.g., in case a user wants to\n",
      "     |      do help(some_corpus).\n",
      "     |  \n",
      "     |  fileids(self)\n",
      "     |      Return a list of file identifiers for the fileids that make up\n",
      "     |      this corpus.\n",
      "     |  \n",
      "     |  license(self)\n",
      "     |      Return the contents of the corpus LICENSE file, if it exists.\n",
      "     |  \n",
      "     |  open(self, file)\n",
      "     |      Return an open stream that can be used to read the given file.\n",
      "     |      If the file's encoding is not None, then the stream will\n",
      "     |      automatically decode the file's contents into unicode.\n",
      "     |      \n",
      "     |      :param file: The file identifier of the file to read.\n",
      "     |  \n",
      "     |  readme(self)\n",
      "     |      Return the contents of the corpus README file, if it exists.\n",
      "     |  \n",
      "     |  unicode_repr = __repr__(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from CorpusReader:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  root\n",
      "     |      The directory where this corpus is stored.\n",
      "     |      \n",
      "     |      :type: PathPointer\n",
      "    \n",
      "    class TEICorpusView(nltk.corpus.reader.util.StreamBackedCorpusView)\n",
      "     |  TEICorpusView(corpus_file, tagged, group_by_sent, group_by_para, tagset=None, head_len=0, textids=None)\n",
      "     |  \n",
      "     |  A 'view' of a corpus file, which acts like a sequence of tokens:\n",
      "     |  it can be accessed by index, iterated over, etc.  However, the\n",
      "     |  tokens are only constructed as-needed -- the entire corpus is\n",
      "     |  never stored in memory at once.\n",
      "     |  \n",
      "     |  The constructor to ``StreamBackedCorpusView`` takes two arguments:\n",
      "     |  a corpus fileid (specified as a string or as a ``PathPointer``);\n",
      "     |  and a block reader.  A \"block reader\" is a function that reads\n",
      "     |  zero or more tokens from a stream, and returns them as a list.  A\n",
      "     |  very simple example of a block reader is:\n",
      "     |  \n",
      "     |      >>> def simple_block_reader(stream):\n",
      "     |      ...     return stream.readline().split()\n",
      "     |  \n",
      "     |  This simple block reader reads a single line at a time, and\n",
      "     |  returns a single token (consisting of a string) for each\n",
      "     |  whitespace-separated substring on the line.\n",
      "     |  \n",
      "     |  When deciding how to define the block reader for a given\n",
      "     |  corpus, careful consideration should be given to the size of\n",
      "     |  blocks handled by the block reader.  Smaller block sizes will\n",
      "     |  increase the memory requirements of the corpus view's internal\n",
      "     |  data structures (by 2 integers per block).  On the other hand,\n",
      "     |  larger block sizes may decrease performance for random access to\n",
      "     |  the corpus.  (But note that larger block sizes will *not*\n",
      "     |  decrease performance for iteration.)\n",
      "     |  \n",
      "     |  Internally, ``CorpusView`` maintains a partial mapping from token\n",
      "     |  index to file position, with one entry per block.  When a token\n",
      "     |  with a given index *i* is requested, the ``CorpusView`` constructs\n",
      "     |  it as follows:\n",
      "     |  \n",
      "     |    1. First, it searches the toknum/filepos mapping for the token\n",
      "     |       index closest to (but less than or equal to) *i*.\n",
      "     |  \n",
      "     |    2. Then, starting at the file position corresponding to that\n",
      "     |       index, it reads one block at a time using the block reader\n",
      "     |       until it reaches the requested token.\n",
      "     |  \n",
      "     |  The toknum/filepos mapping is created lazily: it is initially\n",
      "     |  empty, but every time a new block is read, the block's\n",
      "     |  initial token is added to the mapping.  (Thus, the toknum/filepos\n",
      "     |  map has one entry per block.)\n",
      "     |  \n",
      "     |  In order to increase efficiency for random access patterns that\n",
      "     |  have high degrees of locality, the corpus view may cache one or\n",
      "     |  more blocks.\n",
      "     |  \n",
      "     |  :note: Each ``CorpusView`` object internally maintains an open file\n",
      "     |      object for its underlying corpus file.  This file should be\n",
      "     |      automatically closed when the ``CorpusView`` is garbage collected,\n",
      "     |      but if you wish to close it manually, use the ``close()``\n",
      "     |      method.  If you access a ``CorpusView``'s items after it has been\n",
      "     |      closed, the file object will be automatically re-opened.\n",
      "     |  \n",
      "     |  :warning: If the contents of the file are modified during the\n",
      "     |      lifetime of the ``CorpusView``, then the ``CorpusView``'s behavior\n",
      "     |      is undefined.\n",
      "     |  \n",
      "     |  :warning: If a unicode encoding is specified when constructing a\n",
      "     |      ``CorpusView``, then the block reader may only call\n",
      "     |      ``stream.seek()`` with offsets that have been returned by\n",
      "     |      ``stream.tell()``; in particular, calling ``stream.seek()`` with\n",
      "     |      relative offsets, or with offsets based on string lengths, may\n",
      "     |      lead to incorrect behavior.\n",
      "     |  \n",
      "     |  :ivar _block_reader: The function used to read\n",
      "     |      a single block from the underlying file stream.\n",
      "     |  :ivar _toknum: A list containing the token index of each block\n",
      "     |      that has been processed.  In particular, ``_toknum[i]`` is the\n",
      "     |      token index of the first token in block ``i``.  Together\n",
      "     |      with ``_filepos``, this forms a partial mapping between token\n",
      "     |      indices and file positions.\n",
      "     |  :ivar _filepos: A list containing the file position of each block\n",
      "     |      that has been processed.  In particular, ``_toknum[i]`` is the\n",
      "     |      file position of the first character in block ``i``.  Together\n",
      "     |      with ``_toknum``, this forms a partial mapping between token\n",
      "     |      indices and file positions.\n",
      "     |  :ivar _stream: The stream used to access the underlying corpus file.\n",
      "     |  :ivar _len: The total number of tokens in the corpus, if known;\n",
      "     |      or None, if the number of tokens is not yet known.\n",
      "     |  :ivar _eofpos: The character position of the last character in the\n",
      "     |      file.  This is calculated when the corpus view is initialized,\n",
      "     |      and is used to decide when the end of file has been reached.\n",
      "     |  :ivar _cache: A cache of the most recently read block.  It\n",
      "     |     is encoded as a tuple (start_toknum, end_toknum, tokens), where\n",
      "     |     start_toknum is the token index of the first token in the block;\n",
      "     |     end_toknum is the token index of the first token not in the\n",
      "     |     block; and tokens is a list of the tokens in the block.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      TEICorpusView\n",
      "     |      nltk.corpus.reader.util.StreamBackedCorpusView\n",
      "     |      nltk.collections.AbstractLazySequence\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, corpus_file, tagged, group_by_sent, group_by_para, tagset=None, head_len=0, textids=None)\n",
      "     |      Create a new corpus view, based on the file ``fileid``, and\n",
      "     |      read with ``block_reader``.  See the class documentation\n",
      "     |      for more information.\n",
      "     |      \n",
      "     |      :param fileid: The path to the file that is read by this\n",
      "     |          corpus view.  ``fileid`` can either be a string or a\n",
      "     |          ``PathPointer``.\n",
      "     |      \n",
      "     |      :param startpos: The file position at which the view will\n",
      "     |          start reading.  This can be used to skip over preface\n",
      "     |          sections.\n",
      "     |      \n",
      "     |      :param encoding: The unicode encoding that should be used to\n",
      "     |          read the file's contents.  If no encoding is specified,\n",
      "     |          then the file's contents will be read as a non-unicode\n",
      "     |          string (i.e., a str).\n",
      "     |  \n",
      "     |  read_block(self, stream)\n",
      "     |      Read a block from the input stream.\n",
      "     |      \n",
      "     |      :return: a block of tokens from the input stream\n",
      "     |      :rtype: list(any)\n",
      "     |      :param stream: an input stream\n",
      "     |      :type stream: stream\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from nltk.corpus.reader.util.StreamBackedCorpusView:\n",
      "     |  \n",
      "     |  __add__(self, other)\n",
      "     |      Return a list concatenating self with other.\n",
      "     |  \n",
      "     |  __getitem__(self, i)\n",
      "     |      Return the *i* th token in the corpus file underlying this\n",
      "     |      corpus view.  Negative indices and spans are both supported.\n",
      "     |  \n",
      "     |  __len__(self)\n",
      "     |      Return the number of tokens in the corpus file underlying this\n",
      "     |      corpus view.\n",
      "     |  \n",
      "     |  __mul__(self, count)\n",
      "     |      Return a list concatenating self with itself ``count`` times.\n",
      "     |  \n",
      "     |  __radd__(self, other)\n",
      "     |      Return a list concatenating other with self.\n",
      "     |  \n",
      "     |  __rmul__(self, count)\n",
      "     |      Return a list concatenating self with itself ``count`` times.\n",
      "     |  \n",
      "     |  close(self)\n",
      "     |      Close the file stream associated with this corpus view.  This\n",
      "     |      can be useful if you are worried about running out of file\n",
      "     |      handles (although the stream should automatically be closed\n",
      "     |      upon garbage collection of the corpus view).  If the corpus\n",
      "     |      view is accessed after it is closed, it will be automatically\n",
      "     |      re-opened.\n",
      "     |  \n",
      "     |  iterate_from(self, start_tok)\n",
      "     |      Return an iterator that generates the tokens in the corpus\n",
      "     |      file underlying this corpus view, starting at the token number\n",
      "     |      ``start``.  If ``start>=len(self)``, then this iterator will\n",
      "     |      generate no tokens.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from nltk.corpus.reader.util.StreamBackedCorpusView:\n",
      "     |  \n",
      "     |  fileid\n",
      "     |      The fileid of the file that is accessed by this view.\n",
      "     |      \n",
      "     |      :type: str or PathPointer\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from nltk.collections.AbstractLazySequence:\n",
      "     |  \n",
      "     |  __contains__(self, value)\n",
      "     |      Return true if this list contains ``value``.\n",
      "     |  \n",
      "     |  __eq__(self, other)\n",
      "     |      Return self==value.\n",
      "     |  \n",
      "     |  __ge__(self, other, NotImplemented=NotImplemented)\n",
      "     |      Return a >= b.  Computed by @total_ordering from (not a < b).\n",
      "     |  \n",
      "     |  __gt__(self, other, NotImplemented=NotImplemented)\n",
      "     |      Return a > b.  Computed by @total_ordering from (not a < b) and (a != b).\n",
      "     |  \n",
      "     |  __hash__(self)\n",
      "     |      :raise ValueError: Corpus view objects are unhashable.\n",
      "     |  \n",
      "     |  __iter__(self)\n",
      "     |      Return an iterator that generates the tokens in the corpus\n",
      "     |      file underlying this corpus view.\n",
      "     |  \n",
      "     |  __le__(self, other, NotImplemented=NotImplemented)\n",
      "     |      Return a <= b.  Computed by @total_ordering from (a < b) or (a == b).\n",
      "     |  \n",
      "     |  __lt__(self, other)\n",
      "     |      Return self<value.\n",
      "     |  \n",
      "     |  __ne__(self, other)\n",
      "     |      Return self!=value.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return a string representation for this corpus view that is\n",
      "     |      similar to a list's representation; but if it would be more\n",
      "     |      than 60 characters long, it is truncated.\n",
      "     |  \n",
      "     |  __unicode__ = __str__(self, /)\n",
      "     |  \n",
      "     |  count(self, value)\n",
      "     |      Return the number of times this list contains ``value``.\n",
      "     |  \n",
      "     |  index(self, value, start=None, stop=None)\n",
      "     |      Return the index of the first occurrence of ``value`` in this\n",
      "     |      list that is greater than or equal to ``start`` and less than\n",
      "     |      ``stop``.  Negative start and stop values are treated like negative\n",
      "     |      slice bounds -- i.e., they count from the end of the list.\n",
      "     |  \n",
      "     |  unicode_repr = __repr__(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from nltk.collections.AbstractLazySequence:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class TaggedCorpusReader(nltk.corpus.reader.api.CorpusReader)\n",
      "     |  TaggedCorpusReader(root, fileids, sep='/', word_tokenizer=WhitespaceTokenizer(pattern='\\\\s+', gaps=True, discard_empty=True, flags=<RegexFlag.UNICODE|DOTALL|MULTILINE: 56>), sent_tokenizer=RegexpTokenizer(pattern='\\n', gaps=True, discard_empty=True, flags=<RegexFlag.UNICODE|DOTALL|MULTILINE: 56>), para_block_reader=<function read_blankline_block at 0x125911320>, encoding='utf8', tagset=None)\n",
      "     |  \n",
      "     |  Reader for simple part-of-speech tagged corpora.  Paragraphs are\n",
      "     |  assumed to be split using blank lines.  Sentences and words can be\n",
      "     |  tokenized using the default tokenizers, or by custom tokenizers\n",
      "     |  specified as parameters to the constructor.  Words are parsed\n",
      "     |  using ``nltk.tag.str2tuple``.  By default, ``'/'`` is used as the\n",
      "     |  separator.  I.e., words should have the form::\n",
      "     |  \n",
      "     |     word1/tag1 word2/tag2 word3/tag3 ...\n",
      "     |  \n",
      "     |  But custom separators may be specified as parameters to the\n",
      "     |  constructor.  Part of speech tags are case-normalized to upper\n",
      "     |  case.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      TaggedCorpusReader\n",
      "     |      nltk.corpus.reader.api.CorpusReader\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, root, fileids, sep='/', word_tokenizer=WhitespaceTokenizer(pattern='\\\\s+', gaps=True, discard_empty=True, flags=<RegexFlag.UNICODE|DOTALL|MULTILINE: 56>), sent_tokenizer=RegexpTokenizer(pattern='\\n', gaps=True, discard_empty=True, flags=<RegexFlag.UNICODE|DOTALL|MULTILINE: 56>), para_block_reader=<function read_blankline_block at 0x125911320>, encoding='utf8', tagset=None)\n",
      "     |      Construct a new Tagged Corpus reader for a set of documents\n",
      "     |      located at the given root directory.  Example usage:\n",
      "     |      \n",
      "     |          >>> root = '/...path to corpus.../'\n",
      "     |          >>> reader = TaggedCorpusReader(root, '.*', '.txt') # doctest: +SKIP\n",
      "     |      \n",
      "     |      :param root: The root directory for this corpus.\n",
      "     |      :param fileids: A list or regexp specifying the fileids in this corpus.\n",
      "     |  \n",
      "     |  paras(self, fileids=None)\n",
      "     |      :return: the given file(s) as a list of\n",
      "     |          paragraphs, each encoded as a list of sentences, which are\n",
      "     |          in turn encoded as lists of word strings.\n",
      "     |      :rtype: list(list(list(str)))\n",
      "     |  \n",
      "     |  raw(self, fileids=None)\n",
      "     |      :return: the given file(s) as a single string.\n",
      "     |      :rtype: str\n",
      "     |  \n",
      "     |  sents(self, fileids=None)\n",
      "     |      :return: the given file(s) as a list of\n",
      "     |          sentences or utterances, each encoded as a list of word\n",
      "     |          strings.\n",
      "     |      :rtype: list(list(str))\n",
      "     |  \n",
      "     |  tagged_paras(self, fileids=None, tagset=None)\n",
      "     |      :return: the given file(s) as a list of\n",
      "     |          paragraphs, each encoded as a list of sentences, which are\n",
      "     |          in turn encoded as lists of ``(word,tag)`` tuples.\n",
      "     |      :rtype: list(list(list(tuple(str,str))))\n",
      "     |  \n",
      "     |  tagged_sents(self, fileids=None, tagset=None)\n",
      "     |      :return: the given file(s) as a list of\n",
      "     |          sentences, each encoded as a list of ``(word,tag)`` tuples.\n",
      "     |      \n",
      "     |      :rtype: list(list(tuple(str,str)))\n",
      "     |  \n",
      "     |  tagged_words(self, fileids=None, tagset=None)\n",
      "     |      :return: the given file(s) as a list of tagged\n",
      "     |          words and punctuation symbols, encoded as tuples\n",
      "     |          ``(word,tag)``.\n",
      "     |      :rtype: list(tuple(str,str))\n",
      "     |  \n",
      "     |  words(self, fileids=None)\n",
      "     |      :return: the given file(s) as a list of words\n",
      "     |          and punctuation symbols.\n",
      "     |      :rtype: list(str)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __unicode__ = __str__(self, /)\n",
      "     |  \n",
      "     |  abspath(self, fileid)\n",
      "     |      Return the absolute path for the given file.\n",
      "     |      \n",
      "     |      :type fileid: str\n",
      "     |      :param fileid: The file identifier for the file whose path\n",
      "     |          should be returned.\n",
      "     |      :rtype: PathPointer\n",
      "     |  \n",
      "     |  abspaths(self, fileids=None, include_encoding=False, include_fileid=False)\n",
      "     |      Return a list of the absolute paths for all fileids in this corpus;\n",
      "     |      or for the given list of fileids, if specified.\n",
      "     |      \n",
      "     |      :type fileids: None or str or list\n",
      "     |      :param fileids: Specifies the set of fileids for which paths should\n",
      "     |          be returned.  Can be None, for all fileids; a list of\n",
      "     |          file identifiers, for a specified set of fileids; or a single\n",
      "     |          file identifier, for a single file.  Note that the return\n",
      "     |          value is always a list of paths, even if ``fileids`` is a\n",
      "     |          single file identifier.\n",
      "     |      \n",
      "     |      :param include_encoding: If true, then return a list of\n",
      "     |          ``(path_pointer, encoding)`` tuples.\n",
      "     |      \n",
      "     |      :rtype: list(PathPointer)\n",
      "     |  \n",
      "     |  citation(self)\n",
      "     |      Return the contents of the corpus citation.bib file, if it exists.\n",
      "     |  \n",
      "     |  encoding(self, file)\n",
      "     |      Return the unicode encoding for the given corpus file, if known.\n",
      "     |      If the encoding is unknown, or if the given file should be\n",
      "     |      processed using byte strings (str), then return None.\n",
      "     |  \n",
      "     |  ensure_loaded(self)\n",
      "     |      Load this corpus (if it has not already been loaded).  This is\n",
      "     |      used by LazyCorpusLoader as a simple method that can be used to\n",
      "     |      make sure a corpus is loaded -- e.g., in case a user wants to\n",
      "     |      do help(some_corpus).\n",
      "     |  \n",
      "     |  fileids(self)\n",
      "     |      Return a list of file identifiers for the fileids that make up\n",
      "     |      this corpus.\n",
      "     |  \n",
      "     |  license(self)\n",
      "     |      Return the contents of the corpus LICENSE file, if it exists.\n",
      "     |  \n",
      "     |  open(self, file)\n",
      "     |      Return an open stream that can be used to read the given file.\n",
      "     |      If the file's encoding is not None, then the stream will\n",
      "     |      automatically decode the file's contents into unicode.\n",
      "     |      \n",
      "     |      :param file: The file identifier of the file to read.\n",
      "     |  \n",
      "     |  readme(self)\n",
      "     |      Return the contents of the corpus README file, if it exists.\n",
      "     |  \n",
      "     |  unicode_repr = __repr__(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  root\n",
      "     |      The directory where this corpus is stored.\n",
      "     |      \n",
      "     |      :type: PathPointer\n",
      "    \n",
      "    class TimitCorpusReader(nltk.corpus.reader.api.CorpusReader)\n",
      "     |  TimitCorpusReader(root, encoding='utf8')\n",
      "     |  \n",
      "     |  Reader for the TIMIT corpus (or any other corpus with the same\n",
      "     |  file layout and use of file formats).  The corpus root directory\n",
      "     |  should contain the following files:\n",
      "     |  \n",
      "     |    - timitdic.txt: dictionary of standard transcriptions\n",
      "     |    - spkrinfo.txt: table of speaker information\n",
      "     |  \n",
      "     |  In addition, the root directory should contain one subdirectory\n",
      "     |  for each speaker, containing three files for each utterance:\n",
      "     |  \n",
      "     |    - <utterance-id>.txt: text content of utterances\n",
      "     |    - <utterance-id>.wrd: tokenized text content of utterances\n",
      "     |    - <utterance-id>.phn: phonetic transcription of utterances\n",
      "     |    - <utterance-id>.wav: utterance sound file\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      TimitCorpusReader\n",
      "     |      nltk.corpus.reader.api.CorpusReader\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, root, encoding='utf8')\n",
      "     |      Construct a new TIMIT corpus reader in the given directory.\n",
      "     |      :param root: The root directory for this corpus.\n",
      "     |  \n",
      "     |  audiodata(self, utterance, start=0, end=None)\n",
      "     |  \n",
      "     |  fileids(self, filetype=None)\n",
      "     |      Return a list of file identifiers for the files that make up\n",
      "     |      this corpus.\n",
      "     |      \n",
      "     |      :param filetype: If specified, then ``filetype`` indicates that\n",
      "     |          only the files that have the given type should be\n",
      "     |          returned.  Accepted values are: ``txt``, ``wrd``, ``phn``,\n",
      "     |          ``wav``, or ``metadata``,\n",
      "     |  \n",
      "     |  phone_times(self, utterances=None)\n",
      "     |      offset is represented as a number of 16kHz samples!\n",
      "     |  \n",
      "     |  phone_trees(self, utterances=None)\n",
      "     |  \n",
      "     |  phones(self, utterances=None)\n",
      "     |  \n",
      "     |  play(self, utterance, start=0, end=None)\n",
      "     |      Play the given audio sample.\n",
      "     |      \n",
      "     |      :param utterance: The utterance id of the sample to play\n",
      "     |  \n",
      "     |  sent_times(self, utterances=None)\n",
      "     |  \n",
      "     |  sentid(self, utterance)\n",
      "     |  \n",
      "     |  sents(self, utterances=None)\n",
      "     |  \n",
      "     |  spkrid(self, utterance)\n",
      "     |  \n",
      "     |  spkrinfo(self, speaker)\n",
      "     |      :return: A dictionary mapping .. something.\n",
      "     |  \n",
      "     |  spkrutteranceids(self, speaker)\n",
      "     |      :return: A list of all utterances associated with a given\n",
      "     |      speaker.\n",
      "     |  \n",
      "     |  transcription_dict(self)\n",
      "     |      :return: A dictionary giving the 'standard' transcription for\n",
      "     |      each word.\n",
      "     |  \n",
      "     |  utterance(self, spkrid, sentid)\n",
      "     |  \n",
      "     |  utteranceids(self, dialect=None, sex=None, spkrid=None, sent_type=None, sentid=None)\n",
      "     |      :return: A list of the utterance identifiers for all\n",
      "     |      utterances in this corpus, or for the given speaker, dialect\n",
      "     |      region, gender, sentence type, or sentence number, if\n",
      "     |      specified.\n",
      "     |  \n",
      "     |  wav(self, utterance, start=0, end=None)\n",
      "     |      # [xx] NOTE: This is currently broken -- we're assuming that the\n",
      "     |      # fileids are WAV fileids (aka RIFF), but they're actually NIST SPHERE\n",
      "     |      # fileids.\n",
      "     |  \n",
      "     |  word_times(self, utterances=None)\n",
      "     |  \n",
      "     |  words(self, utterances=None)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __unicode__ = __str__(self, /)\n",
      "     |  \n",
      "     |  abspath(self, fileid)\n",
      "     |      Return the absolute path for the given file.\n",
      "     |      \n",
      "     |      :type fileid: str\n",
      "     |      :param fileid: The file identifier for the file whose path\n",
      "     |          should be returned.\n",
      "     |      :rtype: PathPointer\n",
      "     |  \n",
      "     |  abspaths(self, fileids=None, include_encoding=False, include_fileid=False)\n",
      "     |      Return a list of the absolute paths for all fileids in this corpus;\n",
      "     |      or for the given list of fileids, if specified.\n",
      "     |      \n",
      "     |      :type fileids: None or str or list\n",
      "     |      :param fileids: Specifies the set of fileids for which paths should\n",
      "     |          be returned.  Can be None, for all fileids; a list of\n",
      "     |          file identifiers, for a specified set of fileids; or a single\n",
      "     |          file identifier, for a single file.  Note that the return\n",
      "     |          value is always a list of paths, even if ``fileids`` is a\n",
      "     |          single file identifier.\n",
      "     |      \n",
      "     |      :param include_encoding: If true, then return a list of\n",
      "     |          ``(path_pointer, encoding)`` tuples.\n",
      "     |      \n",
      "     |      :rtype: list(PathPointer)\n",
      "     |  \n",
      "     |  citation(self)\n",
      "     |      Return the contents of the corpus citation.bib file, if it exists.\n",
      "     |  \n",
      "     |  encoding(self, file)\n",
      "     |      Return the unicode encoding for the given corpus file, if known.\n",
      "     |      If the encoding is unknown, or if the given file should be\n",
      "     |      processed using byte strings (str), then return None.\n",
      "     |  \n",
      "     |  ensure_loaded(self)\n",
      "     |      Load this corpus (if it has not already been loaded).  This is\n",
      "     |      used by LazyCorpusLoader as a simple method that can be used to\n",
      "     |      make sure a corpus is loaded -- e.g., in case a user wants to\n",
      "     |      do help(some_corpus).\n",
      "     |  \n",
      "     |  license(self)\n",
      "     |      Return the contents of the corpus LICENSE file, if it exists.\n",
      "     |  \n",
      "     |  open(self, file)\n",
      "     |      Return an open stream that can be used to read the given file.\n",
      "     |      If the file's encoding is not None, then the stream will\n",
      "     |      automatically decode the file's contents into unicode.\n",
      "     |      \n",
      "     |      :param file: The file identifier of the file to read.\n",
      "     |  \n",
      "     |  readme(self)\n",
      "     |      Return the contents of the corpus README file, if it exists.\n",
      "     |  \n",
      "     |  unicode_repr = __repr__(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  root\n",
      "     |      The directory where this corpus is stored.\n",
      "     |      \n",
      "     |      :type: PathPointer\n",
      "    \n",
      "    class TimitTaggedCorpusReader(TaggedCorpusReader)\n",
      "     |  TimitTaggedCorpusReader(*args, **kwargs)\n",
      "     |  \n",
      "     |  A corpus reader for tagged sentences that are included in the TIMIT corpus.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      TimitTaggedCorpusReader\n",
      "     |      TaggedCorpusReader\n",
      "     |      nltk.corpus.reader.api.CorpusReader\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, *args, **kwargs)\n",
      "     |      Construct a new Tagged Corpus reader for a set of documents\n",
      "     |      located at the given root directory.  Example usage:\n",
      "     |      \n",
      "     |          >>> root = '/...path to corpus.../'\n",
      "     |          >>> reader = TaggedCorpusReader(root, '.*', '.txt') # doctest: +SKIP\n",
      "     |      \n",
      "     |      :param root: The root directory for this corpus.\n",
      "     |      :param fileids: A list or regexp specifying the fileids in this corpus.\n",
      "     |  \n",
      "     |  paras(self)\n",
      "     |      :return: the given file(s) as a list of\n",
      "     |          paragraphs, each encoded as a list of sentences, which are\n",
      "     |          in turn encoded as lists of word strings.\n",
      "     |      :rtype: list(list(list(str)))\n",
      "     |  \n",
      "     |  tagged_paras(self)\n",
      "     |      :return: the given file(s) as a list of\n",
      "     |          paragraphs, each encoded as a list of sentences, which are\n",
      "     |          in turn encoded as lists of ``(word,tag)`` tuples.\n",
      "     |      :rtype: list(list(list(tuple(str,str))))\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from TaggedCorpusReader:\n",
      "     |  \n",
      "     |  raw(self, fileids=None)\n",
      "     |      :return: the given file(s) as a single string.\n",
      "     |      :rtype: str\n",
      "     |  \n",
      "     |  sents(self, fileids=None)\n",
      "     |      :return: the given file(s) as a list of\n",
      "     |          sentences or utterances, each encoded as a list of word\n",
      "     |          strings.\n",
      "     |      :rtype: list(list(str))\n",
      "     |  \n",
      "     |  tagged_sents(self, fileids=None, tagset=None)\n",
      "     |      :return: the given file(s) as a list of\n",
      "     |          sentences, each encoded as a list of ``(word,tag)`` tuples.\n",
      "     |      \n",
      "     |      :rtype: list(list(tuple(str,str)))\n",
      "     |  \n",
      "     |  tagged_words(self, fileids=None, tagset=None)\n",
      "     |      :return: the given file(s) as a list of tagged\n",
      "     |          words and punctuation symbols, encoded as tuples\n",
      "     |          ``(word,tag)``.\n",
      "     |      :rtype: list(tuple(str,str))\n",
      "     |  \n",
      "     |  words(self, fileids=None)\n",
      "     |      :return: the given file(s) as a list of words\n",
      "     |          and punctuation symbols.\n",
      "     |      :rtype: list(str)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __unicode__ = __str__(self, /)\n",
      "     |  \n",
      "     |  abspath(self, fileid)\n",
      "     |      Return the absolute path for the given file.\n",
      "     |      \n",
      "     |      :type fileid: str\n",
      "     |      :param fileid: The file identifier for the file whose path\n",
      "     |          should be returned.\n",
      "     |      :rtype: PathPointer\n",
      "     |  \n",
      "     |  abspaths(self, fileids=None, include_encoding=False, include_fileid=False)\n",
      "     |      Return a list of the absolute paths for all fileids in this corpus;\n",
      "     |      or for the given list of fileids, if specified.\n",
      "     |      \n",
      "     |      :type fileids: None or str or list\n",
      "     |      :param fileids: Specifies the set of fileids for which paths should\n",
      "     |          be returned.  Can be None, for all fileids; a list of\n",
      "     |          file identifiers, for a specified set of fileids; or a single\n",
      "     |          file identifier, for a single file.  Note that the return\n",
      "     |          value is always a list of paths, even if ``fileids`` is a\n",
      "     |          single file identifier.\n",
      "     |      \n",
      "     |      :param include_encoding: If true, then return a list of\n",
      "     |          ``(path_pointer, encoding)`` tuples.\n",
      "     |      \n",
      "     |      :rtype: list(PathPointer)\n",
      "     |  \n",
      "     |  citation(self)\n",
      "     |      Return the contents of the corpus citation.bib file, if it exists.\n",
      "     |  \n",
      "     |  encoding(self, file)\n",
      "     |      Return the unicode encoding for the given corpus file, if known.\n",
      "     |      If the encoding is unknown, or if the given file should be\n",
      "     |      processed using byte strings (str), then return None.\n",
      "     |  \n",
      "     |  ensure_loaded(self)\n",
      "     |      Load this corpus (if it has not already been loaded).  This is\n",
      "     |      used by LazyCorpusLoader as a simple method that can be used to\n",
      "     |      make sure a corpus is loaded -- e.g., in case a user wants to\n",
      "     |      do help(some_corpus).\n",
      "     |  \n",
      "     |  fileids(self)\n",
      "     |      Return a list of file identifiers for the fileids that make up\n",
      "     |      this corpus.\n",
      "     |  \n",
      "     |  license(self)\n",
      "     |      Return the contents of the corpus LICENSE file, if it exists.\n",
      "     |  \n",
      "     |  open(self, file)\n",
      "     |      Return an open stream that can be used to read the given file.\n",
      "     |      If the file's encoding is not None, then the stream will\n",
      "     |      automatically decode the file's contents into unicode.\n",
      "     |      \n",
      "     |      :param file: The file identifier of the file to read.\n",
      "     |  \n",
      "     |  readme(self)\n",
      "     |      Return the contents of the corpus README file, if it exists.\n",
      "     |  \n",
      "     |  unicode_repr = __repr__(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  root\n",
      "     |      The directory where this corpus is stored.\n",
      "     |      \n",
      "     |      :type: PathPointer\n",
      "    \n",
      "    class ToolboxCorpusReader(nltk.corpus.reader.api.CorpusReader)\n",
      "     |  ToolboxCorpusReader(root, fileids, encoding='utf8', tagset=None)\n",
      "     |  \n",
      "     |  A base class for \"corpus reader\" classes, each of which can be\n",
      "     |  used to read a specific corpus format.  Each individual corpus\n",
      "     |  reader instance is used to read a specific corpus, consisting of\n",
      "     |  one or more files under a common root directory.  Each file is\n",
      "     |  identified by its ``file identifier``, which is the relative path\n",
      "     |  to the file from the root directory.\n",
      "     |  \n",
      "     |  A separate subclass is defined for each corpus format.  These\n",
      "     |  subclasses define one or more methods that provide 'views' on the\n",
      "     |  corpus contents, such as ``words()`` (for a list of words) and\n",
      "     |  ``parsed_sents()`` (for a list of parsed sentences).  Called with\n",
      "     |  no arguments, these methods will return the contents of the entire\n",
      "     |  corpus.  For most corpora, these methods define one or more\n",
      "     |  selection arguments, such as ``fileids`` or ``categories``, which can\n",
      "     |  be used to select which portion of the corpus should be returned.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      ToolboxCorpusReader\n",
      "     |      nltk.corpus.reader.api.CorpusReader\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  entries(self, fileids, **kwargs)\n",
      "     |      # should probably be done lazily:\n",
      "     |  \n",
      "     |  fields(self, fileids, strip=True, unwrap=True, encoding='utf8', errors='strict', unicode_fields=None)\n",
      "     |  \n",
      "     |  raw(self, fileids)\n",
      "     |  \n",
      "     |  words(self, fileids, key='lx')\n",
      "     |  \n",
      "     |  xml(self, fileids, key=None)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __init__(self, root, fileids, encoding='utf8', tagset=None)\n",
      "     |      :type root: PathPointer or str\n",
      "     |      :param root: A path pointer identifying the root directory for\n",
      "     |          this corpus.  If a string is specified, then it will be\n",
      "     |          converted to a ``PathPointer`` automatically.\n",
      "     |      :param fileids: A list of the files that make up this corpus.\n",
      "     |          This list can either be specified explicitly, as a list of\n",
      "     |          strings; or implicitly, as a regular expression over file\n",
      "     |          paths.  The absolute path for each file will be constructed\n",
      "     |          by joining the reader's root to each file name.\n",
      "     |      :param encoding: The default unicode encoding for the files\n",
      "     |          that make up the corpus.  The value of ``encoding`` can be any\n",
      "     |          of the following:\n",
      "     |          - A string: ``encoding`` is the encoding name for all files.\n",
      "     |          - A dictionary: ``encoding[file_id]`` is the encoding\n",
      "     |            name for the file whose identifier is ``file_id``.  If\n",
      "     |            ``file_id`` is not in ``encoding``, then the file\n",
      "     |            contents will be processed using non-unicode byte strings.\n",
      "     |          - A list: ``encoding`` should be a list of ``(regexp, encoding)``\n",
      "     |            tuples.  The encoding for a file whose identifier is ``file_id``\n",
      "     |            will be the ``encoding`` value for the first tuple whose\n",
      "     |            ``regexp`` matches the ``file_id``.  If no tuple's ``regexp``\n",
      "     |            matches the ``file_id``, the file contents will be processed\n",
      "     |            using non-unicode byte strings.\n",
      "     |          - None: the file contents of all files will be\n",
      "     |            processed using non-unicode byte strings.\n",
      "     |      :param tagset: The name of the tagset used by this corpus, to be used\n",
      "     |            for normalizing or converting the POS tags returned by the\n",
      "     |            tagged_...() methods.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __unicode__ = __str__(self, /)\n",
      "     |  \n",
      "     |  abspath(self, fileid)\n",
      "     |      Return the absolute path for the given file.\n",
      "     |      \n",
      "     |      :type fileid: str\n",
      "     |      :param fileid: The file identifier for the file whose path\n",
      "     |          should be returned.\n",
      "     |      :rtype: PathPointer\n",
      "     |  \n",
      "     |  abspaths(self, fileids=None, include_encoding=False, include_fileid=False)\n",
      "     |      Return a list of the absolute paths for all fileids in this corpus;\n",
      "     |      or for the given list of fileids, if specified.\n",
      "     |      \n",
      "     |      :type fileids: None or str or list\n",
      "     |      :param fileids: Specifies the set of fileids for which paths should\n",
      "     |          be returned.  Can be None, for all fileids; a list of\n",
      "     |          file identifiers, for a specified set of fileids; or a single\n",
      "     |          file identifier, for a single file.  Note that the return\n",
      "     |          value is always a list of paths, even if ``fileids`` is a\n",
      "     |          single file identifier.\n",
      "     |      \n",
      "     |      :param include_encoding: If true, then return a list of\n",
      "     |          ``(path_pointer, encoding)`` tuples.\n",
      "     |      \n",
      "     |      :rtype: list(PathPointer)\n",
      "     |  \n",
      "     |  citation(self)\n",
      "     |      Return the contents of the corpus citation.bib file, if it exists.\n",
      "     |  \n",
      "     |  encoding(self, file)\n",
      "     |      Return the unicode encoding for the given corpus file, if known.\n",
      "     |      If the encoding is unknown, or if the given file should be\n",
      "     |      processed using byte strings (str), then return None.\n",
      "     |  \n",
      "     |  ensure_loaded(self)\n",
      "     |      Load this corpus (if it has not already been loaded).  This is\n",
      "     |      used by LazyCorpusLoader as a simple method that can be used to\n",
      "     |      make sure a corpus is loaded -- e.g., in case a user wants to\n",
      "     |      do help(some_corpus).\n",
      "     |  \n",
      "     |  fileids(self)\n",
      "     |      Return a list of file identifiers for the fileids that make up\n",
      "     |      this corpus.\n",
      "     |  \n",
      "     |  license(self)\n",
      "     |      Return the contents of the corpus LICENSE file, if it exists.\n",
      "     |  \n",
      "     |  open(self, file)\n",
      "     |      Return an open stream that can be used to read the given file.\n",
      "     |      If the file's encoding is not None, then the stream will\n",
      "     |      automatically decode the file's contents into unicode.\n",
      "     |      \n",
      "     |      :param file: The file identifier of the file to read.\n",
      "     |  \n",
      "     |  readme(self)\n",
      "     |      Return the contents of the corpus README file, if it exists.\n",
      "     |  \n",
      "     |  unicode_repr = __repr__(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  root\n",
      "     |      The directory where this corpus is stored.\n",
      "     |      \n",
      "     |      :type: PathPointer\n",
      "    \n",
      "    class TwitterCorpusReader(nltk.corpus.reader.api.CorpusReader)\n",
      "     |  TwitterCorpusReader(root, fileids=None, word_tokenizer=<nltk.tokenize.casual.TweetTokenizer object at 0x125a52950>, encoding='utf8')\n",
      "     |  \n",
      "     |  Reader for corpora that consist of Tweets represented as a list of line-delimited JSON.\n",
      "     |  \n",
      "     |  Individual Tweets can be tokenized using the default tokenizer, or by a\n",
      "     |  custom tokenizer specified as a parameter to the constructor.\n",
      "     |  \n",
      "     |  Construct a new Tweet corpus reader for a set of documents\n",
      "     |  located at the given root directory.\n",
      "     |  \n",
      "     |  If you made your own tweet collection in a directory called\n",
      "     |  `twitter-files`, then you can initialise the reader as::\n",
      "     |  \n",
      "     |      from nltk.corpus import TwitterCorpusReader\n",
      "     |      reader = TwitterCorpusReader(root='/path/to/twitter-files', '.*\\.json')\n",
      "     |  \n",
      "     |  However, the recommended approach is to set the relevant directory as the\n",
      "     |  value of the environmental variable `TWITTER`, and then invoke the reader\n",
      "     |  as follows::\n",
      "     |  \n",
      "     |     root = os.environ['TWITTER']\n",
      "     |     reader = TwitterCorpusReader(root, '.*\\.json')\n",
      "     |  \n",
      "     |  If you want to work directly with the raw Tweets, the `json` library can\n",
      "     |  be used::\n",
      "     |  \n",
      "     |     import json\n",
      "     |     for tweet in reader.docs():\n",
      "     |         print(json.dumps(tweet, indent=1, sort_keys=True))\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      TwitterCorpusReader\n",
      "     |      nltk.corpus.reader.api.CorpusReader\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, root, fileids=None, word_tokenizer=<nltk.tokenize.casual.TweetTokenizer object at 0x125a52950>, encoding='utf8')\n",
      "     |      :param root: The root directory for this corpus.\n",
      "     |      \n",
      "     |      :param fileids: A list or regexp specifying the fileids in this corpus.\n",
      "     |      \n",
      "     |      :param word_tokenizer: Tokenizer for breaking the text of Tweets into\n",
      "     |      smaller units, including but not limited to words.\n",
      "     |  \n",
      "     |  docs(self, fileids=None)\n",
      "     |      Returns the full Tweet objects, as specified by `Twitter\n",
      "     |      documentation on Tweets\n",
      "     |      <https://dev.twitter.com/docs/platform-objects/tweets>`_\n",
      "     |      \n",
      "     |      :return: the given file(s) as a list of dictionaries deserialised\n",
      "     |      from JSON.\n",
      "     |      :rtype: list(dict)\n",
      "     |  \n",
      "     |  raw(self, fileids=None)\n",
      "     |      Return the corpora in their raw form.\n",
      "     |  \n",
      "     |  strings(self, fileids=None)\n",
      "     |      Returns only the text content of Tweets in the file(s)\n",
      "     |      \n",
      "     |      :return: the given file(s) as a list of Tweets.\n",
      "     |      :rtype: list(str)\n",
      "     |  \n",
      "     |  tokenized(self, fileids=None)\n",
      "     |      :return: the given file(s) as a list of the text content of Tweets as\n",
      "     |      as a list of words, screenanames, hashtags, URLs and punctuation symbols.\n",
      "     |      \n",
      "     |      :rtype: list(list(str))\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  CorpusView = <class 'nltk.corpus.reader.util.StreamBackedCorpusView'>\n",
      "     |      A 'view' of a corpus file, which acts like a sequence of tokens:\n",
      "     |      it can be accessed by index, iterated over, etc.  However, the\n",
      "     |      tokens are only constructed as-needed -- the entire corpus is\n",
      "     |      never stored in memory at once.\n",
      "     |      \n",
      "     |      The constructor to ``StreamBackedCorpusView`` takes two arguments:\n",
      "     |      a corpus fileid (specified as a string or as a ``PathPointer``);\n",
      "     |      and a block reader.  A \"block reader\" is a function that reads\n",
      "     |      zero or more tokens from a stream, and returns them as a list.  A\n",
      "     |      very simple example of a block reader is:\n",
      "     |      \n",
      "     |          >>> def simple_block_reader(stream):\n",
      "     |          ...     return stream.readline().split()\n",
      "     |      \n",
      "     |      This simple block reader reads a single line at a time, and\n",
      "     |      returns a single token (consisting of a string) for each\n",
      "     |      whitespace-separated substring on the line.\n",
      "     |      \n",
      "     |      When deciding how to define the block reader for a given\n",
      "     |      corpus, careful consideration should be given to the size of\n",
      "     |      blocks handled by the block reader.  Smaller block sizes will\n",
      "     |      increase the memory requirements of the corpus view's internal\n",
      "     |      data structures (by 2 integers per block).  On the other hand,\n",
      "     |      larger block sizes may decrease performance for random access to\n",
      "     |      the corpus.  (But note that larger block sizes will *not*\n",
      "     |      decrease performance for iteration.)\n",
      "     |      \n",
      "     |      Internally, ``CorpusView`` maintains a partial mapping from token\n",
      "     |      index to file position, with one entry per block.  When a token\n",
      "     |      with a given index *i* is requested, the ``CorpusView`` constructs\n",
      "     |      it as follows:\n",
      "     |      \n",
      "     |        1. First, it searches the toknum/filepos mapping for the token\n",
      "     |           index closest to (but less than or equal to) *i*.\n",
      "     |      \n",
      "     |        2. Then, starting at the file position corresponding to that\n",
      "     |           index, it reads one block at a time using the block reader\n",
      "     |           until it reaches the requested token.\n",
      "     |      \n",
      "     |      The toknum/filepos mapping is created lazily: it is initially\n",
      "     |      empty, but every time a new block is read, the block's\n",
      "     |      initial token is added to the mapping.  (Thus, the toknum/filepos\n",
      "     |      map has one entry per block.)\n",
      "     |      \n",
      "     |      In order to increase efficiency for random access patterns that\n",
      "     |      have high degrees of locality, the corpus view may cache one or\n",
      "     |      more blocks.\n",
      "     |      \n",
      "     |      :note: Each ``CorpusView`` object internally maintains an open file\n",
      "     |          object for its underlying corpus file.  This file should be\n",
      "     |          automatically closed when the ``CorpusView`` is garbage collected,\n",
      "     |          but if you wish to close it manually, use the ``close()``\n",
      "     |          method.  If you access a ``CorpusView``'s items after it has been\n",
      "     |          closed, the file object will be automatically re-opened.\n",
      "     |      \n",
      "     |      :warning: If the contents of the file are modified during the\n",
      "     |          lifetime of the ``CorpusView``, then the ``CorpusView``'s behavior\n",
      "     |          is undefined.\n",
      "     |      \n",
      "     |      :warning: If a unicode encoding is specified when constructing a\n",
      "     |          ``CorpusView``, then the block reader may only call\n",
      "     |          ``stream.seek()`` with offsets that have been returned by\n",
      "     |          ``stream.tell()``; in particular, calling ``stream.seek()`` with\n",
      "     |          relative offsets, or with offsets based on string lengths, may\n",
      "     |          lead to incorrect behavior.\n",
      "     |      \n",
      "     |      :ivar _block_reader: The function used to read\n",
      "     |          a single block from the underlying file stream.\n",
      "     |      :ivar _toknum: A list containing the token index of each block\n",
      "     |          that has been processed.  In particular, ``_toknum[i]`` is the\n",
      "     |          token index of the first token in block ``i``.  Together\n",
      "     |          with ``_filepos``, this forms a partial mapping between token\n",
      "     |          indices and file positions.\n",
      "     |      :ivar _filepos: A list containing the file position of each block\n",
      "     |          that has been processed.  In particular, ``_toknum[i]`` is the\n",
      "     |          file position of the first character in block ``i``.  Together\n",
      "     |          with ``_toknum``, this forms a partial mapping between token\n",
      "     |          indices and file positions.\n",
      "     |      :ivar _stream: The stream used to access the underlying corpus file.\n",
      "     |      :ivar _len: The total number of tokens in the corpus, if known;\n",
      "     |          or None, if the number of tokens is not yet known.\n",
      "     |      :ivar _eofpos: The character position of the last character in the\n",
      "     |          file.  This is calculated when the corpus view is initialized,\n",
      "     |          and is used to decide when the end of file has been reached.\n",
      "     |      :ivar _cache: A cache of the most recently read block.  It\n",
      "     |         is encoded as a tuple (start_toknum, end_toknum, tokens), where\n",
      "     |         start_toknum is the token index of the first token in the block;\n",
      "     |         end_toknum is the token index of the first token not in the\n",
      "     |         block; and tokens is a list of the tokens in the block.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __unicode__ = __str__(self, /)\n",
      "     |  \n",
      "     |  abspath(self, fileid)\n",
      "     |      Return the absolute path for the given file.\n",
      "     |      \n",
      "     |      :type fileid: str\n",
      "     |      :param fileid: The file identifier for the file whose path\n",
      "     |          should be returned.\n",
      "     |      :rtype: PathPointer\n",
      "     |  \n",
      "     |  abspaths(self, fileids=None, include_encoding=False, include_fileid=False)\n",
      "     |      Return a list of the absolute paths for all fileids in this corpus;\n",
      "     |      or for the given list of fileids, if specified.\n",
      "     |      \n",
      "     |      :type fileids: None or str or list\n",
      "     |      :param fileids: Specifies the set of fileids for which paths should\n",
      "     |          be returned.  Can be None, for all fileids; a list of\n",
      "     |          file identifiers, for a specified set of fileids; or a single\n",
      "     |          file identifier, for a single file.  Note that the return\n",
      "     |          value is always a list of paths, even if ``fileids`` is a\n",
      "     |          single file identifier.\n",
      "     |      \n",
      "     |      :param include_encoding: If true, then return a list of\n",
      "     |          ``(path_pointer, encoding)`` tuples.\n",
      "     |      \n",
      "     |      :rtype: list(PathPointer)\n",
      "     |  \n",
      "     |  citation(self)\n",
      "     |      Return the contents of the corpus citation.bib file, if it exists.\n",
      "     |  \n",
      "     |  encoding(self, file)\n",
      "     |      Return the unicode encoding for the given corpus file, if known.\n",
      "     |      If the encoding is unknown, or if the given file should be\n",
      "     |      processed using byte strings (str), then return None.\n",
      "     |  \n",
      "     |  ensure_loaded(self)\n",
      "     |      Load this corpus (if it has not already been loaded).  This is\n",
      "     |      used by LazyCorpusLoader as a simple method that can be used to\n",
      "     |      make sure a corpus is loaded -- e.g., in case a user wants to\n",
      "     |      do help(some_corpus).\n",
      "     |  \n",
      "     |  fileids(self)\n",
      "     |      Return a list of file identifiers for the fileids that make up\n",
      "     |      this corpus.\n",
      "     |  \n",
      "     |  license(self)\n",
      "     |      Return the contents of the corpus LICENSE file, if it exists.\n",
      "     |  \n",
      "     |  open(self, file)\n",
      "     |      Return an open stream that can be used to read the given file.\n",
      "     |      If the file's encoding is not None, then the stream will\n",
      "     |      automatically decode the file's contents into unicode.\n",
      "     |      \n",
      "     |      :param file: The file identifier of the file to read.\n",
      "     |  \n",
      "     |  readme(self)\n",
      "     |      Return the contents of the corpus README file, if it exists.\n",
      "     |  \n",
      "     |  unicode_repr = __repr__(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  root\n",
      "     |      The directory where this corpus is stored.\n",
      "     |      \n",
      "     |      :type: PathPointer\n",
      "    \n",
      "    class UdhrCorpusReader(nltk.corpus.reader.plaintext.PlaintextCorpusReader)\n",
      "     |  UdhrCorpusReader(root='udhr')\n",
      "     |  \n",
      "     |  Reader for corpora that consist of plaintext documents.  Paragraphs\n",
      "     |  are assumed to be split using blank lines.  Sentences and words can\n",
      "     |  be tokenized using the default tokenizers, or by custom tokenizers\n",
      "     |  specificed as parameters to the constructor.\n",
      "     |  \n",
      "     |  This corpus reader can be customized (e.g., to skip preface\n",
      "     |  sections of specific document formats) by creating a subclass and\n",
      "     |  overriding the ``CorpusView`` class variable.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      UdhrCorpusReader\n",
      "     |      nltk.corpus.reader.plaintext.PlaintextCorpusReader\n",
      "     |      nltk.corpus.reader.api.CorpusReader\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, root='udhr')\n",
      "     |      Construct a new plaintext corpus reader for a set of documents\n",
      "     |      located at the given root directory.  Example usage:\n",
      "     |      \n",
      "     |          >>> root = '/usr/local/share/nltk_data/corpora/webtext/'\n",
      "     |          >>> reader = PlaintextCorpusReader(root, '.*\\.txt') # doctest: +SKIP\n",
      "     |      \n",
      "     |      :param root: The root directory for this corpus.\n",
      "     |      :param fileids: A list or regexp specifying the fileids in this corpus.\n",
      "     |      :param word_tokenizer: Tokenizer for breaking sentences or\n",
      "     |          paragraphs into words.\n",
      "     |      :param sent_tokenizer: Tokenizer for breaking paragraphs\n",
      "     |          into words.\n",
      "     |      :param para_block_reader: The block reader used to divide the\n",
      "     |          corpus into paragraph blocks.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  ENCODINGS = [('.*-Latin1$', 'latin-1'), ('.*-Hebrew$', 'hebrew'), ('.*...\n",
      "     |  \n",
      "     |  SKIP = {'Amharic-Afenegus6..60375', 'Armenian-DallakHelv', 'Azeri_Azer...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from nltk.corpus.reader.plaintext.PlaintextCorpusReader:\n",
      "     |  \n",
      "     |  paras(self, fileids=None)\n",
      "     |      :return: the given file(s) as a list of\n",
      "     |          paragraphs, each encoded as a list of sentences, which are\n",
      "     |          in turn encoded as lists of word strings.\n",
      "     |      :rtype: list(list(list(str)))\n",
      "     |  \n",
      "     |  raw(self, fileids=None)\n",
      "     |      :return: the given file(s) as a single string.\n",
      "     |      :rtype: str\n",
      "     |  \n",
      "     |  sents(self, fileids=None)\n",
      "     |      :return: the given file(s) as a list of\n",
      "     |          sentences or utterances, each encoded as a list of word\n",
      "     |          strings.\n",
      "     |      :rtype: list(list(str))\n",
      "     |  \n",
      "     |  words(self, fileids=None)\n",
      "     |      :return: the given file(s) as a list of words\n",
      "     |          and punctuation symbols.\n",
      "     |      :rtype: list(str)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from nltk.corpus.reader.plaintext.PlaintextCorpusReader:\n",
      "     |  \n",
      "     |  CorpusView = <class 'nltk.corpus.reader.util.StreamBackedCorpusView'>\n",
      "     |      A 'view' of a corpus file, which acts like a sequence of tokens:\n",
      "     |      it can be accessed by index, iterated over, etc.  However, the\n",
      "     |      tokens are only constructed as-needed -- the entire corpus is\n",
      "     |      never stored in memory at once.\n",
      "     |      \n",
      "     |      The constructor to ``StreamBackedCorpusView`` takes two arguments:\n",
      "     |      a corpus fileid (specified as a string or as a ``PathPointer``);\n",
      "     |      and a block reader.  A \"block reader\" is a function that reads\n",
      "     |      zero or more tokens from a stream, and returns them as a list.  A\n",
      "     |      very simple example of a block reader is:\n",
      "     |      \n",
      "     |          >>> def simple_block_reader(stream):\n",
      "     |          ...     return stream.readline().split()\n",
      "     |      \n",
      "     |      This simple block reader reads a single line at a time, and\n",
      "     |      returns a single token (consisting of a string) for each\n",
      "     |      whitespace-separated substring on the line.\n",
      "     |      \n",
      "     |      When deciding how to define the block reader for a given\n",
      "     |      corpus, careful consideration should be given to the size of\n",
      "     |      blocks handled by the block reader.  Smaller block sizes will\n",
      "     |      increase the memory requirements of the corpus view's internal\n",
      "     |      data structures (by 2 integers per block).  On the other hand,\n",
      "     |      larger block sizes may decrease performance for random access to\n",
      "     |      the corpus.  (But note that larger block sizes will *not*\n",
      "     |      decrease performance for iteration.)\n",
      "     |      \n",
      "     |      Internally, ``CorpusView`` maintains a partial mapping from token\n",
      "     |      index to file position, with one entry per block.  When a token\n",
      "     |      with a given index *i* is requested, the ``CorpusView`` constructs\n",
      "     |      it as follows:\n",
      "     |      \n",
      "     |        1. First, it searches the toknum/filepos mapping for the token\n",
      "     |           index closest to (but less than or equal to) *i*.\n",
      "     |      \n",
      "     |        2. Then, starting at the file position corresponding to that\n",
      "     |           index, it reads one block at a time using the block reader\n",
      "     |           until it reaches the requested token.\n",
      "     |      \n",
      "     |      The toknum/filepos mapping is created lazily: it is initially\n",
      "     |      empty, but every time a new block is read, the block's\n",
      "     |      initial token is added to the mapping.  (Thus, the toknum/filepos\n",
      "     |      map has one entry per block.)\n",
      "     |      \n",
      "     |      In order to increase efficiency for random access patterns that\n",
      "     |      have high degrees of locality, the corpus view may cache one or\n",
      "     |      more blocks.\n",
      "     |      \n",
      "     |      :note: Each ``CorpusView`` object internally maintains an open file\n",
      "     |          object for its underlying corpus file.  This file should be\n",
      "     |          automatically closed when the ``CorpusView`` is garbage collected,\n",
      "     |          but if you wish to close it manually, use the ``close()``\n",
      "     |          method.  If you access a ``CorpusView``'s items after it has been\n",
      "     |          closed, the file object will be automatically re-opened.\n",
      "     |      \n",
      "     |      :warning: If the contents of the file are modified during the\n",
      "     |          lifetime of the ``CorpusView``, then the ``CorpusView``'s behavior\n",
      "     |          is undefined.\n",
      "     |      \n",
      "     |      :warning: If a unicode encoding is specified when constructing a\n",
      "     |          ``CorpusView``, then the block reader may only call\n",
      "     |          ``stream.seek()`` with offsets that have been returned by\n",
      "     |          ``stream.tell()``; in particular, calling ``stream.seek()`` with\n",
      "     |          relative offsets, or with offsets based on string lengths, may\n",
      "     |          lead to incorrect behavior.\n",
      "     |      \n",
      "     |      :ivar _block_reader: The function used to read\n",
      "     |          a single block from the underlying file stream.\n",
      "     |      :ivar _toknum: A list containing the token index of each block\n",
      "     |          that has been processed.  In particular, ``_toknum[i]`` is the\n",
      "     |          token index of the first token in block ``i``.  Together\n",
      "     |          with ``_filepos``, this forms a partial mapping between token\n",
      "     |          indices and file positions.\n",
      "     |      :ivar _filepos: A list containing the file position of each block\n",
      "     |          that has been processed.  In particular, ``_toknum[i]`` is the\n",
      "     |          file position of the first character in block ``i``.  Together\n",
      "     |          with ``_toknum``, this forms a partial mapping between token\n",
      "     |          indices and file positions.\n",
      "     |      :ivar _stream: The stream used to access the underlying corpus file.\n",
      "     |      :ivar _len: The total number of tokens in the corpus, if known;\n",
      "     |          or None, if the number of tokens is not yet known.\n",
      "     |      :ivar _eofpos: The character position of the last character in the\n",
      "     |          file.  This is calculated when the corpus view is initialized,\n",
      "     |          and is used to decide when the end of file has been reached.\n",
      "     |      :ivar _cache: A cache of the most recently read block.  It\n",
      "     |         is encoded as a tuple (start_toknum, end_toknum, tokens), where\n",
      "     |         start_toknum is the token index of the first token in the block;\n",
      "     |         end_toknum is the token index of the first token not in the\n",
      "     |         block; and tokens is a list of the tokens in the block.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __unicode__ = __str__(self, /)\n",
      "     |  \n",
      "     |  abspath(self, fileid)\n",
      "     |      Return the absolute path for the given file.\n",
      "     |      \n",
      "     |      :type fileid: str\n",
      "     |      :param fileid: The file identifier for the file whose path\n",
      "     |          should be returned.\n",
      "     |      :rtype: PathPointer\n",
      "     |  \n",
      "     |  abspaths(self, fileids=None, include_encoding=False, include_fileid=False)\n",
      "     |      Return a list of the absolute paths for all fileids in this corpus;\n",
      "     |      or for the given list of fileids, if specified.\n",
      "     |      \n",
      "     |      :type fileids: None or str or list\n",
      "     |      :param fileids: Specifies the set of fileids for which paths should\n",
      "     |          be returned.  Can be None, for all fileids; a list of\n",
      "     |          file identifiers, for a specified set of fileids; or a single\n",
      "     |          file identifier, for a single file.  Note that the return\n",
      "     |          value is always a list of paths, even if ``fileids`` is a\n",
      "     |          single file identifier.\n",
      "     |      \n",
      "     |      :param include_encoding: If true, then return a list of\n",
      "     |          ``(path_pointer, encoding)`` tuples.\n",
      "     |      \n",
      "     |      :rtype: list(PathPointer)\n",
      "     |  \n",
      "     |  citation(self)\n",
      "     |      Return the contents of the corpus citation.bib file, if it exists.\n",
      "     |  \n",
      "     |  encoding(self, file)\n",
      "     |      Return the unicode encoding for the given corpus file, if known.\n",
      "     |      If the encoding is unknown, or if the given file should be\n",
      "     |      processed using byte strings (str), then return None.\n",
      "     |  \n",
      "     |  ensure_loaded(self)\n",
      "     |      Load this corpus (if it has not already been loaded).  This is\n",
      "     |      used by LazyCorpusLoader as a simple method that can be used to\n",
      "     |      make sure a corpus is loaded -- e.g., in case a user wants to\n",
      "     |      do help(some_corpus).\n",
      "     |  \n",
      "     |  fileids(self)\n",
      "     |      Return a list of file identifiers for the fileids that make up\n",
      "     |      this corpus.\n",
      "     |  \n",
      "     |  license(self)\n",
      "     |      Return the contents of the corpus LICENSE file, if it exists.\n",
      "     |  \n",
      "     |  open(self, file)\n",
      "     |      Return an open stream that can be used to read the given file.\n",
      "     |      If the file's encoding is not None, then the stream will\n",
      "     |      automatically decode the file's contents into unicode.\n",
      "     |      \n",
      "     |      :param file: The file identifier of the file to read.\n",
      "     |  \n",
      "     |  readme(self)\n",
      "     |      Return the contents of the corpus README file, if it exists.\n",
      "     |  \n",
      "     |  unicode_repr = __repr__(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  root\n",
      "     |      The directory where this corpus is stored.\n",
      "     |      \n",
      "     |      :type: PathPointer\n",
      "    \n",
      "    class UnicharsCorpusReader(WordListCorpusReader)\n",
      "     |  UnicharsCorpusReader(root, fileids, encoding='utf8', tagset=None)\n",
      "     |  \n",
      "     |  This class is used to read lists of characters from the Perl Unicode\n",
      "     |  Properties (see http://perldoc.perl.org/perluniprops.html).\n",
      "     |  The files in the perluniprop.zip are extracted using the Unicode::Tussle\n",
      "     |  module from http://search.cpan.org/~bdfoy/Unicode-Tussle-1.11/lib/Unicode/Tussle.pm\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      UnicharsCorpusReader\n",
      "     |      WordListCorpusReader\n",
      "     |      nltk.corpus.reader.api.CorpusReader\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  chars(self, category=None, fileids=None)\n",
      "     |      This module returns a list of characters from  the Perl Unicode Properties.\n",
      "     |      They are very useful when porting Perl tokenizers to Python.\n",
      "     |      \n",
      "     |      >>> from nltk.corpus import perluniprops as pup\n",
      "     |      >>> pup.chars('Open_Punctuation')[:5] == [u'(', u'[', u'{', u'༺', u'༼']\n",
      "     |      True\n",
      "     |      >>> pup.chars('Currency_Symbol')[:5] == [u'$', u'¢', u'£', u'¤', u'¥']\n",
      "     |      True\n",
      "     |      >>> pup.available_categories\n",
      "     |      ['Close_Punctuation', 'Currency_Symbol', 'IsAlnum', 'IsAlpha', 'IsLower', 'IsN', 'IsSc', 'IsSo', 'IsUpper', 'Line_Separator', 'Number', 'Open_Punctuation', 'Punctuation', 'Separator', 'Symbol']\n",
      "     |      \n",
      "     |      :return: a list of characters given the specific unicode character category\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  available_categories = ['Close_Punctuation', 'Currency_Symbol', 'IsAln...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from WordListCorpusReader:\n",
      "     |  \n",
      "     |  raw(self, fileids=None)\n",
      "     |  \n",
      "     |  words(self, fileids=None, ignore_lines_startswith='\\n')\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __init__(self, root, fileids, encoding='utf8', tagset=None)\n",
      "     |      :type root: PathPointer or str\n",
      "     |      :param root: A path pointer identifying the root directory for\n",
      "     |          this corpus.  If a string is specified, then it will be\n",
      "     |          converted to a ``PathPointer`` automatically.\n",
      "     |      :param fileids: A list of the files that make up this corpus.\n",
      "     |          This list can either be specified explicitly, as a list of\n",
      "     |          strings; or implicitly, as a regular expression over file\n",
      "     |          paths.  The absolute path for each file will be constructed\n",
      "     |          by joining the reader's root to each file name.\n",
      "     |      :param encoding: The default unicode encoding for the files\n",
      "     |          that make up the corpus.  The value of ``encoding`` can be any\n",
      "     |          of the following:\n",
      "     |          - A string: ``encoding`` is the encoding name for all files.\n",
      "     |          - A dictionary: ``encoding[file_id]`` is the encoding\n",
      "     |            name for the file whose identifier is ``file_id``.  If\n",
      "     |            ``file_id`` is not in ``encoding``, then the file\n",
      "     |            contents will be processed using non-unicode byte strings.\n",
      "     |          - A list: ``encoding`` should be a list of ``(regexp, encoding)``\n",
      "     |            tuples.  The encoding for a file whose identifier is ``file_id``\n",
      "     |            will be the ``encoding`` value for the first tuple whose\n",
      "     |            ``regexp`` matches the ``file_id``.  If no tuple's ``regexp``\n",
      "     |            matches the ``file_id``, the file contents will be processed\n",
      "     |            using non-unicode byte strings.\n",
      "     |          - None: the file contents of all files will be\n",
      "     |            processed using non-unicode byte strings.\n",
      "     |      :param tagset: The name of the tagset used by this corpus, to be used\n",
      "     |            for normalizing or converting the POS tags returned by the\n",
      "     |            tagged_...() methods.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __unicode__ = __str__(self, /)\n",
      "     |  \n",
      "     |  abspath(self, fileid)\n",
      "     |      Return the absolute path for the given file.\n",
      "     |      \n",
      "     |      :type fileid: str\n",
      "     |      :param fileid: The file identifier for the file whose path\n",
      "     |          should be returned.\n",
      "     |      :rtype: PathPointer\n",
      "     |  \n",
      "     |  abspaths(self, fileids=None, include_encoding=False, include_fileid=False)\n",
      "     |      Return a list of the absolute paths for all fileids in this corpus;\n",
      "     |      or for the given list of fileids, if specified.\n",
      "     |      \n",
      "     |      :type fileids: None or str or list\n",
      "     |      :param fileids: Specifies the set of fileids for which paths should\n",
      "     |          be returned.  Can be None, for all fileids; a list of\n",
      "     |          file identifiers, for a specified set of fileids; or a single\n",
      "     |          file identifier, for a single file.  Note that the return\n",
      "     |          value is always a list of paths, even if ``fileids`` is a\n",
      "     |          single file identifier.\n",
      "     |      \n",
      "     |      :param include_encoding: If true, then return a list of\n",
      "     |          ``(path_pointer, encoding)`` tuples.\n",
      "     |      \n",
      "     |      :rtype: list(PathPointer)\n",
      "     |  \n",
      "     |  citation(self)\n",
      "     |      Return the contents of the corpus citation.bib file, if it exists.\n",
      "     |  \n",
      "     |  encoding(self, file)\n",
      "     |      Return the unicode encoding for the given corpus file, if known.\n",
      "     |      If the encoding is unknown, or if the given file should be\n",
      "     |      processed using byte strings (str), then return None.\n",
      "     |  \n",
      "     |  ensure_loaded(self)\n",
      "     |      Load this corpus (if it has not already been loaded).  This is\n",
      "     |      used by LazyCorpusLoader as a simple method that can be used to\n",
      "     |      make sure a corpus is loaded -- e.g., in case a user wants to\n",
      "     |      do help(some_corpus).\n",
      "     |  \n",
      "     |  fileids(self)\n",
      "     |      Return a list of file identifiers for the fileids that make up\n",
      "     |      this corpus.\n",
      "     |  \n",
      "     |  license(self)\n",
      "     |      Return the contents of the corpus LICENSE file, if it exists.\n",
      "     |  \n",
      "     |  open(self, file)\n",
      "     |      Return an open stream that can be used to read the given file.\n",
      "     |      If the file's encoding is not None, then the stream will\n",
      "     |      automatically decode the file's contents into unicode.\n",
      "     |      \n",
      "     |      :param file: The file identifier of the file to read.\n",
      "     |  \n",
      "     |  readme(self)\n",
      "     |      Return the contents of the corpus README file, if it exists.\n",
      "     |  \n",
      "     |  unicode_repr = __repr__(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  root\n",
      "     |      The directory where this corpus is stored.\n",
      "     |      \n",
      "     |      :type: PathPointer\n",
      "    \n",
      "    class VerbnetCorpusReader(nltk.corpus.reader.xmldocs.XMLCorpusReader)\n",
      "     |  VerbnetCorpusReader(root, fileids, wrap_etree=False)\n",
      "     |  \n",
      "     |  An NLTK interface to the VerbNet verb lexicon.\n",
      "     |  \n",
      "     |  From the VerbNet site: \"VerbNet (VN) (Kipper-Schuler 2006) is the largest\n",
      "     |  on-line verb lexicon currently available for English. It is a hierarchical\n",
      "     |  domain-independent, broad-coverage verb lexicon with mappings to other\n",
      "     |  lexical resources such as WordNet (Miller, 1990; Fellbaum, 1998), XTAG\n",
      "     |  (XTAG Research Group, 2001), and FrameNet (Baker et al., 1998).\"\n",
      "     |  \n",
      "     |  For details about VerbNet see:\n",
      "     |  https://verbs.colorado.edu/~mpalmer/projects/verbnet.html\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      VerbnetCorpusReader\n",
      "     |      nltk.corpus.reader.xmldocs.XMLCorpusReader\n",
      "     |      nltk.corpus.reader.api.CorpusReader\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, root, fileids, wrap_etree=False)\n",
      "     |      :type root: PathPointer or str\n",
      "     |      :param root: A path pointer identifying the root directory for\n",
      "     |          this corpus.  If a string is specified, then it will be\n",
      "     |          converted to a ``PathPointer`` automatically.\n",
      "     |      :param fileids: A list of the files that make up this corpus.\n",
      "     |          This list can either be specified explicitly, as a list of\n",
      "     |          strings; or implicitly, as a regular expression over file\n",
      "     |          paths.  The absolute path for each file will be constructed\n",
      "     |          by joining the reader's root to each file name.\n",
      "     |      :param encoding: The default unicode encoding for the files\n",
      "     |          that make up the corpus.  The value of ``encoding`` can be any\n",
      "     |          of the following:\n",
      "     |          - A string: ``encoding`` is the encoding name for all files.\n",
      "     |          - A dictionary: ``encoding[file_id]`` is the encoding\n",
      "     |            name for the file whose identifier is ``file_id``.  If\n",
      "     |            ``file_id`` is not in ``encoding``, then the file\n",
      "     |            contents will be processed using non-unicode byte strings.\n",
      "     |          - A list: ``encoding`` should be a list of ``(regexp, encoding)``\n",
      "     |            tuples.  The encoding for a file whose identifier is ``file_id``\n",
      "     |            will be the ``encoding`` value for the first tuple whose\n",
      "     |            ``regexp`` matches the ``file_id``.  If no tuple's ``regexp``\n",
      "     |            matches the ``file_id``, the file contents will be processed\n",
      "     |            using non-unicode byte strings.\n",
      "     |          - None: the file contents of all files will be\n",
      "     |            processed using non-unicode byte strings.\n",
      "     |      :param tagset: The name of the tagset used by this corpus, to be used\n",
      "     |            for normalizing or converting the POS tags returned by the\n",
      "     |            tagged_...() methods.\n",
      "     |  \n",
      "     |  classids(self, lemma=None, wordnetid=None, fileid=None, classid=None)\n",
      "     |      Return a list of the VerbNet class identifiers.  If a file\n",
      "     |      identifier is specified, then return only the VerbNet class\n",
      "     |      identifiers for classes (and subclasses) defined by that file.\n",
      "     |      If a lemma is specified, then return only VerbNet class\n",
      "     |      identifiers for classes that contain that lemma as a member.\n",
      "     |      If a wordnetid is specified, then return only identifiers for\n",
      "     |      classes that contain that wordnetid as a member.  If a classid\n",
      "     |      is specified, then return only identifiers for subclasses of\n",
      "     |      the specified VerbNet class.\n",
      "     |      If nothing is specified, return all classids within VerbNet\n",
      "     |  \n",
      "     |  fileids(self, vnclass_ids=None)\n",
      "     |      Return a list of fileids that make up this corpus.  If\n",
      "     |      ``vnclass_ids`` is specified, then return the fileids that make\n",
      "     |      up the specified VerbNet class(es).\n",
      "     |  \n",
      "     |  frames(self, vnclass)\n",
      "     |      Given a VerbNet class, this method returns VerbNet frames\n",
      "     |      \n",
      "     |      The members returned are:\n",
      "     |      1) Example\n",
      "     |      2) Description\n",
      "     |      3) Syntax\n",
      "     |      4) Semantics\n",
      "     |      \n",
      "     |      :param vnclass: A VerbNet class identifier; or an ElementTree\n",
      "     |          containing the xml contents of a VerbNet class.\n",
      "     |      :return: frames - a list of frame dictionaries\n",
      "     |  \n",
      "     |  lemmas(self, vnclass=None)\n",
      "     |      Return a list of all verb lemmas that appear in any class, or\n",
      "     |      in the ``classid`` if specified.\n",
      "     |  \n",
      "     |  longid(self, shortid)\n",
      "     |      Returns longid of a VerbNet class\n",
      "     |      \n",
      "     |      Given a short VerbNet class identifier (eg '37.10'), map it\n",
      "     |      to a long id (eg 'confess-37.10').  If ``shortid`` is already a\n",
      "     |      long id, then return it as-is\n",
      "     |  \n",
      "     |  pprint(self, vnclass)\n",
      "     |      Returns pretty printed version of a VerbNet class\n",
      "     |      \n",
      "     |      Return a string containing a pretty-printed representation of\n",
      "     |      the given VerbNet class.\n",
      "     |      \n",
      "     |      :param vnclass: A VerbNet class identifier; or an ElementTree\n",
      "     |      containing the xml contents of a VerbNet class.\n",
      "     |  \n",
      "     |  pprint_frames(self, vnclass, indent='')\n",
      "     |      Returns pretty version of all frames in a VerbNet class\n",
      "     |      \n",
      "     |      Return a string containing a pretty-printed representation of\n",
      "     |      the list of frames within the VerbNet class.\n",
      "     |      \n",
      "     |      :param vnclass: A VerbNet class identifier; or an ElementTree\n",
      "     |          containing the xml contents of a VerbNet class.\n",
      "     |  \n",
      "     |  pprint_members(self, vnclass, indent='')\n",
      "     |      Returns pretty printed version of members in a VerbNet class\n",
      "     |      \n",
      "     |      Return a string containing a pretty-printed representation of\n",
      "     |      the given VerbNet class's member verbs.\n",
      "     |      \n",
      "     |      :param vnclass: A VerbNet class identifier; or an ElementTree\n",
      "     |          containing the xml contents of a VerbNet class.\n",
      "     |  \n",
      "     |  pprint_subclasses(self, vnclass, indent='')\n",
      "     |      Returns pretty printed version of subclasses of VerbNet class\n",
      "     |      \n",
      "     |      Return a string containing a pretty-printed representation of\n",
      "     |      the given VerbNet class's subclasses.\n",
      "     |      \n",
      "     |      :param vnclass: A VerbNet class identifier; or an ElementTree\n",
      "     |          containing the xml contents of a VerbNet class.\n",
      "     |  \n",
      "     |  pprint_themroles(self, vnclass, indent='')\n",
      "     |      Returns pretty printed version of thematic roles in a VerbNet class\n",
      "     |      \n",
      "     |      Return a string containing a pretty-printed representation of\n",
      "     |      the given VerbNet class's thematic roles.\n",
      "     |      \n",
      "     |      :param vnclass: A VerbNet class identifier; or an ElementTree\n",
      "     |          containing the xml contents of a VerbNet class.\n",
      "     |  \n",
      "     |  shortid(self, longid)\n",
      "     |      Returns shortid of a VerbNet class\n",
      "     |      \n",
      "     |      Given a long VerbNet class identifier (eg 'confess-37.10'),\n",
      "     |      map it to a short id (eg '37.10').  If ``longid`` is already a\n",
      "     |      short id, then return it as-is.\n",
      "     |  \n",
      "     |  subclasses(self, vnclass)\n",
      "     |      Returns subclass ids, if any exist\n",
      "     |      \n",
      "     |      Given a VerbNet class, this method returns subclass ids (if they exist)\n",
      "     |      in a list of strings.\n",
      "     |      \n",
      "     |      :param vnclass: A VerbNet class identifier; or an ElementTree\n",
      "     |          containing the xml contents of a VerbNet class.\n",
      "     |      :return: list of subclasses\n",
      "     |  \n",
      "     |  themroles(self, vnclass)\n",
      "     |      Returns thematic roles participating in a VerbNet class\n",
      "     |      \n",
      "     |      Members returned as part of roles are-\n",
      "     |      1) Type\n",
      "     |      2) Modifiers\n",
      "     |      \n",
      "     |      :param vnclass: A VerbNet class identifier; or an ElementTree\n",
      "     |          containing the xml contents of a VerbNet class.\n",
      "     |      :return: themroles: A list of thematic roles in the VerbNet class\n",
      "     |  \n",
      "     |  vnclass(self, fileid_or_classid)\n",
      "     |      Returns VerbNet class ElementTree\n",
      "     |      \n",
      "     |      Return an ElementTree containing the xml for the specified\n",
      "     |      VerbNet class.\n",
      "     |      \n",
      "     |      :param fileid_or_classid: An identifier specifying which class\n",
      "     |          should be returned.  Can be a file identifier (such as\n",
      "     |          ``'put-9.1.xml'``), or a VerbNet class identifier (such as\n",
      "     |          ``'put-9.1'``) or a short VerbNet class identifier (such as\n",
      "     |          ``'9.1'``).\n",
      "     |  \n",
      "     |  wordnetids(self, vnclass=None)\n",
      "     |      Return a list of all wordnet identifiers that appear in any\n",
      "     |      class, or in ``classid`` if specified.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from nltk.corpus.reader.xmldocs.XMLCorpusReader:\n",
      "     |  \n",
      "     |  raw(self, fileids=None)\n",
      "     |  \n",
      "     |  words(self, fileid=None)\n",
      "     |      Returns all of the words and punctuation symbols in the specified file\n",
      "     |      that were in text nodes -- ie, tags are ignored. Like the xml() method,\n",
      "     |      fileid can only specify one file.\n",
      "     |      \n",
      "     |      :return: the given file's text nodes as a list of words and punctuation symbols\n",
      "     |      :rtype: list(str)\n",
      "     |  \n",
      "     |  xml(self, fileid=None)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __unicode__ = __str__(self, /)\n",
      "     |  \n",
      "     |  abspath(self, fileid)\n",
      "     |      Return the absolute path for the given file.\n",
      "     |      \n",
      "     |      :type fileid: str\n",
      "     |      :param fileid: The file identifier for the file whose path\n",
      "     |          should be returned.\n",
      "     |      :rtype: PathPointer\n",
      "     |  \n",
      "     |  abspaths(self, fileids=None, include_encoding=False, include_fileid=False)\n",
      "     |      Return a list of the absolute paths for all fileids in this corpus;\n",
      "     |      or for the given list of fileids, if specified.\n",
      "     |      \n",
      "     |      :type fileids: None or str or list\n",
      "     |      :param fileids: Specifies the set of fileids for which paths should\n",
      "     |          be returned.  Can be None, for all fileids; a list of\n",
      "     |          file identifiers, for a specified set of fileids; or a single\n",
      "     |          file identifier, for a single file.  Note that the return\n",
      "     |          value is always a list of paths, even if ``fileids`` is a\n",
      "     |          single file identifier.\n",
      "     |      \n",
      "     |      :param include_encoding: If true, then return a list of\n",
      "     |          ``(path_pointer, encoding)`` tuples.\n",
      "     |      \n",
      "     |      :rtype: list(PathPointer)\n",
      "     |  \n",
      "     |  citation(self)\n",
      "     |      Return the contents of the corpus citation.bib file, if it exists.\n",
      "     |  \n",
      "     |  encoding(self, file)\n",
      "     |      Return the unicode encoding for the given corpus file, if known.\n",
      "     |      If the encoding is unknown, or if the given file should be\n",
      "     |      processed using byte strings (str), then return None.\n",
      "     |  \n",
      "     |  ensure_loaded(self)\n",
      "     |      Load this corpus (if it has not already been loaded).  This is\n",
      "     |      used by LazyCorpusLoader as a simple method that can be used to\n",
      "     |      make sure a corpus is loaded -- e.g., in case a user wants to\n",
      "     |      do help(some_corpus).\n",
      "     |  \n",
      "     |  license(self)\n",
      "     |      Return the contents of the corpus LICENSE file, if it exists.\n",
      "     |  \n",
      "     |  open(self, file)\n",
      "     |      Return an open stream that can be used to read the given file.\n",
      "     |      If the file's encoding is not None, then the stream will\n",
      "     |      automatically decode the file's contents into unicode.\n",
      "     |      \n",
      "     |      :param file: The file identifier of the file to read.\n",
      "     |  \n",
      "     |  readme(self)\n",
      "     |      Return the contents of the corpus README file, if it exists.\n",
      "     |  \n",
      "     |  unicode_repr = __repr__(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  root\n",
      "     |      The directory where this corpus is stored.\n",
      "     |      \n",
      "     |      :type: PathPointer\n",
      "    \n",
      "    class WordListCorpusReader(nltk.corpus.reader.api.CorpusReader)\n",
      "     |  WordListCorpusReader(root, fileids, encoding='utf8', tagset=None)\n",
      "     |  \n",
      "     |  List of words, one per line.  Blank lines are ignored.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      WordListCorpusReader\n",
      "     |      nltk.corpus.reader.api.CorpusReader\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  raw(self, fileids=None)\n",
      "     |  \n",
      "     |  words(self, fileids=None, ignore_lines_startswith='\\n')\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __init__(self, root, fileids, encoding='utf8', tagset=None)\n",
      "     |      :type root: PathPointer or str\n",
      "     |      :param root: A path pointer identifying the root directory for\n",
      "     |          this corpus.  If a string is specified, then it will be\n",
      "     |          converted to a ``PathPointer`` automatically.\n",
      "     |      :param fileids: A list of the files that make up this corpus.\n",
      "     |          This list can either be specified explicitly, as a list of\n",
      "     |          strings; or implicitly, as a regular expression over file\n",
      "     |          paths.  The absolute path for each file will be constructed\n",
      "     |          by joining the reader's root to each file name.\n",
      "     |      :param encoding: The default unicode encoding for the files\n",
      "     |          that make up the corpus.  The value of ``encoding`` can be any\n",
      "     |          of the following:\n",
      "     |          - A string: ``encoding`` is the encoding name for all files.\n",
      "     |          - A dictionary: ``encoding[file_id]`` is the encoding\n",
      "     |            name for the file whose identifier is ``file_id``.  If\n",
      "     |            ``file_id`` is not in ``encoding``, then the file\n",
      "     |            contents will be processed using non-unicode byte strings.\n",
      "     |          - A list: ``encoding`` should be a list of ``(regexp, encoding)``\n",
      "     |            tuples.  The encoding for a file whose identifier is ``file_id``\n",
      "     |            will be the ``encoding`` value for the first tuple whose\n",
      "     |            ``regexp`` matches the ``file_id``.  If no tuple's ``regexp``\n",
      "     |            matches the ``file_id``, the file contents will be processed\n",
      "     |            using non-unicode byte strings.\n",
      "     |          - None: the file contents of all files will be\n",
      "     |            processed using non-unicode byte strings.\n",
      "     |      :param tagset: The name of the tagset used by this corpus, to be used\n",
      "     |            for normalizing or converting the POS tags returned by the\n",
      "     |            tagged_...() methods.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __unicode__ = __str__(self, /)\n",
      "     |  \n",
      "     |  abspath(self, fileid)\n",
      "     |      Return the absolute path for the given file.\n",
      "     |      \n",
      "     |      :type fileid: str\n",
      "     |      :param fileid: The file identifier for the file whose path\n",
      "     |          should be returned.\n",
      "     |      :rtype: PathPointer\n",
      "     |  \n",
      "     |  abspaths(self, fileids=None, include_encoding=False, include_fileid=False)\n",
      "     |      Return a list of the absolute paths for all fileids in this corpus;\n",
      "     |      or for the given list of fileids, if specified.\n",
      "     |      \n",
      "     |      :type fileids: None or str or list\n",
      "     |      :param fileids: Specifies the set of fileids for which paths should\n",
      "     |          be returned.  Can be None, for all fileids; a list of\n",
      "     |          file identifiers, for a specified set of fileids; or a single\n",
      "     |          file identifier, for a single file.  Note that the return\n",
      "     |          value is always a list of paths, even if ``fileids`` is a\n",
      "     |          single file identifier.\n",
      "     |      \n",
      "     |      :param include_encoding: If true, then return a list of\n",
      "     |          ``(path_pointer, encoding)`` tuples.\n",
      "     |      \n",
      "     |      :rtype: list(PathPointer)\n",
      "     |  \n",
      "     |  citation(self)\n",
      "     |      Return the contents of the corpus citation.bib file, if it exists.\n",
      "     |  \n",
      "     |  encoding(self, file)\n",
      "     |      Return the unicode encoding for the given corpus file, if known.\n",
      "     |      If the encoding is unknown, or if the given file should be\n",
      "     |      processed using byte strings (str), then return None.\n",
      "     |  \n",
      "     |  ensure_loaded(self)\n",
      "     |      Load this corpus (if it has not already been loaded).  This is\n",
      "     |      used by LazyCorpusLoader as a simple method that can be used to\n",
      "     |      make sure a corpus is loaded -- e.g., in case a user wants to\n",
      "     |      do help(some_corpus).\n",
      "     |  \n",
      "     |  fileids(self)\n",
      "     |      Return a list of file identifiers for the fileids that make up\n",
      "     |      this corpus.\n",
      "     |  \n",
      "     |  license(self)\n",
      "     |      Return the contents of the corpus LICENSE file, if it exists.\n",
      "     |  \n",
      "     |  open(self, file)\n",
      "     |      Return an open stream that can be used to read the given file.\n",
      "     |      If the file's encoding is not None, then the stream will\n",
      "     |      automatically decode the file's contents into unicode.\n",
      "     |      \n",
      "     |      :param file: The file identifier of the file to read.\n",
      "     |  \n",
      "     |  readme(self)\n",
      "     |      Return the contents of the corpus README file, if it exists.\n",
      "     |  \n",
      "     |  unicode_repr = __repr__(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  root\n",
      "     |      The directory where this corpus is stored.\n",
      "     |      \n",
      "     |      :type: PathPointer\n",
      "    \n",
      "    class WordNetCorpusReader(nltk.corpus.reader.api.CorpusReader)\n",
      "     |  WordNetCorpusReader(root, omw_reader)\n",
      "     |  \n",
      "     |  A corpus reader used to access wordnet or its variants.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      WordNetCorpusReader\n",
      "     |      nltk.corpus.reader.api.CorpusReader\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, root, omw_reader)\n",
      "     |      Construct a new wordnet corpus reader, with the given root\n",
      "     |      directory.\n",
      "     |  \n",
      "     |  all_lemma_names(self, pos=None, lang='eng')\n",
      "     |      Return all lemma names for all synsets for the given\n",
      "     |      part of speech tag and language or languages. If pos is\n",
      "     |      not specified, all synsets for all parts of speech will\n",
      "     |      be used.\n",
      "     |  \n",
      "     |  all_synsets(self, pos=None)\n",
      "     |      Iterate over all synsets with a given part of speech tag.\n",
      "     |      If no pos is specified, all synsets for all parts of speech\n",
      "     |      will be loaded.\n",
      "     |  \n",
      "     |  citation(self, lang='omw')\n",
      "     |      Return the contents of citation.bib file (for omw)\n",
      "     |      use lang=lang to get the citation for an individual language\n",
      "     |  \n",
      "     |  custom_lemmas(self, tab_file, lang)\n",
      "     |      Reads a custom tab file containing mappings of lemmas in the given\n",
      "     |      language to Princeton WordNet 3.0 synset offsets, allowing NLTK's\n",
      "     |      WordNet functions to then be used with that language.\n",
      "     |      \n",
      "     |      See the \"Tab files\" section at http://compling.hss.ntu.edu.sg/omw/ for\n",
      "     |      documentation on the Multilingual WordNet tab file format.\n",
      "     |      \n",
      "     |      :param tab_file: Tab file as a file or file-like object\n",
      "     |      :type  lang str\n",
      "     |      :param lang ISO 639-3 code of the language of the tab file\n",
      "     |  \n",
      "     |  get_version(self)\n",
      "     |  \n",
      "     |  ic(self, corpus, weight_senses_equally=False, smoothing=1.0)\n",
      "     |      Creates an information content lookup dictionary from a corpus.\n",
      "     |      \n",
      "     |      :type corpus: CorpusReader\n",
      "     |      :param corpus: The corpus from which we create an information\n",
      "     |      content dictionary.\n",
      "     |      :type weight_senses_equally: bool\n",
      "     |      :param weight_senses_equally: If this is True, gives all\n",
      "     |      possible senses equal weight rather than dividing by the\n",
      "     |      number of possible senses.  (If a word has 3 synses, each\n",
      "     |      sense gets 0.3333 per appearance when this is False, 1.0 when\n",
      "     |      it is true.)\n",
      "     |      :param smoothing: How much do we smooth synset counts (default is 1.0)\n",
      "     |      :type smoothing: float\n",
      "     |      :return: An information content dictionary\n",
      "     |  \n",
      "     |  jcn_similarity(self, synset1, synset2, ic, verbose=False)\n",
      "     |      Jiang-Conrath Similarity:\n",
      "     |      Return a score denoting how similar two word senses are, based on the\n",
      "     |      Information Content (IC) of the Least Common Subsumer (most specific\n",
      "     |      ancestor node) and that of the two input Synsets. The relationship is\n",
      "     |      given by the equation 1 / (IC(s1) + IC(s2) - 2 * IC(lcs)).\n",
      "     |      \n",
      "     |      :type  other: Synset\n",
      "     |      :param other: The ``Synset`` that this ``Synset`` is being compared to.\n",
      "     |      :type  ic: dict\n",
      "     |      :param ic: an information content object (as returned by\n",
      "     |          ``nltk.corpus.wordnet_ic.ic()``).\n",
      "     |      :return: A float score denoting the similarity of the two ``Synset``\n",
      "     |          objects.\n",
      "     |  \n",
      "     |  langs(self)\n",
      "     |      return a list of languages supported by Multilingual Wordnet\n",
      "     |  \n",
      "     |  lch_similarity(self, synset1, synset2, verbose=False, simulate_root=True)\n",
      "     |      Leacock Chodorow Similarity:\n",
      "     |      Return a score denoting how similar two word senses are, based on the\n",
      "     |      shortest path that connects the senses (as above) and the maximum depth\n",
      "     |      of the taxonomy in which the senses occur. The relationship is given as\n",
      "     |      -log(p/2d) where p is the shortest path length and d is the taxonomy\n",
      "     |      depth.\n",
      "     |      \n",
      "     |      :type  other: Synset\n",
      "     |      :param other: The ``Synset`` that this ``Synset`` is being compared to.\n",
      "     |      :type simulate_root: bool\n",
      "     |      :param simulate_root: The various verb taxonomies do not\n",
      "     |          share a single root which disallows this metric from working for\n",
      "     |          synsets that are not connected. This flag (True by default)\n",
      "     |          creates a fake root that connects all the taxonomies. Set it\n",
      "     |          to false to disable this behavior. For the noun taxonomy,\n",
      "     |          there is usually a default root except for WordNet version 1.6.\n",
      "     |          If you are using wordnet 1.6, a fake root will be added for nouns\n",
      "     |          as well.\n",
      "     |      :return: A score denoting the similarity of the two ``Synset`` objects,\n",
      "     |          normally greater than 0. None is returned if no connecting path\n",
      "     |          could be found. If a ``Synset`` is compared with itself, the\n",
      "     |          maximum score is returned, which varies depending on the taxonomy\n",
      "     |          depth.\n",
      "     |  \n",
      "     |  lemma(self, name, lang='eng')\n",
      "     |      Return lemma object that matches the name\n",
      "     |  \n",
      "     |  lemma_count(self, lemma)\n",
      "     |      Return the frequency count for this Lemma\n",
      "     |  \n",
      "     |  lemma_from_key(self, key)\n",
      "     |  \n",
      "     |  lemmas(self, lemma, pos=None, lang='eng')\n",
      "     |      Return all Lemma objects with a name matching the specified lemma\n",
      "     |      name and part of speech tag. Matches any part of speech tag if none is\n",
      "     |      specified.\n",
      "     |  \n",
      "     |  license(self, lang='eng')\n",
      "     |      Return the contents of LICENSE (for omw)\n",
      "     |      use lang=lang to get the license for an individual language\n",
      "     |  \n",
      "     |  lin_similarity(self, synset1, synset2, ic, verbose=False)\n",
      "     |      Lin Similarity:\n",
      "     |      Return a score denoting how similar two word senses are, based on the\n",
      "     |      Information Content (IC) of the Least Common Subsumer (most specific\n",
      "     |      ancestor node) and that of the two input Synsets. The relationship is\n",
      "     |      given by the equation 2 * IC(lcs) / (IC(s1) + IC(s2)).\n",
      "     |      \n",
      "     |      :type other: Synset\n",
      "     |      :param other: The ``Synset`` that this ``Synset`` is being compared to.\n",
      "     |      :type ic: dict\n",
      "     |      :param ic: an information content object (as returned by\n",
      "     |          ``nltk.corpus.wordnet_ic.ic()``).\n",
      "     |      :return: A float score denoting the similarity of the two ``Synset``\n",
      "     |          objects, in the range 0 to 1.\n",
      "     |  \n",
      "     |  morphy(self, form, pos=None, check_exceptions=True)\n",
      "     |      Find a possible base form for the given form, with the given\n",
      "     |      part of speech, by checking WordNet's list of exceptional\n",
      "     |      forms, and by recursively stripping affixes for this part of\n",
      "     |      speech until a form in WordNet is found.\n",
      "     |      \n",
      "     |      >>> from nltk.corpus import wordnet as wn\n",
      "     |      >>> print(wn.morphy('dogs'))\n",
      "     |      dog\n",
      "     |      >>> print(wn.morphy('churches'))\n",
      "     |      church\n",
      "     |      >>> print(wn.morphy('aardwolves'))\n",
      "     |      aardwolf\n",
      "     |      >>> print(wn.morphy('abaci'))\n",
      "     |      abacus\n",
      "     |      >>> wn.morphy('hardrock', wn.ADV)\n",
      "     |      >>> print(wn.morphy('book', wn.NOUN))\n",
      "     |      book\n",
      "     |      >>> wn.morphy('book', wn.ADJ)\n",
      "     |  \n",
      "     |  of2ss(self, of)\n",
      "     |      take an id and return the synsets\n",
      "     |  \n",
      "     |  path_similarity(self, synset1, synset2, verbose=False, simulate_root=True)\n",
      "     |      Path Distance Similarity:\n",
      "     |      Return a score denoting how similar two word senses are, based on the\n",
      "     |      shortest path that connects the senses in the is-a (hypernym/hypnoym)\n",
      "     |      taxonomy. The score is in the range 0 to 1, except in those cases where\n",
      "     |      a path cannot be found (will only be true for verbs as there are many\n",
      "     |      distinct verb taxonomies), in which case None is returned. A score of\n",
      "     |      1 represents identity i.e. comparing a sense with itself will return 1.\n",
      "     |      \n",
      "     |      :type other: Synset\n",
      "     |      :param other: The ``Synset`` that this ``Synset`` is being compared to.\n",
      "     |      :type simulate_root: bool\n",
      "     |      :param simulate_root: The various verb taxonomies do not\n",
      "     |          share a single root which disallows this metric from working for\n",
      "     |          synsets that are not connected. This flag (True by default)\n",
      "     |          creates a fake root that connects all the taxonomies. Set it\n",
      "     |          to false to disable this behavior. For the noun taxonomy,\n",
      "     |          there is usually a default root except for WordNet version 1.6.\n",
      "     |          If you are using wordnet 1.6, a fake root will be added for nouns\n",
      "     |          as well.\n",
      "     |      :return: A score denoting the similarity of the two ``Synset`` objects,\n",
      "     |          normally between 0 and 1. None is returned if no connecting path\n",
      "     |          could be found. 1 is returned if a ``Synset`` is compared with\n",
      "     |          itself.\n",
      "     |  \n",
      "     |  readme(self, lang='omw')\n",
      "     |      Return the contents of README (for omw)\n",
      "     |      use lang=lang to get the readme for an individual language\n",
      "     |  \n",
      "     |  res_similarity(self, synset1, synset2, ic, verbose=False)\n",
      "     |      Resnik Similarity:\n",
      "     |      Return a score denoting how similar two word senses are, based on the\n",
      "     |      Information Content (IC) of the Least Common Subsumer (most specific\n",
      "     |      ancestor node).\n",
      "     |      \n",
      "     |      :type  other: Synset\n",
      "     |      :param other: The ``Synset`` that this ``Synset`` is being compared to.\n",
      "     |      :type ic: dict\n",
      "     |      :param ic: an information content object (as returned by\n",
      "     |          ``nltk.corpus.wordnet_ic.ic()``).\n",
      "     |      :return: A float score denoting the similarity of the two ``Synset``\n",
      "     |          objects. Synsets whose LCS is the root node of the taxonomy will\n",
      "     |          have a score of 0 (e.g. N['dog'][0] and N['table'][0]).\n",
      "     |  \n",
      "     |  ss2of(self, ss, lang=None)\n",
      "     |      return the ID of the synset\n",
      "     |  \n",
      "     |  synset(self, name)\n",
      "     |      #############################################################\n",
      "     |      # Loading Synsets\n",
      "     |      #############################################################\n",
      "     |  \n",
      "     |  synset_from_pos_and_offset(self, pos, offset)\n",
      "     |  \n",
      "     |  synset_from_sense_key(self, sense_key)\n",
      "     |      Retrieves synset based on a given sense_key. Sense keys can be\n",
      "     |      obtained from lemma.key()\n",
      "     |      \n",
      "     |      From https://wordnet.princeton.edu/documentation/senseidx5wn:\n",
      "     |      A sense_key is represented as:\n",
      "     |          lemma % lex_sense (e.g. 'dog%1:18:01::')\n",
      "     |      where lex_sense is encoded as:\n",
      "     |          ss_type:lex_filenum:lex_id:head_word:head_id\n",
      "     |      \n",
      "     |      lemma:       ASCII text of word/collocation, in lower case\n",
      "     |      ss_type:     synset type for the sense (1 digit int)\n",
      "     |                   The synset type is encoded as follows:\n",
      "     |                   1    NOUN\n",
      "     |                   2    VERB\n",
      "     |                   3    ADJECTIVE\n",
      "     |                   4    ADVERB\n",
      "     |                   5    ADJECTIVE SATELLITE\n",
      "     |      lex_filenum: name of lexicographer file containing the synset for the sense (2 digit int)\n",
      "     |      lex_id:      when paired with lemma, uniquely identifies a sense in the lexicographer file (2 digit int)\n",
      "     |      head_word:   lemma of the first word in satellite's head synset\n",
      "     |                   Only used if sense is in an adjective satellite synset\n",
      "     |      head_id:     uniquely identifies sense in a lexicographer file when paired with head_word\n",
      "     |                   Only used if head_word is present (2 digit int)\n",
      "     |  \n",
      "     |  synsets(self, lemma, pos=None, lang='eng', check_exceptions=True)\n",
      "     |      Load all synsets with a given lemma and part of speech tag.\n",
      "     |      If no pos is specified, all synsets for all parts of speech\n",
      "     |      will be loaded.\n",
      "     |      If lang is specified, all the synsets associated with the lemma name\n",
      "     |      of that language will be returned.\n",
      "     |  \n",
      "     |  words(self, lang='eng')\n",
      "     |      return lemmas of the given language as list of words\n",
      "     |  \n",
      "     |  wup_similarity(self, synset1, synset2, verbose=False, simulate_root=True)\n",
      "     |      Wu-Palmer Similarity:\n",
      "     |      Return a score denoting how similar two word senses are, based on the\n",
      "     |      depth of the two senses in the taxonomy and that of their Least Common\n",
      "     |      Subsumer (most specific ancestor node). Previously, the scores computed\n",
      "     |      by this implementation did _not_ always agree with those given by\n",
      "     |      Pedersen's Perl implementation of WordNet Similarity. However, with\n",
      "     |      the addition of the simulate_root flag (see below), the score for\n",
      "     |      verbs now almost always agree but not always for nouns.\n",
      "     |      \n",
      "     |      The LCS does not necessarily feature in the shortest path connecting\n",
      "     |      the two senses, as it is by definition the common ancestor deepest in\n",
      "     |      the taxonomy, not closest to the two senses. Typically, however, it\n",
      "     |      will so feature. Where multiple candidates for the LCS exist, that\n",
      "     |      whose shortest path to the root node is the longest will be selected.\n",
      "     |      Where the LCS has multiple paths to the root, the longer path is used\n",
      "     |      for the purposes of the calculation.\n",
      "     |      \n",
      "     |      :type  other: Synset\n",
      "     |      :param other: The ``Synset`` that this ``Synset`` is being compared to.\n",
      "     |      :type simulate_root: bool\n",
      "     |      :param simulate_root: The various verb taxonomies do not\n",
      "     |          share a single root which disallows this metric from working for\n",
      "     |          synsets that are not connected. This flag (True by default)\n",
      "     |          creates a fake root that connects all the taxonomies. Set it\n",
      "     |          to false to disable this behavior. For the noun taxonomy,\n",
      "     |          there is usually a default root except for WordNet version 1.6.\n",
      "     |          If you are using wordnet 1.6, a fake root will be added for nouns\n",
      "     |          as well.\n",
      "     |      :return: A float score denoting the similarity of the two ``Synset``\n",
      "     |          objects, normally greater than zero. If no connecting path between\n",
      "     |          the two senses can be found, None is returned.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  ADJ = 'a'\n",
      "     |  \n",
      "     |  ADJ_SAT = 's'\n",
      "     |  \n",
      "     |  ADV = 'r'\n",
      "     |  \n",
      "     |  MORPHOLOGICAL_SUBSTITUTIONS = {'a': [('er', ''), ('est', ''), ('er', '...\n",
      "     |  \n",
      "     |  NOUN = 'n'\n",
      "     |  \n",
      "     |  VERB = 'v'\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __unicode__ = __str__(self, /)\n",
      "     |  \n",
      "     |  abspath(self, fileid)\n",
      "     |      Return the absolute path for the given file.\n",
      "     |      \n",
      "     |      :type fileid: str\n",
      "     |      :param fileid: The file identifier for the file whose path\n",
      "     |          should be returned.\n",
      "     |      :rtype: PathPointer\n",
      "     |  \n",
      "     |  abspaths(self, fileids=None, include_encoding=False, include_fileid=False)\n",
      "     |      Return a list of the absolute paths for all fileids in this corpus;\n",
      "     |      or for the given list of fileids, if specified.\n",
      "     |      \n",
      "     |      :type fileids: None or str or list\n",
      "     |      :param fileids: Specifies the set of fileids for which paths should\n",
      "     |          be returned.  Can be None, for all fileids; a list of\n",
      "     |          file identifiers, for a specified set of fileids; or a single\n",
      "     |          file identifier, for a single file.  Note that the return\n",
      "     |          value is always a list of paths, even if ``fileids`` is a\n",
      "     |          single file identifier.\n",
      "     |      \n",
      "     |      :param include_encoding: If true, then return a list of\n",
      "     |          ``(path_pointer, encoding)`` tuples.\n",
      "     |      \n",
      "     |      :rtype: list(PathPointer)\n",
      "     |  \n",
      "     |  encoding(self, file)\n",
      "     |      Return the unicode encoding for the given corpus file, if known.\n",
      "     |      If the encoding is unknown, or if the given file should be\n",
      "     |      processed using byte strings (str), then return None.\n",
      "     |  \n",
      "     |  ensure_loaded(self)\n",
      "     |      Load this corpus (if it has not already been loaded).  This is\n",
      "     |      used by LazyCorpusLoader as a simple method that can be used to\n",
      "     |      make sure a corpus is loaded -- e.g., in case a user wants to\n",
      "     |      do help(some_corpus).\n",
      "     |  \n",
      "     |  fileids(self)\n",
      "     |      Return a list of file identifiers for the fileids that make up\n",
      "     |      this corpus.\n",
      "     |  \n",
      "     |  open(self, file)\n",
      "     |      Return an open stream that can be used to read the given file.\n",
      "     |      If the file's encoding is not None, then the stream will\n",
      "     |      automatically decode the file's contents into unicode.\n",
      "     |      \n",
      "     |      :param file: The file identifier of the file to read.\n",
      "     |  \n",
      "     |  unicode_repr = __repr__(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  root\n",
      "     |      The directory where this corpus is stored.\n",
      "     |      \n",
      "     |      :type: PathPointer\n",
      "    \n",
      "    class WordNetICCorpusReader(nltk.corpus.reader.api.CorpusReader)\n",
      "     |  WordNetICCorpusReader(root, fileids)\n",
      "     |  \n",
      "     |  A corpus reader for the WordNet information content corpus.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      WordNetICCorpusReader\n",
      "     |      nltk.corpus.reader.api.CorpusReader\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, root, fileids)\n",
      "     |      :type root: PathPointer or str\n",
      "     |      :param root: A path pointer identifying the root directory for\n",
      "     |          this corpus.  If a string is specified, then it will be\n",
      "     |          converted to a ``PathPointer`` automatically.\n",
      "     |      :param fileids: A list of the files that make up this corpus.\n",
      "     |          This list can either be specified explicitly, as a list of\n",
      "     |          strings; or implicitly, as a regular expression over file\n",
      "     |          paths.  The absolute path for each file will be constructed\n",
      "     |          by joining the reader's root to each file name.\n",
      "     |      :param encoding: The default unicode encoding for the files\n",
      "     |          that make up the corpus.  The value of ``encoding`` can be any\n",
      "     |          of the following:\n",
      "     |          - A string: ``encoding`` is the encoding name for all files.\n",
      "     |          - A dictionary: ``encoding[file_id]`` is the encoding\n",
      "     |            name for the file whose identifier is ``file_id``.  If\n",
      "     |            ``file_id`` is not in ``encoding``, then the file\n",
      "     |            contents will be processed using non-unicode byte strings.\n",
      "     |          - A list: ``encoding`` should be a list of ``(regexp, encoding)``\n",
      "     |            tuples.  The encoding for a file whose identifier is ``file_id``\n",
      "     |            will be the ``encoding`` value for the first tuple whose\n",
      "     |            ``regexp`` matches the ``file_id``.  If no tuple's ``regexp``\n",
      "     |            matches the ``file_id``, the file contents will be processed\n",
      "     |            using non-unicode byte strings.\n",
      "     |          - None: the file contents of all files will be\n",
      "     |            processed using non-unicode byte strings.\n",
      "     |      :param tagset: The name of the tagset used by this corpus, to be used\n",
      "     |            for normalizing or converting the POS tags returned by the\n",
      "     |            tagged_...() methods.\n",
      "     |  \n",
      "     |  ic(self, icfile)\n",
      "     |      Load an information content file from the wordnet_ic corpus\n",
      "     |      and return a dictionary.  This dictionary has just two keys,\n",
      "     |      NOUN and VERB, whose values are dictionaries that map from\n",
      "     |      synsets to information content values.\n",
      "     |      \n",
      "     |      :type icfile: str\n",
      "     |      :param icfile: The name of the wordnet_ic file (e.g. \"ic-brown.dat\")\n",
      "     |      :return: An information content dictionary\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __unicode__ = __str__(self, /)\n",
      "     |  \n",
      "     |  abspath(self, fileid)\n",
      "     |      Return the absolute path for the given file.\n",
      "     |      \n",
      "     |      :type fileid: str\n",
      "     |      :param fileid: The file identifier for the file whose path\n",
      "     |          should be returned.\n",
      "     |      :rtype: PathPointer\n",
      "     |  \n",
      "     |  abspaths(self, fileids=None, include_encoding=False, include_fileid=False)\n",
      "     |      Return a list of the absolute paths for all fileids in this corpus;\n",
      "     |      or for the given list of fileids, if specified.\n",
      "     |      \n",
      "     |      :type fileids: None or str or list\n",
      "     |      :param fileids: Specifies the set of fileids for which paths should\n",
      "     |          be returned.  Can be None, for all fileids; a list of\n",
      "     |          file identifiers, for a specified set of fileids; or a single\n",
      "     |          file identifier, for a single file.  Note that the return\n",
      "     |          value is always a list of paths, even if ``fileids`` is a\n",
      "     |          single file identifier.\n",
      "     |      \n",
      "     |      :param include_encoding: If true, then return a list of\n",
      "     |          ``(path_pointer, encoding)`` tuples.\n",
      "     |      \n",
      "     |      :rtype: list(PathPointer)\n",
      "     |  \n",
      "     |  citation(self)\n",
      "     |      Return the contents of the corpus citation.bib file, if it exists.\n",
      "     |  \n",
      "     |  encoding(self, file)\n",
      "     |      Return the unicode encoding for the given corpus file, if known.\n",
      "     |      If the encoding is unknown, or if the given file should be\n",
      "     |      processed using byte strings (str), then return None.\n",
      "     |  \n",
      "     |  ensure_loaded(self)\n",
      "     |      Load this corpus (if it has not already been loaded).  This is\n",
      "     |      used by LazyCorpusLoader as a simple method that can be used to\n",
      "     |      make sure a corpus is loaded -- e.g., in case a user wants to\n",
      "     |      do help(some_corpus).\n",
      "     |  \n",
      "     |  fileids(self)\n",
      "     |      Return a list of file identifiers for the fileids that make up\n",
      "     |      this corpus.\n",
      "     |  \n",
      "     |  license(self)\n",
      "     |      Return the contents of the corpus LICENSE file, if it exists.\n",
      "     |  \n",
      "     |  open(self, file)\n",
      "     |      Return an open stream that can be used to read the given file.\n",
      "     |      If the file's encoding is not None, then the stream will\n",
      "     |      automatically decode the file's contents into unicode.\n",
      "     |      \n",
      "     |      :param file: The file identifier of the file to read.\n",
      "     |  \n",
      "     |  readme(self)\n",
      "     |      Return the contents of the corpus README file, if it exists.\n",
      "     |  \n",
      "     |  unicode_repr = __repr__(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  root\n",
      "     |      The directory where this corpus is stored.\n",
      "     |      \n",
      "     |      :type: PathPointer\n",
      "    \n",
      "    class XMLCorpusReader(nltk.corpus.reader.api.CorpusReader)\n",
      "     |  XMLCorpusReader(root, fileids, wrap_etree=False)\n",
      "     |  \n",
      "     |  Corpus reader for corpora whose documents are xml files.\n",
      "     |  \n",
      "     |  Note that the ``XMLCorpusReader`` constructor does not take an\n",
      "     |  ``encoding`` argument, because the unicode encoding is specified by\n",
      "     |  the XML files themselves.  See the XML specs for more info.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      XMLCorpusReader\n",
      "     |      nltk.corpus.reader.api.CorpusReader\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, root, fileids, wrap_etree=False)\n",
      "     |      :type root: PathPointer or str\n",
      "     |      :param root: A path pointer identifying the root directory for\n",
      "     |          this corpus.  If a string is specified, then it will be\n",
      "     |          converted to a ``PathPointer`` automatically.\n",
      "     |      :param fileids: A list of the files that make up this corpus.\n",
      "     |          This list can either be specified explicitly, as a list of\n",
      "     |          strings; or implicitly, as a regular expression over file\n",
      "     |          paths.  The absolute path for each file will be constructed\n",
      "     |          by joining the reader's root to each file name.\n",
      "     |      :param encoding: The default unicode encoding for the files\n",
      "     |          that make up the corpus.  The value of ``encoding`` can be any\n",
      "     |          of the following:\n",
      "     |          - A string: ``encoding`` is the encoding name for all files.\n",
      "     |          - A dictionary: ``encoding[file_id]`` is the encoding\n",
      "     |            name for the file whose identifier is ``file_id``.  If\n",
      "     |            ``file_id`` is not in ``encoding``, then the file\n",
      "     |            contents will be processed using non-unicode byte strings.\n",
      "     |          - A list: ``encoding`` should be a list of ``(regexp, encoding)``\n",
      "     |            tuples.  The encoding for a file whose identifier is ``file_id``\n",
      "     |            will be the ``encoding`` value for the first tuple whose\n",
      "     |            ``regexp`` matches the ``file_id``.  If no tuple's ``regexp``\n",
      "     |            matches the ``file_id``, the file contents will be processed\n",
      "     |            using non-unicode byte strings.\n",
      "     |          - None: the file contents of all files will be\n",
      "     |            processed using non-unicode byte strings.\n",
      "     |      :param tagset: The name of the tagset used by this corpus, to be used\n",
      "     |            for normalizing or converting the POS tags returned by the\n",
      "     |            tagged_...() methods.\n",
      "     |  \n",
      "     |  raw(self, fileids=None)\n",
      "     |  \n",
      "     |  words(self, fileid=None)\n",
      "     |      Returns all of the words and punctuation symbols in the specified file\n",
      "     |      that were in text nodes -- ie, tags are ignored. Like the xml() method,\n",
      "     |      fileid can only specify one file.\n",
      "     |      \n",
      "     |      :return: the given file's text nodes as a list of words and punctuation symbols\n",
      "     |      :rtype: list(str)\n",
      "     |  \n",
      "     |  xml(self, fileid=None)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __unicode__ = __str__(self, /)\n",
      "     |  \n",
      "     |  abspath(self, fileid)\n",
      "     |      Return the absolute path for the given file.\n",
      "     |      \n",
      "     |      :type fileid: str\n",
      "     |      :param fileid: The file identifier for the file whose path\n",
      "     |          should be returned.\n",
      "     |      :rtype: PathPointer\n",
      "     |  \n",
      "     |  abspaths(self, fileids=None, include_encoding=False, include_fileid=False)\n",
      "     |      Return a list of the absolute paths for all fileids in this corpus;\n",
      "     |      or for the given list of fileids, if specified.\n",
      "     |      \n",
      "     |      :type fileids: None or str or list\n",
      "     |      :param fileids: Specifies the set of fileids for which paths should\n",
      "     |          be returned.  Can be None, for all fileids; a list of\n",
      "     |          file identifiers, for a specified set of fileids; or a single\n",
      "     |          file identifier, for a single file.  Note that the return\n",
      "     |          value is always a list of paths, even if ``fileids`` is a\n",
      "     |          single file identifier.\n",
      "     |      \n",
      "     |      :param include_encoding: If true, then return a list of\n",
      "     |          ``(path_pointer, encoding)`` tuples.\n",
      "     |      \n",
      "     |      :rtype: list(PathPointer)\n",
      "     |  \n",
      "     |  citation(self)\n",
      "     |      Return the contents of the corpus citation.bib file, if it exists.\n",
      "     |  \n",
      "     |  encoding(self, file)\n",
      "     |      Return the unicode encoding for the given corpus file, if known.\n",
      "     |      If the encoding is unknown, or if the given file should be\n",
      "     |      processed using byte strings (str), then return None.\n",
      "     |  \n",
      "     |  ensure_loaded(self)\n",
      "     |      Load this corpus (if it has not already been loaded).  This is\n",
      "     |      used by LazyCorpusLoader as a simple method that can be used to\n",
      "     |      make sure a corpus is loaded -- e.g., in case a user wants to\n",
      "     |      do help(some_corpus).\n",
      "     |  \n",
      "     |  fileids(self)\n",
      "     |      Return a list of file identifiers for the fileids that make up\n",
      "     |      this corpus.\n",
      "     |  \n",
      "     |  license(self)\n",
      "     |      Return the contents of the corpus LICENSE file, if it exists.\n",
      "     |  \n",
      "     |  open(self, file)\n",
      "     |      Return an open stream that can be used to read the given file.\n",
      "     |      If the file's encoding is not None, then the stream will\n",
      "     |      automatically decode the file's contents into unicode.\n",
      "     |      \n",
      "     |      :param file: The file identifier of the file to read.\n",
      "     |  \n",
      "     |  readme(self)\n",
      "     |      Return the contents of the corpus README file, if it exists.\n",
      "     |  \n",
      "     |  unicode_repr = __repr__(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  root\n",
      "     |      The directory where this corpus is stored.\n",
      "     |      \n",
      "     |      :type: PathPointer\n",
      "    \n",
      "    class YCOECorpusReader(nltk.corpus.reader.api.CorpusReader)\n",
      "     |  YCOECorpusReader(root, encoding='utf8')\n",
      "     |  \n",
      "     |  Corpus reader for the York-Toronto-Helsinki Parsed Corpus of Old\n",
      "     |  English Prose (YCOE), a 1.5 million word syntactically-annotated\n",
      "     |  corpus of Old English prose texts.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      YCOECorpusReader\n",
      "     |      nltk.corpus.reader.api.CorpusReader\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, root, encoding='utf8')\n",
      "     |      :type root: PathPointer or str\n",
      "     |      :param root: A path pointer identifying the root directory for\n",
      "     |          this corpus.  If a string is specified, then it will be\n",
      "     |          converted to a ``PathPointer`` automatically.\n",
      "     |      :param fileids: A list of the files that make up this corpus.\n",
      "     |          This list can either be specified explicitly, as a list of\n",
      "     |          strings; or implicitly, as a regular expression over file\n",
      "     |          paths.  The absolute path for each file will be constructed\n",
      "     |          by joining the reader's root to each file name.\n",
      "     |      :param encoding: The default unicode encoding for the files\n",
      "     |          that make up the corpus.  The value of ``encoding`` can be any\n",
      "     |          of the following:\n",
      "     |          - A string: ``encoding`` is the encoding name for all files.\n",
      "     |          - A dictionary: ``encoding[file_id]`` is the encoding\n",
      "     |            name for the file whose identifier is ``file_id``.  If\n",
      "     |            ``file_id`` is not in ``encoding``, then the file\n",
      "     |            contents will be processed using non-unicode byte strings.\n",
      "     |          - A list: ``encoding`` should be a list of ``(regexp, encoding)``\n",
      "     |            tuples.  The encoding for a file whose identifier is ``file_id``\n",
      "     |            will be the ``encoding`` value for the first tuple whose\n",
      "     |            ``regexp`` matches the ``file_id``.  If no tuple's ``regexp``\n",
      "     |            matches the ``file_id``, the file contents will be processed\n",
      "     |            using non-unicode byte strings.\n",
      "     |          - None: the file contents of all files will be\n",
      "     |            processed using non-unicode byte strings.\n",
      "     |      :param tagset: The name of the tagset used by this corpus, to be used\n",
      "     |            for normalizing or converting the POS tags returned by the\n",
      "     |            tagged_...() methods.\n",
      "     |  \n",
      "     |  documents(self, fileids=None)\n",
      "     |      Return a list of document identifiers for all documents in\n",
      "     |      this corpus, or for the documents with the given file(s) if\n",
      "     |      specified.\n",
      "     |  \n",
      "     |  fileids(self, documents=None)\n",
      "     |      Return a list of file identifiers for the files that make up\n",
      "     |      this corpus, or that store the given document(s) if specified.\n",
      "     |  \n",
      "     |  paras(self, documents=None)\n",
      "     |  \n",
      "     |  parsed_sents(self, documents=None)\n",
      "     |  \n",
      "     |  sents(self, documents=None)\n",
      "     |  \n",
      "     |  tagged_paras(self, documents=None)\n",
      "     |  \n",
      "     |  tagged_sents(self, documents=None)\n",
      "     |  \n",
      "     |  tagged_words(self, documents=None)\n",
      "     |  \n",
      "     |  words(self, documents=None)\n",
      "     |      # Delegate to one of our two sub-readers:\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __unicode__ = __str__(self, /)\n",
      "     |  \n",
      "     |  abspath(self, fileid)\n",
      "     |      Return the absolute path for the given file.\n",
      "     |      \n",
      "     |      :type fileid: str\n",
      "     |      :param fileid: The file identifier for the file whose path\n",
      "     |          should be returned.\n",
      "     |      :rtype: PathPointer\n",
      "     |  \n",
      "     |  abspaths(self, fileids=None, include_encoding=False, include_fileid=False)\n",
      "     |      Return a list of the absolute paths for all fileids in this corpus;\n",
      "     |      or for the given list of fileids, if specified.\n",
      "     |      \n",
      "     |      :type fileids: None or str or list\n",
      "     |      :param fileids: Specifies the set of fileids for which paths should\n",
      "     |          be returned.  Can be None, for all fileids; a list of\n",
      "     |          file identifiers, for a specified set of fileids; or a single\n",
      "     |          file identifier, for a single file.  Note that the return\n",
      "     |          value is always a list of paths, even if ``fileids`` is a\n",
      "     |          single file identifier.\n",
      "     |      \n",
      "     |      :param include_encoding: If true, then return a list of\n",
      "     |          ``(path_pointer, encoding)`` tuples.\n",
      "     |      \n",
      "     |      :rtype: list(PathPointer)\n",
      "     |  \n",
      "     |  citation(self)\n",
      "     |      Return the contents of the corpus citation.bib file, if it exists.\n",
      "     |  \n",
      "     |  encoding(self, file)\n",
      "     |      Return the unicode encoding for the given corpus file, if known.\n",
      "     |      If the encoding is unknown, or if the given file should be\n",
      "     |      processed using byte strings (str), then return None.\n",
      "     |  \n",
      "     |  ensure_loaded(self)\n",
      "     |      Load this corpus (if it has not already been loaded).  This is\n",
      "     |      used by LazyCorpusLoader as a simple method that can be used to\n",
      "     |      make sure a corpus is loaded -- e.g., in case a user wants to\n",
      "     |      do help(some_corpus).\n",
      "     |  \n",
      "     |  license(self)\n",
      "     |      Return the contents of the corpus LICENSE file, if it exists.\n",
      "     |  \n",
      "     |  open(self, file)\n",
      "     |      Return an open stream that can be used to read the given file.\n",
      "     |      If the file's encoding is not None, then the stream will\n",
      "     |      automatically decode the file's contents into unicode.\n",
      "     |      \n",
      "     |      :param file: The file identifier of the file to read.\n",
      "     |  \n",
      "     |  readme(self)\n",
      "     |      Return the contents of the corpus README file, if it exists.\n",
      "     |  \n",
      "     |  unicode_repr = __repr__(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  root\n",
      "     |      The directory where this corpus is stored.\n",
      "     |      \n",
      "     |      :type: PathPointer\n",
      "\n",
      "FUNCTIONS\n",
      "    find_corpus_fileids(root, regexp)\n",
      "    \n",
      "    tagged_treebank_para_block_reader(stream)\n",
      "\n",
      "DATA\n",
      "    __all__ = ['CorpusReader', 'CategorizedCorpusReader', 'PlaintextCorpus...\n",
      "\n",
      "FILE\n",
      "    /Users/gleonardo/Library/Python/3.7/lib/python/site-packages/nltk/corpus/reader/__init__.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(nltk.corpus.reader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "加载自己的语料库\n",
    "\n",
    "如果您想要使用上述方法访问自己的文本文件集合，则可以借助NLTK的PlaintextCorpusReader轻松加载它们。 检查文件在文件系统中的位置； 在以下示例中，我们将其作为目录/ usr / share / dict。 无论位置在哪里，都将其设置为corpus_root [1]的值。 PlaintextCorpusReader初始值设定项[2]的第二个参数可以是文件名列表，例如['a.txt'，'test / b.txt']，也可以是匹配所有文件名的模式，例如'[abc] /.* \\ .txt”（有关正则表达式的信息，请参见3.4）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import PlaintextCorpusReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_root = '/usr/share/dict'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordlist = PlaintextCorpusReader(corpus_root, '.*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wordlists.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wordlists.words()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "再举一个例子，假设您在C：\\ corpora中拥有自己的Penn Treebank本地副本（版本3）。 我们可以使用BracketParseCorpusReader来访问该语料库。 我们将corpus_root指定为已分析的语料库[1]的《华尔街日报》组件的位置，并给出一个与其子文件夹[2]中包含的文件匹配的file_pattern（使用正斜杠）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import BracketParseCorpusReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_root = r\"C:\\corpora\\penntreebank\\parsed\\mrg\\wsj\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_pattern = r\".*/wsj_.*\\.mrg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ptb = BracketParseCorpusReader(corpus_root, file_pattern)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "条件频率分布\n",
    "\n",
    "我们在3中介绍了频率分布。我们看到，给定单词或其他项目的列表mylist，FreqDist（mylist）将计算列表中每个项目的出现次数。 在这里，我们将概括这个想法。\n",
    "\n",
    "将语料库的文本按体裁，主题，作者等划分为几个类别时，我们可以为每个类别维护单独的频率分布。 这将使我们能够研究类别之间的系统差异。 在上一节中，我们使用NLTK的ConditionalFreqDist数据类型实现了此目的。 条件频率分布是频率分布的集合，每个频率分布对应一个不同的“条件”。 条件通常是文本的类别。 2.1描绘了一个条件频率分布的片段，其中只有两个条件，一个条件用于新闻文本，一个条件用于浪漫文本。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "频率分布计算可观察的事件，例如文本中单词的出现。 条件频率分布需要将每个事件与条件配对。 因此，我们不必处理单词对[1]的序列，而必须处理对pairs的序列[2]："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在1中，我们看到了条件频率分布，其中条件是布朗语料库的部分，并且针对每个条件我们都对单词进行计数。 FreqDist（）采用简单列表作为输入，而ConditionalFreqDist（）采用成对列表"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "把文件变成 （类别，文字）的pairs==然后用conditionalfreqdist来查询频率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfd = nltk.ConditionalFreqDist(\n",
    "    (genre,word)\n",
    "    for genre in brown.categories()\n",
    "    for word in brown.words(categories = genre))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "genre_word = [(genre, word)\n",
    "             for genre in ['news','romance']\n",
    "             for word in brown.words(categories=genre)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "170576"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(genre_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('news', 'The'),\n",
       " ('news', 'Fulton'),\n",
       " ('news', 'County'),\n",
       " ('news', 'Grand'),\n",
       " ('news', 'Jury'),\n",
       " ('news', 'said'),\n",
       " ('news', 'Friday'),\n",
       " ('news', 'an'),\n",
       " ('news', 'investigation'),\n",
       " ('news', 'of'),\n",
       " ('news', \"Atlanta's\"),\n",
       " ('news', 'recent'),\n",
       " ('news', 'primary'),\n",
       " ('news', 'election'),\n",
       " ('news', 'produced'),\n",
       " ('news', '``'),\n",
       " ('news', 'no'),\n",
       " ('news', 'evidence'),\n",
       " ('news', \"''\"),\n",
       " ('news', 'that'),\n",
       " ('news', 'any'),\n",
       " ('news', 'irregularities'),\n",
       " ('news', 'took'),\n",
       " ('news', 'place'),\n",
       " ('news', '.'),\n",
       " ('news', 'The'),\n",
       " ('news', 'jury'),\n",
       " ('news', 'further'),\n",
       " ('news', 'said'),\n",
       " ('news', 'in'),\n",
       " ('news', 'term-end'),\n",
       " ('news', 'presentments'),\n",
       " ('news', 'that'),\n",
       " ('news', 'the'),\n",
       " ('news', 'City'),\n",
       " ('news', 'Executive'),\n",
       " ('news', 'Committee'),\n",
       " ('news', ','),\n",
       " ('news', 'which'),\n",
       " ('news', 'had'),\n",
       " ('news', 'over-all'),\n",
       " ('news', 'charge'),\n",
       " ('news', 'of'),\n",
       " ('news', 'the'),\n",
       " ('news', 'election'),\n",
       " ('news', ','),\n",
       " ('news', '``'),\n",
       " ('news', 'deserves'),\n",
       " ('news', 'the'),\n",
       " ('news', 'praise'),\n",
       " ('news', 'and'),\n",
       " ('news', 'thanks'),\n",
       " ('news', 'of'),\n",
       " ('news', 'the'),\n",
       " ('news', 'City'),\n",
       " ('news', 'of'),\n",
       " ('news', 'Atlanta'),\n",
       " ('news', \"''\"),\n",
       " ('news', 'for'),\n",
       " ('news', 'the'),\n",
       " ('news', 'manner'),\n",
       " ('news', 'in'),\n",
       " ('news', 'which'),\n",
       " ('news', 'the'),\n",
       " ('news', 'election'),\n",
       " ('news', 'was'),\n",
       " ('news', 'conducted'),\n",
       " ('news', '.'),\n",
       " ('news', 'The'),\n",
       " ('news', 'September-October'),\n",
       " ('news', 'term'),\n",
       " ('news', 'jury'),\n",
       " ('news', 'had'),\n",
       " ('news', 'been'),\n",
       " ('news', 'charged'),\n",
       " ('news', 'by'),\n",
       " ('news', 'Fulton'),\n",
       " ('news', 'Superior'),\n",
       " ('news', 'Court'),\n",
       " ('news', 'Judge'),\n",
       " ('news', 'Durwood'),\n",
       " ('news', 'Pye'),\n",
       " ('news', 'to'),\n",
       " ('news', 'investigate'),\n",
       " ('news', 'reports'),\n",
       " ('news', 'of'),\n",
       " ('news', 'possible'),\n",
       " ('news', '``'),\n",
       " ('news', 'irregularities'),\n",
       " ('news', \"''\"),\n",
       " ('news', 'in'),\n",
       " ('news', 'the'),\n",
       " ('news', 'hard-fought'),\n",
       " ('news', 'primary'),\n",
       " ('news', 'which'),\n",
       " ('news', 'was'),\n",
       " ('news', 'won'),\n",
       " ('news', 'by'),\n",
       " ('news', 'Mayor-nominate'),\n",
       " ('news', 'Ivan'),\n",
       " ('news', 'Allen'),\n",
       " ('news', 'Jr.'),\n",
       " ('news', '.'),\n",
       " ('news', '``'),\n",
       " ('news', 'Only'),\n",
       " ('news', 'a'),\n",
       " ('news', 'relative'),\n",
       " ('news', 'handful'),\n",
       " ('news', 'of'),\n",
       " ('news', 'such'),\n",
       " ('news', 'reports'),\n",
       " ('news', 'was'),\n",
       " ('news', 'received'),\n",
       " ('news', \"''\"),\n",
       " ('news', ','),\n",
       " ('news', 'the'),\n",
       " ('news', 'jury'),\n",
       " ('news', 'said'),\n",
       " ('news', ','),\n",
       " ('news', '``'),\n",
       " ('news', 'considering'),\n",
       " ('news', 'the'),\n",
       " ('news', 'widespread'),\n",
       " ('news', 'interest'),\n",
       " ('news', 'in'),\n",
       " ('news', 'the'),\n",
       " ('news', 'election'),\n",
       " ('news', ','),\n",
       " ('news', 'the'),\n",
       " ('news', 'number'),\n",
       " ('news', 'of'),\n",
       " ('news', 'voters'),\n",
       " ('news', 'and'),\n",
       " ('news', 'the'),\n",
       " ('news', 'size'),\n",
       " ('news', 'of'),\n",
       " ('news', 'this'),\n",
       " ('news', 'city'),\n",
       " ('news', \"''\"),\n",
       " ('news', '.'),\n",
       " ('news', 'The'),\n",
       " ('news', 'jury'),\n",
       " ('news', 'said'),\n",
       " ('news', 'it'),\n",
       " ('news', 'did'),\n",
       " ('news', 'find'),\n",
       " ('news', 'that'),\n",
       " ('news', 'many'),\n",
       " ('news', 'of'),\n",
       " ('news', \"Georgia's\"),\n",
       " ('news', 'registration'),\n",
       " ('news', 'and'),\n",
       " ('news', 'election'),\n",
       " ('news', 'laws'),\n",
       " ('news', '``'),\n",
       " ('news', 'are'),\n",
       " ('news', 'outmoded'),\n",
       " ('news', 'or'),\n",
       " ('news', 'inadequate'),\n",
       " ('news', 'and'),\n",
       " ('news', 'often'),\n",
       " ('news', 'ambiguous'),\n",
       " ('news', \"''\"),\n",
       " ('news', '.'),\n",
       " ('news', 'It'),\n",
       " ('news', 'recommended'),\n",
       " ('news', 'that'),\n",
       " ('news', 'Fulton'),\n",
       " ('news', 'legislators'),\n",
       " ('news', 'act'),\n",
       " ('news', '``'),\n",
       " ('news', 'to'),\n",
       " ('news', 'have'),\n",
       " ('news', 'these'),\n",
       " ('news', 'laws'),\n",
       " ('news', 'studied'),\n",
       " ('news', 'and'),\n",
       " ('news', 'revised'),\n",
       " ('news', 'to'),\n",
       " ('news', 'the'),\n",
       " ('news', 'end'),\n",
       " ('news', 'of'),\n",
       " ('news', 'modernizing'),\n",
       " ('news', 'and'),\n",
       " ('news', 'improving'),\n",
       " ('news', 'them'),\n",
       " ('news', \"''\"),\n",
       " ('news', '.'),\n",
       " ('news', 'The'),\n",
       " ('news', 'grand'),\n",
       " ('news', 'jury'),\n",
       " ('news', 'commented'),\n",
       " ('news', 'on'),\n",
       " ('news', 'a'),\n",
       " ('news', 'number'),\n",
       " ('news', 'of'),\n",
       " ('news', 'other'),\n",
       " ('news', 'topics'),\n",
       " ('news', ','),\n",
       " ('news', 'among'),\n",
       " ('news', 'them'),\n",
       " ('news', 'the'),\n",
       " ('news', 'Atlanta'),\n",
       " ('news', 'and'),\n",
       " ('news', 'Fulton'),\n",
       " ('news', 'County'),\n",
       " ('news', 'purchasing'),\n",
       " ('news', 'departments'),\n",
       " ('news', 'which'),\n",
       " ('news', 'it'),\n",
       " ('news', 'said'),\n",
       " ('news', '``'),\n",
       " ('news', 'are'),\n",
       " ('news', 'well'),\n",
       " ('news', 'operated'),\n",
       " ('news', 'and'),\n",
       " ('news', 'follow'),\n",
       " ('news', 'generally'),\n",
       " ('news', 'accepted'),\n",
       " ('news', 'practices'),\n",
       " ('news', 'which'),\n",
       " ('news', 'inure'),\n",
       " ('news', 'to'),\n",
       " ('news', 'the'),\n",
       " ('news', 'best'),\n",
       " ('news', 'interest'),\n",
       " ('news', 'of'),\n",
       " ('news', 'both'),\n",
       " ('news', 'governments'),\n",
       " ('news', \"''\"),\n",
       " ('news', '.'),\n",
       " ('news', 'Merger'),\n",
       " ('news', 'proposed'),\n",
       " ('news', 'However'),\n",
       " ('news', ','),\n",
       " ('news', 'the'),\n",
       " ('news', 'jury'),\n",
       " ('news', 'said'),\n",
       " ('news', 'it'),\n",
       " ('news', 'believes'),\n",
       " ('news', '``'),\n",
       " ('news', 'these'),\n",
       " ('news', 'two'),\n",
       " ('news', 'offices'),\n",
       " ('news', 'should'),\n",
       " ('news', 'be'),\n",
       " ('news', 'combined'),\n",
       " ('news', 'to'),\n",
       " ('news', 'achieve'),\n",
       " ('news', 'greater'),\n",
       " ('news', 'efficiency'),\n",
       " ('news', 'and'),\n",
       " ('news', 'reduce'),\n",
       " ('news', 'the'),\n",
       " ('news', 'cost'),\n",
       " ('news', 'of'),\n",
       " ('news', 'administration'),\n",
       " ('news', \"''\"),\n",
       " ('news', '.'),\n",
       " ('news', 'The'),\n",
       " ('news', 'City'),\n",
       " ('news', 'Purchasing'),\n",
       " ('news', 'Department'),\n",
       " ('news', ','),\n",
       " ('news', 'the'),\n",
       " ('news', 'jury'),\n",
       " ('news', 'said'),\n",
       " ('news', ','),\n",
       " ('news', '``'),\n",
       " ('news', 'is'),\n",
       " ('news', 'lacking'),\n",
       " ('news', 'in'),\n",
       " ('news', 'experienced'),\n",
       " ('news', 'clerical'),\n",
       " ('news', 'personnel'),\n",
       " ('news', 'as'),\n",
       " ('news', 'a'),\n",
       " ('news', 'result'),\n",
       " ('news', 'of'),\n",
       " ('news', 'city'),\n",
       " ('news', 'personnel'),\n",
       " ('news', 'policies'),\n",
       " ('news', \"''\"),\n",
       " ('news', '.'),\n",
       " ('news', 'It'),\n",
       " ('news', 'urged'),\n",
       " ('news', 'that'),\n",
       " ('news', 'the'),\n",
       " ('news', 'city'),\n",
       " ('news', '``'),\n",
       " ('news', 'take'),\n",
       " ('news', 'steps'),\n",
       " ('news', 'to'),\n",
       " ('news', 'remedy'),\n",
       " ('news', \"''\"),\n",
       " ('news', 'this'),\n",
       " ('news', 'problem'),\n",
       " ('news', '.'),\n",
       " ('news', 'Implementation'),\n",
       " ('news', 'of'),\n",
       " ('news', \"Georgia's\"),\n",
       " ('news', 'automobile'),\n",
       " ('news', 'title'),\n",
       " ('news', 'law'),\n",
       " ('news', 'was'),\n",
       " ('news', 'also'),\n",
       " ('news', 'recommended'),\n",
       " ('news', 'by'),\n",
       " ('news', 'the'),\n",
       " ('news', 'outgoing'),\n",
       " ('news', 'jury'),\n",
       " ('news', '.'),\n",
       " ('news', 'It'),\n",
       " ('news', 'urged'),\n",
       " ('news', 'that'),\n",
       " ('news', 'the'),\n",
       " ('news', 'next'),\n",
       " ('news', 'Legislature'),\n",
       " ('news', '``'),\n",
       " ('news', 'provide'),\n",
       " ('news', 'enabling'),\n",
       " ('news', 'funds'),\n",
       " ('news', 'and'),\n",
       " ('news', 're-set'),\n",
       " ('news', 'the'),\n",
       " ('news', 'effective'),\n",
       " ('news', 'date'),\n",
       " ('news', 'so'),\n",
       " ('news', 'that'),\n",
       " ('news', 'an'),\n",
       " ('news', 'orderly'),\n",
       " ('news', 'implementation'),\n",
       " ('news', 'of'),\n",
       " ('news', 'the'),\n",
       " ('news', 'law'),\n",
       " ('news', 'may'),\n",
       " ('news', 'be'),\n",
       " ('news', 'effected'),\n",
       " ('news', \"''\"),\n",
       " ('news', '.'),\n",
       " ('news', 'The'),\n",
       " ('news', 'grand'),\n",
       " ('news', 'jury'),\n",
       " ('news', 'took'),\n",
       " ('news', 'a'),\n",
       " ('news', 'swipe'),\n",
       " ('news', 'at'),\n",
       " ('news', 'the'),\n",
       " ('news', 'State'),\n",
       " ('news', 'Welfare'),\n",
       " ('news', \"Department's\"),\n",
       " ('news', 'handling'),\n",
       " ('news', 'of'),\n",
       " ('news', 'federal'),\n",
       " ('news', 'funds'),\n",
       " ('news', 'granted'),\n",
       " ('news', 'for'),\n",
       " ('news', 'child'),\n",
       " ('news', 'welfare'),\n",
       " ('news', 'services'),\n",
       " ('news', 'in'),\n",
       " ('news', 'foster'),\n",
       " ('news', 'homes'),\n",
       " ('news', '.'),\n",
       " ('news', '``'),\n",
       " ('news', 'This'),\n",
       " ('news', 'is'),\n",
       " ('news', 'one'),\n",
       " ('news', 'of'),\n",
       " ('news', 'the'),\n",
       " ('news', 'major'),\n",
       " ('news', 'items'),\n",
       " ('news', 'in'),\n",
       " ('news', 'the'),\n",
       " ('news', 'Fulton'),\n",
       " ('news', 'County'),\n",
       " ('news', 'general'),\n",
       " ('news', 'assistance'),\n",
       " ('news', 'program'),\n",
       " ('news', \"''\"),\n",
       " ('news', ','),\n",
       " ('news', 'the'),\n",
       " ('news', 'jury'),\n",
       " ('news', 'said'),\n",
       " ('news', ','),\n",
       " ('news', 'but'),\n",
       " ('news', 'the'),\n",
       " ('news', 'State'),\n",
       " ('news', 'Welfare'),\n",
       " ('news', 'Department'),\n",
       " ('news', '``'),\n",
       " ('news', 'has'),\n",
       " ('news', 'seen'),\n",
       " ('news', 'fit'),\n",
       " ('news', 'to'),\n",
       " ('news', 'distribute'),\n",
       " ('news', 'these'),\n",
       " ('news', 'funds'),\n",
       " ('news', 'through'),\n",
       " ('news', 'the'),\n",
       " ('news', 'welfare'),\n",
       " ('news', 'departments'),\n",
       " ('news', 'of'),\n",
       " ('news', 'all'),\n",
       " ('news', 'the'),\n",
       " ('news', 'counties'),\n",
       " ('news', 'in'),\n",
       " ('news', 'the'),\n",
       " ('news', 'state'),\n",
       " ('news', 'with'),\n",
       " ('news', 'the'),\n",
       " ('news', 'exception'),\n",
       " ('news', 'of'),\n",
       " ('news', 'Fulton'),\n",
       " ('news', 'County'),\n",
       " ('news', ','),\n",
       " ('news', 'which'),\n",
       " ('news', 'receives'),\n",
       " ('news', 'none'),\n",
       " ('news', 'of'),\n",
       " ('news', 'this'),\n",
       " ('news', 'money'),\n",
       " ('news', '.'),\n",
       " ('news', 'The'),\n",
       " ('news', 'jurors'),\n",
       " ('news', 'said'),\n",
       " ('news', 'they'),\n",
       " ('news', 'realize'),\n",
       " ('news', '``'),\n",
       " ('news', 'a'),\n",
       " ('news', 'proportionate'),\n",
       " ('news', 'distribution'),\n",
       " ('news', 'of'),\n",
       " ('news', 'these'),\n",
       " ('news', 'funds'),\n",
       " ('news', 'might'),\n",
       " ('news', 'disable'),\n",
       " ('news', 'this'),\n",
       " ('news', 'program'),\n",
       " ('news', 'in'),\n",
       " ('news', 'our'),\n",
       " ('news', 'less'),\n",
       " ('news', 'populous'),\n",
       " ('news', 'counties'),\n",
       " ('news', \"''\"),\n",
       " ('news', '.'),\n",
       " ('news', 'Nevertheless'),\n",
       " ('news', ','),\n",
       " ('news', '``'),\n",
       " ('news', 'we'),\n",
       " ('news', 'feel'),\n",
       " ('news', 'that'),\n",
       " ('news', 'in'),\n",
       " ('news', 'the'),\n",
       " ('news', 'future'),\n",
       " ('news', 'Fulton'),\n",
       " ('news', 'County'),\n",
       " ('news', 'should'),\n",
       " ('news', 'receive'),\n",
       " ('news', 'some'),\n",
       " ('news', 'portion'),\n",
       " ('news', 'of'),\n",
       " ('news', 'these'),\n",
       " ('news', 'available'),\n",
       " ('news', 'funds'),\n",
       " ('news', \"''\"),\n",
       " ('news', ','),\n",
       " ('news', 'the'),\n",
       " ('news', 'jurors'),\n",
       " ('news', 'said'),\n",
       " ('news', '.'),\n",
       " ('news', '``'),\n",
       " ('news', 'Failure'),\n",
       " ('news', 'to'),\n",
       " ('news', 'do'),\n",
       " ('news', 'this'),\n",
       " ('news', 'will'),\n",
       " ('news', 'continue'),\n",
       " ('news', 'to'),\n",
       " ('news', 'place'),\n",
       " ('news', 'a'),\n",
       " ('news', 'disproportionate'),\n",
       " ('news', 'burden'),\n",
       " ('news', \"''\"),\n",
       " ('news', 'on'),\n",
       " ('news', 'Fulton'),\n",
       " ('news', 'taxpayers'),\n",
       " ('news', '.'),\n",
       " ('news', 'The'),\n",
       " ('news', 'jury'),\n",
       " ('news', 'also'),\n",
       " ('news', 'commented'),\n",
       " ('news', 'on'),\n",
       " ('news', 'the'),\n",
       " ('news', 'Fulton'),\n",
       " ('news', \"ordinary's\"),\n",
       " ('news', 'court'),\n",
       " ('news', 'which'),\n",
       " ('news', 'has'),\n",
       " ('news', 'been'),\n",
       " ('news', 'under'),\n",
       " ('news', 'fire'),\n",
       " ('news', 'for'),\n",
       " ('news', 'its'),\n",
       " ('news', 'practices'),\n",
       " ('news', 'in'),\n",
       " ('news', 'the'),\n",
       " ('news', 'appointment'),\n",
       " ('news', 'of'),\n",
       " ('news', 'appraisers'),\n",
       " ('news', ','),\n",
       " ('news', 'guardians'),\n",
       " ('news', 'and'),\n",
       " ('news', 'administrators'),\n",
       " ('news', 'and'),\n",
       " ('news', 'the'),\n",
       " ('news', 'awarding'),\n",
       " ('news', 'of'),\n",
       " ('news', 'fees'),\n",
       " ('news', 'and'),\n",
       " ('news', 'compensation'),\n",
       " ('news', '.'),\n",
       " ('news', 'Wards'),\n",
       " ('news', 'protected'),\n",
       " ('news', 'The'),\n",
       " ('news', 'jury'),\n",
       " ('news', 'said'),\n",
       " ('news', 'it'),\n",
       " ('news', 'found'),\n",
       " ('news', 'the'),\n",
       " ('news', 'court'),\n",
       " ('news', '``'),\n",
       " ('news', 'has'),\n",
       " ('news', 'incorporated'),\n",
       " ('news', 'into'),\n",
       " ('news', 'its'),\n",
       " ('news', 'operating'),\n",
       " ('news', 'procedures'),\n",
       " ('news', 'the'),\n",
       " ('news', 'recommendations'),\n",
       " ('news', \"''\"),\n",
       " ('news', 'of'),\n",
       " ('news', 'two'),\n",
       " ('news', 'previous'),\n",
       " ('news', 'grand'),\n",
       " ('news', 'juries'),\n",
       " ('news', ','),\n",
       " ('news', 'the'),\n",
       " ('news', 'Atlanta'),\n",
       " ('news', 'Bar'),\n",
       " ('news', 'Association'),\n",
       " ('news', 'and'),\n",
       " ('news', 'an'),\n",
       " ('news', 'interim'),\n",
       " ('news', 'citizens'),\n",
       " ('news', 'committee'),\n",
       " ('news', '.'),\n",
       " ('news', '``'),\n",
       " ('news', 'These'),\n",
       " ('news', 'actions'),\n",
       " ('news', 'should'),\n",
       " ('news', 'serve'),\n",
       " ('news', 'to'),\n",
       " ('news', 'protect'),\n",
       " ('news', 'in'),\n",
       " ('news', 'fact'),\n",
       " ('news', 'and'),\n",
       " ('news', 'in'),\n",
       " ('news', 'effect'),\n",
       " ('news', 'the'),\n",
       " ('news', \"court's\"),\n",
       " ('news', 'wards'),\n",
       " ('news', 'from'),\n",
       " ('news', 'undue'),\n",
       " ('news', 'costs'),\n",
       " ('news', 'and'),\n",
       " ('news', 'its'),\n",
       " ('news', 'appointed'),\n",
       " ('news', 'and'),\n",
       " ('news', 'elected'),\n",
       " ('news', 'servants'),\n",
       " ('news', 'from'),\n",
       " ('news', 'unmeritorious'),\n",
       " ('news', 'criticisms'),\n",
       " ('news', \"''\"),\n",
       " ('news', ','),\n",
       " ('news', 'the'),\n",
       " ('news', 'jury'),\n",
       " ('news', 'said'),\n",
       " ('news', '.'),\n",
       " ('news', 'Regarding'),\n",
       " ('news', \"Atlanta's\"),\n",
       " ('news', 'new'),\n",
       " ('news', 'multi-million-dollar'),\n",
       " ('news', 'airport'),\n",
       " ('news', ','),\n",
       " ('news', 'the'),\n",
       " ('news', 'jury'),\n",
       " ('news', 'recommended'),\n",
       " ('news', '``'),\n",
       " ('news', 'that'),\n",
       " ('news', 'when'),\n",
       " ('news', 'the'),\n",
       " ('news', 'new'),\n",
       " ('news', 'management'),\n",
       " ('news', 'takes'),\n",
       " ('news', 'charge'),\n",
       " ('news', 'Jan.'),\n",
       " ('news', '1'),\n",
       " ('news', 'the'),\n",
       " ('news', 'airport'),\n",
       " ('news', 'be'),\n",
       " ('news', 'operated'),\n",
       " ('news', 'in'),\n",
       " ('news', 'a'),\n",
       " ('news', 'manner'),\n",
       " ('news', 'that'),\n",
       " ('news', 'will'),\n",
       " ('news', 'eliminate'),\n",
       " ('news', 'political'),\n",
       " ('news', 'influences'),\n",
       " ('news', \"''\"),\n",
       " ('news', '.'),\n",
       " ('news', 'The'),\n",
       " ('news', 'jury'),\n",
       " ('news', 'did'),\n",
       " ('news', 'not'),\n",
       " ('news', 'elaborate'),\n",
       " ('news', ','),\n",
       " ('news', 'but'),\n",
       " ('news', 'it'),\n",
       " ('news', 'added'),\n",
       " ('news', 'that'),\n",
       " ('news', '``'),\n",
       " ('news', 'there'),\n",
       " ('news', 'should'),\n",
       " ('news', 'be'),\n",
       " ('news', 'periodic'),\n",
       " ('news', 'surveillance'),\n",
       " ('news', 'of'),\n",
       " ('news', 'the'),\n",
       " ('news', 'pricing'),\n",
       " ('news', 'practices'),\n",
       " ('news', 'of'),\n",
       " ('news', 'the'),\n",
       " ('news', 'concessionaires'),\n",
       " ('news', 'for'),\n",
       " ('news', 'the'),\n",
       " ('news', 'purpose'),\n",
       " ('news', 'of'),\n",
       " ('news', 'keeping'),\n",
       " ('news', 'the'),\n",
       " ('news', 'prices'),\n",
       " ('news', 'reasonable'),\n",
       " ('news', \"''\"),\n",
       " ('news', '.'),\n",
       " ('news', 'Ask'),\n",
       " ('news', 'jail'),\n",
       " ('news', 'deputies'),\n",
       " ('news', 'On'),\n",
       " ('news', 'other'),\n",
       " ('news', 'matters'),\n",
       " ('news', ','),\n",
       " ('news', 'the'),\n",
       " ('news', 'jury'),\n",
       " ('news', 'recommended'),\n",
       " ('news', 'that'),\n",
       " ('news', ':'),\n",
       " ('news', '('),\n",
       " ('news', '1'),\n",
       " ('news', ')'),\n",
       " ('news', 'Four'),\n",
       " ('news', 'additional'),\n",
       " ('news', 'deputies'),\n",
       " ('news', 'be'),\n",
       " ('news', 'employed'),\n",
       " ('news', 'at'),\n",
       " ('news', 'the'),\n",
       " ('news', 'Fulton'),\n",
       " ('news', 'County'),\n",
       " ('news', 'Jail'),\n",
       " ('news', 'and'),\n",
       " ('news', '``'),\n",
       " ('news', 'a'),\n",
       " ('news', 'doctor'),\n",
       " ('news', ','),\n",
       " ('news', 'medical'),\n",
       " ('news', 'intern'),\n",
       " ('news', 'or'),\n",
       " ('news', 'extern'),\n",
       " ('news', 'be'),\n",
       " ('news', 'employed'),\n",
       " ('news', 'for'),\n",
       " ('news', 'night'),\n",
       " ('news', 'and'),\n",
       " ('news', 'weekend'),\n",
       " ('news', 'duty'),\n",
       " ('news', 'at'),\n",
       " ('news', 'the'),\n",
       " ('news', 'jail'),\n",
       " ('news', \"''\"),\n",
       " ('news', '.'),\n",
       " ('news', '('),\n",
       " ('news', '2'),\n",
       " ('news', ')'),\n",
       " ('news', 'Fulton'),\n",
       " ('news', 'legislators'),\n",
       " ('news', '``'),\n",
       " ('news', 'work'),\n",
       " ('news', 'with'),\n",
       " ('news', 'city'),\n",
       " ('news', 'officials'),\n",
       " ('news', 'to'),\n",
       " ('news', 'pass'),\n",
       " ('news', 'enabling'),\n",
       " ('news', 'legislation'),\n",
       " ('news', 'that'),\n",
       " ('news', 'will'),\n",
       " ('news', 'permit'),\n",
       " ('news', 'the'),\n",
       " ('news', 'establishment'),\n",
       " ('news', 'of'),\n",
       " ('news', 'a'),\n",
       " ('news', 'fair'),\n",
       " ('news', 'and'),\n",
       " ('news', 'equitable'),\n",
       " ('news', \"''\"),\n",
       " ('news', 'pension'),\n",
       " ('news', 'plan'),\n",
       " ('news', 'for'),\n",
       " ('news', 'city'),\n",
       " ('news', 'employes'),\n",
       " ('news', '.'),\n",
       " ('news', 'The'),\n",
       " ('news', 'jury'),\n",
       " ('news', 'praised'),\n",
       " ('news', 'the'),\n",
       " ('news', 'administration'),\n",
       " ('news', 'and'),\n",
       " ('news', 'operation'),\n",
       " ('news', 'of'),\n",
       " ('news', 'the'),\n",
       " ('news', 'Atlanta'),\n",
       " ('news', 'Police'),\n",
       " ('news', 'Department'),\n",
       " ('news', ','),\n",
       " ('news', 'the'),\n",
       " ('news', 'Fulton'),\n",
       " ('news', 'Tax'),\n",
       " ('news', \"Commissioner's\"),\n",
       " ('news', 'Office'),\n",
       " ('news', ','),\n",
       " ('news', 'the'),\n",
       " ('news', 'Bellwood'),\n",
       " ('news', 'and'),\n",
       " ('news', 'Alpharetta'),\n",
       " ('news', 'prison'),\n",
       " ('news', 'farms'),\n",
       " ('news', ','),\n",
       " ('news', 'Grady'),\n",
       " ('news', 'Hospital'),\n",
       " ('news', 'and'),\n",
       " ('news', 'the'),\n",
       " ('news', 'Fulton'),\n",
       " ('news', 'Health'),\n",
       " ('news', 'Department'),\n",
       " ('news', '.'),\n",
       " ('news', 'Mayor'),\n",
       " ('news', 'William'),\n",
       " ('news', 'B.'),\n",
       " ('news', 'Hartsfield'),\n",
       " ('news', 'filed'),\n",
       " ('news', 'suit'),\n",
       " ('news', 'for'),\n",
       " ('news', 'divorce'),\n",
       " ('news', 'from'),\n",
       " ('news', 'his'),\n",
       " ('news', 'wife'),\n",
       " ('news', ','),\n",
       " ('news', 'Pearl'),\n",
       " ('news', 'Williams'),\n",
       " ('news', 'Hartsfield'),\n",
       " ('news', ','),\n",
       " ('news', 'in'),\n",
       " ('news', 'Fulton'),\n",
       " ('news', 'Superior'),\n",
       " ('news', 'Court'),\n",
       " ('news', 'Friday'),\n",
       " ('news', '.'),\n",
       " ('news', 'His'),\n",
       " ('news', 'petition'),\n",
       " ('news', 'charged'),\n",
       " ('news', 'mental'),\n",
       " ('news', 'cruelty'),\n",
       " ('news', '.'),\n",
       " ('news', 'The'),\n",
       " ('news', 'couple'),\n",
       " ('news', 'was'),\n",
       " ('news', 'married'),\n",
       " ('news', 'Aug.'),\n",
       " ('news', '2'),\n",
       " ('news', ','),\n",
       " ('news', '1913'),\n",
       " ('news', '.'),\n",
       " ('news', 'They'),\n",
       " ('news', 'have'),\n",
       " ('news', 'a'),\n",
       " ('news', 'son'),\n",
       " ('news', ','),\n",
       " ('news', 'William'),\n",
       " ('news', 'Berry'),\n",
       " ('news', 'Jr.'),\n",
       " ('news', ','),\n",
       " ('news', 'and'),\n",
       " ('news', 'a'),\n",
       " ('news', 'daughter'),\n",
       " ('news', ','),\n",
       " ('news', 'Mrs.'),\n",
       " ('news', 'J.'),\n",
       " ('news', 'M.'),\n",
       " ('news', 'Cheshire'),\n",
       " ('news', 'of'),\n",
       " ('news', 'Griffin'),\n",
       " ('news', '.'),\n",
       " ('news', 'Attorneys'),\n",
       " ('news', 'for'),\n",
       " ('news', 'the'),\n",
       " ('news', 'mayor'),\n",
       " ('news', 'said'),\n",
       " ('news', 'that'),\n",
       " ('news', 'an'),\n",
       " ('news', 'amicable'),\n",
       " ('news', 'property'),\n",
       " ('news', 'settlement'),\n",
       " ('news', 'has'),\n",
       " ('news', 'been'),\n",
       " ('news', 'agreed'),\n",
       " ('news', 'upon'),\n",
       " ('news', '.'),\n",
       " ('news', 'The'),\n",
       " ('news', 'petition'),\n",
       " ('news', 'listed'),\n",
       " ('news', 'the'),\n",
       " ('news', \"mayor's\"),\n",
       " ('news', 'occupation'),\n",
       " ('news', 'as'),\n",
       " ('news', '``'),\n",
       " ('news', 'attorney'),\n",
       " ('news', \"''\"),\n",
       " ('news', 'and'),\n",
       " ('news', 'his'),\n",
       " ('news', 'age'),\n",
       " ('news', 'as'),\n",
       " ('news', '71'),\n",
       " ('news', '.'),\n",
       " ('news', 'It'),\n",
       " ('news', 'listed'),\n",
       " ('news', 'his'),\n",
       " ('news', \"wife's\"),\n",
       " ('news', 'age'),\n",
       " ('news', 'as'),\n",
       " ('news', '74'),\n",
       " ('news', 'and'),\n",
       " ('news', 'place'),\n",
       " ('news', 'of'),\n",
       " ('news', 'birth'),\n",
       " ('news', 'as'),\n",
       " ('news', 'Opelika'),\n",
       " ('news', ','),\n",
       " ('news', 'Ala.'),\n",
       " ('news', '.'),\n",
       " ('news', 'The'),\n",
       " ('news', 'petition'),\n",
       " ('news', 'said'),\n",
       " ('news', 'that'),\n",
       " ('news', 'the'),\n",
       " ('news', 'couple'),\n",
       " ('news', 'has'),\n",
       " ('news', 'not'),\n",
       " ('news', 'lived'),\n",
       " ('news', 'together'),\n",
       " ('news', 'as'),\n",
       " ('news', 'man'),\n",
       " ('news', 'and'),\n",
       " ('news', 'wife'),\n",
       " ('news', 'for'),\n",
       " ('news', 'more'),\n",
       " ('news', 'than'),\n",
       " ('news', 'a'),\n",
       " ('news', 'year'),\n",
       " ('news', '.'),\n",
       " ('news', 'The'),\n",
       " ('news', 'Hartsfield'),\n",
       " ('news', 'home'),\n",
       " ('news', 'is'),\n",
       " ('news', 'at'),\n",
       " ('news', '637'),\n",
       " ('news', 'E.'),\n",
       " ('news', 'Pelham'),\n",
       " ('news', 'Rd.'),\n",
       " ('news', 'Aj'),\n",
       " ('news', '.'),\n",
       " ('news', 'Henry'),\n",
       " ('news', 'L.'),\n",
       " ('news', 'Bowden'),\n",
       " ('news', 'was'),\n",
       " ('news', 'listed'),\n",
       " ('news', 'on'),\n",
       " ('news', 'the'),\n",
       " ('news', 'petition'),\n",
       " ('news', 'as'),\n",
       " ('news', 'the'),\n",
       " ('news', \"mayor's\"),\n",
       " ('news', 'attorney'),\n",
       " ('news', '.'),\n",
       " ('news', 'Hartsfield'),\n",
       " ('news', 'has'),\n",
       " ('news', 'been'),\n",
       " ('news', 'mayor'),\n",
       " ('news', 'of'),\n",
       " ('news', 'Atlanta'),\n",
       " ('news', ','),\n",
       " ('news', 'with'),\n",
       " ('news', 'exception'),\n",
       " ('news', 'of'),\n",
       " ('news', 'one'),\n",
       " ('news', 'brief'),\n",
       " ('news', 'interlude'),\n",
       " ('news', ','),\n",
       " ('news', 'since'),\n",
       " ('news', '1937'),\n",
       " ('news', '.'),\n",
       " ('news', 'His'),\n",
       " ('news', 'political'),\n",
       " ('news', 'career'),\n",
       " ('news', 'goes'),\n",
       " ('news', 'back'),\n",
       " ('news', 'to'),\n",
       " ('news', 'his'),\n",
       " ('news', 'election'),\n",
       " ('news', 'to'),\n",
       " ('news', 'city'),\n",
       " ('news', 'council'),\n",
       " ('news', 'in'),\n",
       " ('news', '1923'),\n",
       " ('news', '.'),\n",
       " ('news', 'The'),\n",
       " ('news', \"mayor's\"),\n",
       " ('news', 'present'),\n",
       " ('news', 'term'),\n",
       " ('news', 'of'),\n",
       " ('news', 'office'),\n",
       " ('news', 'expires'),\n",
       " ('news', 'Jan.'),\n",
       " ('news', '1'),\n",
       " ('news', '.'),\n",
       " ('news', 'He'),\n",
       " ('news', 'will'),\n",
       " ('news', 'be'),\n",
       " ('news', 'succeeded'),\n",
       " ('news', 'by'),\n",
       " ('news', 'Ivan'),\n",
       " ('news', 'Allen'),\n",
       " ('news', 'Jr.'),\n",
       " ('news', ','),\n",
       " ('news', 'who'),\n",
       " ('news', 'became'),\n",
       " ('news', 'a'),\n",
       " ('news', 'candidate'),\n",
       " ('news', 'in'),\n",
       " ('news', 'the'),\n",
       " ('news', 'Sept.'),\n",
       " ('news', '13'),\n",
       " ('news', 'primary'),\n",
       " ('news', 'after'),\n",
       " ('news', 'Mayor'),\n",
       " ('news', 'Hartsfield'),\n",
       " ('news', 'announced'),\n",
       " ('news', 'that'),\n",
       " ('news', 'he'),\n",
       " ('news', 'would'),\n",
       " ('news', 'not'),\n",
       " ('news', 'run'),\n",
       " ('news', 'for'),\n",
       " ('news', 'reelection'),\n",
       " ('news', '.'),\n",
       " ('news', 'Georgia'),\n",
       " ('news', 'Republicans'),\n",
       " ('news', 'are'),\n",
       " ('news', 'getting'),\n",
       " ('news', 'strong'),\n",
       " ('news', 'encouragement'),\n",
       " ('news', 'to'),\n",
       " ('news', 'enter'),\n",
       " ('news', 'a'),\n",
       " ('news', 'candidate'),\n",
       " ('news', 'in'),\n",
       " ('news', 'the'),\n",
       " ('news', '1962'),\n",
       " ('news', \"governor's\"),\n",
       " ...]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "genre_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfd = nltk.ConditionalFreqDist(genre_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ConditionalFreqDist with 2 conditions>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['news', 'romance']"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfd.conditions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'the': 5580, ',': 5188, '.': 4030, 'of': 2849, 'and': 2146, 'to': 2116, 'a': 1993, 'in': 1893, 'for': 943, 'The': 806, ...})"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfd['news']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<FreqDist with 14394 samples and 100554 outcomes>\n"
     ]
    }
   ],
   "source": [
    "print(cfd['news'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(',', 3899),\n",
       " ('.', 3736),\n",
       " ('the', 2758),\n",
       " ('and', 1776),\n",
       " ('to', 1502),\n",
       " ('a', 1335),\n",
       " ('of', 1186),\n",
       " ('``', 1045),\n",
       " (\"''\", 1044),\n",
       " ('was', 993)]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfd['romance'].most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1776"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfd['romance']['and']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在plot（）和tabulate（）方法中，我们可以选择使用conditions =参数指定要显示的条件。 当我们忽略它时，我们得到了所有条件。 同样，我们可以限制样本显示为带有samples =参数。 这样就可以将大量数据加载到条件频率分布中，然后通过对选定条件和样本进行绘图或制表来对其进行探索。 它还使我们可以完全控制任何显示中条件和样品的顺序。 例如，我们可以将两种语言以及少于10个字符的单词的累积频率数据制成表格，如下所示。 我们将第一行的最后一个单元格解释为英语文本中1,638个单词的字母少于9个。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        0 1 2 3 4 5 6 7 8 9 \n",
      "   news 0 0 0 0 0 0 0 0 0 0 \n",
      "romance 0 0 0 0 0 0 0 0 0 0 \n"
     ]
    }
   ],
   "source": [
    "cfd.tabulate(conditions=['news','romance'],samples=range(10),cumulative=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " when we use a list comprehension as a parameter to a function, like set([w.lower() for w in t]), we are permitted to omit the square brackets and just write: set(w.lower() for w in t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
